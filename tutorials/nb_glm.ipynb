{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.glm_nb.Simulator(num_features=100)\n",
    "sim.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 1000, features: 100)>\n",
       "array([[ 592,  446,  472, ...,  686,  166,  460],\n",
       "       [1040,  436,  545, ..., 1999, 1014,  548],\n",
       "       [ 355, 1246, 1169, ...,  362,  111,  260],\n",
       "       ...,\n",
       "       [ 351,  487, 2142, ...,   43,  664, 1656],\n",
       "       [ 836,  397, 1643, ...,  339,  729, 1277],\n",
       "       [ 406,  752, 1903, ...,  136, 1616,  538]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_scale, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters used to generate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_loc_params: 5, features: 100)>\n",
       "array([[ 6.236370e+00,  6.249975e+00,  6.285998e+00, ...,  6.228511e+00,\n",
       "         6.257668e+00,  6.326149e+00],\n",
       "       [ 2.049018e-01,  2.693441e-01,  6.121345e-01, ...,  5.915393e-01,\n",
       "         6.811870e-01, -4.860269e-03],\n",
       "       [ 3.234056e-01,  1.514454e-02,  4.739485e-02, ...,  1.575983e-01,\n",
       "         3.081887e-01,  5.101046e-01],\n",
       "       [ 1.487480e-02,  7.693484e-03, -3.710693e-01, ..., -9.910821e-02,\n",
       "         6.230951e-01,  5.508496e-02],\n",
       "       [ 2.107061e-01,  2.657196e-02,  5.290659e-01, ..., -6.421812e-01,\n",
       "        -8.659752e-02,  6.675474e-01]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) object 'Intercept' ... 'batch[T.3]'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_scale_params: 5, features: 100)>\n",
       "array([[ 0.663148,  0.388968, -0.029529, ...,  0.533484,  0.523392,  0.330428],\n",
       "       [ 0.430409,  0.641547,  0.270136, ..., -0.093337,  0.65314 ,  0.055318],\n",
       "       [ 0.3044  ,  0.475201,  0.449178, ..., -0.258936,  0.582445, -0.088318],\n",
       "       [ 0.487693,  0.435016,  0.588147, ...,  0.231171,  0.336771, -0.341528],\n",
       "       [-0.576247, -0.144176,  0.43414 , ..., -0.504932,  0.433781,  0.677088]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) object 'Intercept' ... 'batch[T.3]'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = sim.design_loc\n",
    "design_scale = sim.design_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.glm_nb.InputData.new(data=X, design_loc=design_loc, design_scale=design_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[batchglm.models.base_glm.input.InputData object at 0x7f0d4787c320]: data=<xarray.Dataset>\n",
       "    Dimensions:              (design_loc_params: 5, design_scale_params: 5, features: 100, loc_params: 5, observations: 1000, scale_params: 5)\n",
       "    Coordinates:\n",
       "      * observations         (observations) int64 0 1 2 3 4 ... 995 996 997 998 999\n",
       "        feature_allzero      (features) bool False False False ... False False False\n",
       "      * features             (features) int64 0 1 2 3 4 5 6 ... 93 94 95 96 97 98 99\n",
       "      * design_loc_params    (design_loc_params) object 'Intercept' ... 'batch[T.3]'\n",
       "      * design_scale_params  (design_scale_params) object 'Intercept' ... 'batch[T.3]'\n",
       "        loc_params           (design_loc_params) object 'Intercept' ... 'batch[T.3]'\n",
       "        scale_params         (design_scale_params) object 'Intercept' ... 'batch[T.3]'\n",
       "    Data variables:\n",
       "        X                    (observations, features) int64 592 446 472 ... 1616 538\n",
       "        design_loc           (observations, design_loc_params) float64 1.0 ... 1.0\n",
       "        design_scale         (observations, design_scale_params) float64 1.0 ... 1.0\n",
       "        constraints_loc      (design_loc_params, loc_params) float64 1.0 0.0 ... 1.0\n",
       "        constraints_scale    (design_scale_params, scale_params) float64 1.0 ... 1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4947ae74a0c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglm_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#estimator.initialize()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Praktikum/batchglm/batchglm/train/tf/glm_nb/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_data, batch_size, graph, init_model, init_a, init_b, quick_scale, model, provide_optimizers, provide_batched, termination_type, extended_summary, dtype)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mextended_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_summary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mnoise_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Praktikum/batchglm/batchglm/train/tf/base_glm_all/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_data, batch_size, graph, init_a, init_b, model, provide_optimizers, provide_batched, termination_type, extended_summary, noise_model, dtype)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mextended_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_summary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mnoise_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             )\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Praktikum/batchglm/batchglm/train/tf/base_glm_all/estimator_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetch_fn, feature_isnonzero, num_observations, num_features, num_design_loc_params, num_design_scale_params, num_loc_params, num_scale_params, constraints_loc, constraints_scale, graph, batch_size, init_a, init_b, train_loc, train_scale, provide_optimizers, provide_batched, termination_type, extended_summary, noise_model, dtype)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;31m# Check whether it is necessary to compute FIM:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;31m# The according sub-graphs are only compiled if this is needed during training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mprovide_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"irls\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprovide_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"irls_tr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mprovide_fim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.glm_nb.Estimator(input_data)\n",
    "#estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train:\n",
    "\n",
    "There are multiple possible training strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTO\n",
      "DEFAULT\n",
      "EXACT\n",
      "QUICK\n",
      "PRE_INITIALIZED\n"
     ]
    }
   ],
   "source": [
    "for i in estimator.TrainingStrategy:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one of them corresponds to a list of training options which will be passed to the estimator.train() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(estimator.TrainingStrategy.DEFAULT.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, when choosing the training strategy \"DEFAULT\", the following call:\n",
    "```python\n",
    "estimator.train_sequence(\"DEFAULT\")\n",
    "```\n",
    "is equal to:\n",
    "```python\n",
    "estimator.train_sequence(estimator.TrainingStrategy.DEFAULT)\n",
    "```\n",
    "is equal to:\n",
    "```python\n",
    "estimator.train(\n",
    "    convergence_criteria = 't_test',\n",
    "    learning_rate = 0.1,\n",
    "    loss_window_size = 100,\n",
    "    optim_algo = 'ADAM',\n",
    "    stop_at_loss_change = 0.05,\n",
    "    use_batching = True\n",
    ")\n",
    "estimator.train(\n",
    "    convergence_criteria = 't_test',\n",
    "    learning_rate = 0.05,\n",
    "    loss_window_size = 10,\n",
    "    optim_algo = 'GD',\n",
    "    stop_at_loss_change = 0.05,\n",
    "    use_batching = False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 885.513913\n",
      "Step: 2\tloss: 898.976127\n",
      "Step: 3\tloss: 889.199063\n",
      "Step: 4\tloss: 890.597800\n",
      "Step: 5\tloss: 890.676163\n",
      "Step: 6\tloss: 890.348473\n",
      "Step: 7\tloss: 888.962466\n",
      "Step: 8\tloss: 888.290230\n",
      "Step: 9\tloss: 888.311687\n",
      "Step: 10\tloss: 889.301279\n",
      "Step: 11\tloss: 887.976396\n",
      "Step: 12\tloss: 887.533220\n",
      "Step: 13\tloss: 887.237896\n",
      "Step: 14\tloss: 887.413583\n",
      "Step: 15\tloss: 887.046304\n",
      "Step: 16\tloss: 886.387423\n",
      "Step: 17\tloss: 887.318333\n",
      "Step: 18\tloss: 887.345315\n",
      "Step: 19\tloss: 886.430518\n",
      "Step: 20\tloss: 885.893532\n",
      "Step: 21\tloss: 886.252933\n",
      "Step: 22\tloss: 887.000912\n",
      "Step: 23\tloss: 886.164973\n",
      "Step: 24\tloss: 885.516419\n",
      "Step: 25\tloss: 885.399947\n",
      "Step: 26\tloss: 886.760356\n",
      "Step: 27\tloss: 886.788990\n",
      "Step: 28\tloss: 885.502744\n",
      "Step: 29\tloss: 886.114336\n",
      "Step: 30\tloss: 885.763258\n",
      "Step: 31\tloss: 885.582171\n",
      "Step: 32\tloss: 886.363210\n",
      "Step: 33\tloss: 886.464011\n",
      "Step: 34\tloss: 886.208091\n",
      "Step: 35\tloss: 885.418267\n",
      "Step: 36\tloss: 885.248987\n",
      "Step: 37\tloss: 886.098450\n",
      "Step: 38\tloss: 886.443648\n",
      "Step: 39\tloss: 885.249253\n",
      "Step: 40\tloss: 885.207692\n",
      "Step: 41\tloss: 886.275501\n",
      "Step: 42\tloss: 885.892441\n",
      "Step: 43\tloss: 885.101431\n",
      "Step: 44\tloss: 885.532208\n",
      "Step: 45\tloss: 885.189953\n",
      "Step: 46\tloss: 886.483929\n",
      "Step: 47\tloss: 885.844448\n",
      "Step: 48\tloss: 885.288976\n",
      "Step: 49\tloss: 886.033752\n",
      "Step: 50\tloss: 885.877834\n",
      "Step: 51\tloss: 885.262609\n",
      "Step: 52\tloss: 885.564448\n",
      "Step: 53\tloss: 886.809548\n",
      "Step: 54\tloss: 885.651337\n",
      "Step: 55\tloss: 884.964231\n",
      "Step: 56\tloss: 885.331178\n",
      "Step: 57\tloss: 885.712226\n",
      "Step: 58\tloss: 885.790924\n",
      "Step: 59\tloss: 885.546945\n",
      "Step: 60\tloss: 885.706013\n",
      "Step: 61\tloss: 885.318294\n",
      "Step: 62\tloss: 886.700413\n",
      "Step: 63\tloss: 885.356035\n",
      "Step: 64\tloss: 885.256497\n",
      "Step: 65\tloss: 885.957041\n",
      "Step: 66\tloss: 886.226514\n",
      "Step: 67\tloss: 884.953874\n",
      "Step: 68\tloss: 885.713630\n",
      "Step: 69\tloss: 886.284824\n",
      "Step: 70\tloss: 886.811733\n",
      "Step: 71\tloss: 884.916732\n",
      "Step: 72\tloss: 884.731114\n",
      "Step: 73\tloss: 885.726330\n",
      "Step: 74\tloss: 886.574139\n",
      "Step: 75\tloss: 885.889522\n",
      "Step: 76\tloss: 884.555865\n",
      "Step: 77\tloss: 886.353722\n",
      "Step: 78\tloss: 886.097547\n",
      "Step: 79\tloss: 885.073838\n",
      "Step: 80\tloss: 885.265330\n",
      "Step: 81\tloss: 885.375273\n",
      "Step: 82\tloss: 886.106321\n",
      "Step: 83\tloss: 885.303383\n",
      "Step: 84\tloss: 886.018753\n",
      "Step: 85\tloss: 886.055232\n",
      "Step: 86\tloss: 886.678678\n",
      "Step: 87\tloss: 885.249728\n",
      "Step: 88\tloss: 884.957463\n",
      "Step: 89\tloss: 885.678425\n",
      "Step: 90\tloss: 886.143267\n",
      "Step: 91\tloss: 885.378764\n",
      "Step: 92\tloss: 885.577179\n",
      "Step: 93\tloss: 886.469197\n",
      "Step: 94\tloss: 886.262913\n",
      "Step: 95\tloss: 884.829433\n",
      "Step: 96\tloss: 885.346466\n",
      "Step: 97\tloss: 886.054807\n",
      "Step: 98\tloss: 886.340502\n",
      "Step: 99\tloss: 886.013304\n",
      "Step: 100\tloss: 884.429521\n",
      "Step: 101\tloss: 885.868947\n",
      "Step: 102\tloss: 885.997614\n",
      "Step: 103\tloss: 885.704543\n",
      "Step: 104\tloss: 885.265192\n",
      "Step: 105\tloss: 886.423887\n",
      "Step: 106\tloss: 885.825850\n",
      "Step: 107\tloss: 885.305626\n",
      "Step: 108\tloss: 885.315901\n",
      "Step: 109\tloss: 885.820695\n",
      "Step: 110\tloss: 886.583452\n",
      "Step: 111\tloss: 885.041982\n",
      "Step: 112\tloss: 885.354619\n",
      "Step: 113\tloss: 886.334991\n",
      "Step: 114\tloss: 885.497668\n",
      "Step: 115\tloss: 885.349627\n",
      "Step: 116\tloss: 885.691297\n",
      "Step: 117\tloss: 885.816976\n",
      "Step: 118\tloss: 885.255328\n",
      "Step: 119\tloss: 886.065183\n",
      "Step: 120\tloss: 885.723165\n",
      "Step: 121\tloss: 885.493758\n",
      "Step: 122\tloss: 886.492555\n",
      "Step: 123\tloss: 885.572141\n",
      "Step: 124\tloss: 885.470284\n",
      "Step: 125\tloss: 886.451076\n",
      "Step: 126\tloss: 886.147512\n",
      "Step: 127\tloss: 884.991307\n",
      "Step: 128\tloss: 885.663093\n",
      "Step: 129\tloss: 886.015598\n",
      "Step: 130\tloss: 886.735470\n",
      "Step: 131\tloss: 886.158062\n",
      "Step: 132\tloss: 884.284670\n",
      "Step: 133\tloss: 886.121103\n",
      "Step: 134\tloss: 885.530606\n",
      "Step: 135\tloss: 885.417590\n",
      "Step: 136\tloss: 886.106759\n",
      "Step: 137\tloss: 886.079245\n",
      "Step: 138\tloss: 886.153492\n",
      "Step: 139\tloss: 885.351173\n",
      "Step: 140\tloss: 885.595391\n",
      "Step: 141\tloss: 886.815123\n",
      "Step: 142\tloss: 885.684422\n",
      "Step: 143\tloss: 885.243644\n",
      "Step: 144\tloss: 885.508355\n",
      "Step: 145\tloss: 885.637982\n",
      "Step: 146\tloss: 886.743502\n",
      "Step: 147\tloss: 884.531085\n",
      "Step: 148\tloss: 886.362005\n",
      "Step: 149\tloss: 886.468905\n",
      "Step: 150\tloss: 885.942340\n",
      "Step: 151\tloss: 885.410312\n",
      "Step: 152\tloss: 885.270253\n",
      "Step: 153\tloss: 885.911051\n",
      "Step: 154\tloss: 886.351204\n",
      "Step: 155\tloss: 885.055822\n",
      "Step: 156\tloss: 885.875476\n",
      "Step: 157\tloss: 887.217439\n",
      "Step: 158\tloss: 886.581170\n",
      "Step: 159\tloss: 884.963734\n",
      "Step: 160\tloss: 884.641726\n",
      "Step: 161\tloss: 886.638848\n",
      "Step: 162\tloss: 885.452679\n",
      "Step: 163\tloss: 885.929797\n",
      "Step: 164\tloss: 885.463053\n",
      "Step: 165\tloss: 886.248543\n",
      "Step: 166\tloss: 885.916307\n",
      "Step: 167\tloss: 885.658364\n",
      "Step: 168\tloss: 885.635794\n",
      "Step: 169\tloss: 885.938015\n",
      "Step: 170\tloss: 886.861065\n",
      "Step: 171\tloss: 884.751652\n",
      "Step: 172\tloss: 886.069397\n",
      "Step: 173\tloss: 885.830983\n",
      "Step: 174\tloss: 886.160683\n",
      "Step: 175\tloss: 886.575279\n",
      "Step: 176\tloss: 884.883997\n",
      "Step: 177\tloss: 886.074439\n",
      "Step: 178\tloss: 886.418480\n",
      "Step: 179\tloss: 885.074489\n",
      "Step: 180\tloss: 885.835972\n",
      "Step: 181\tloss: 885.944050\n",
      "Step: 182\tloss: 885.944512\n",
      "Step: 183\tloss: 885.703785\n",
      "Step: 184\tloss: 885.736206\n",
      "Step: 185\tloss: 886.809619\n",
      "Step: 186\tloss: 885.617069\n",
      "Step: 187\tloss: 885.398756\n",
      "Step: 188\tloss: 885.315971\n",
      "Step: 189\tloss: 885.961132\n",
      "Step: 190\tloss: 886.040078\n",
      "Step: 191\tloss: 885.374905\n",
      "Step: 192\tloss: 885.768148\n",
      "Step: 193\tloss: 887.174846\n",
      "Step: 194\tloss: 886.334036\n",
      "Step: 195\tloss: 885.489855\n",
      "Step: 196\tloss: 884.286067\n",
      "Step: 197\tloss: 886.643171\n",
      "Step: 198\tloss: 885.588283\n",
      "Step: 199\tloss: 885.284479\n",
      "Step: 200\tloss: 885.739223\n",
      "pval: 0.002675\n",
      "Step: 201\tloss: 885.948788\n",
      "Step: 202\tloss: 885.682231\n",
      "Step: 203\tloss: 885.886967\n",
      "Step: 204\tloss: 885.753952\n",
      "Step: 205\tloss: 884.974427\n",
      "Step: 206\tloss: 886.070284\n",
      "Step: 207\tloss: 885.693461\n",
      "Step: 208\tloss: 886.620687\n",
      "Step: 209\tloss: 885.790585\n",
      "Step: 210\tloss: 885.956992\n",
      "Step: 211\tloss: 885.330953\n",
      "Step: 212\tloss: 886.263774\n",
      "Step: 213\tloss: 886.486785\n",
      "Step: 214\tloss: 886.211040\n",
      "Step: 215\tloss: 886.172991\n",
      "Step: 216\tloss: 884.511788\n",
      "Step: 217\tloss: 886.354746\n",
      "Step: 218\tloss: 886.098930\n",
      "Step: 219\tloss: 885.956295\n",
      "Step: 220\tloss: 884.922427\n",
      "Step: 221\tloss: 886.771447\n",
      "Step: 222\tloss: 885.793176\n",
      "Step: 223\tloss: 885.434632\n",
      "Step: 224\tloss: 885.404149\n",
      "Step: 225\tloss: 885.901289\n",
      "Step: 226\tloss: 886.618143\n",
      "Step: 227\tloss: 885.639669\n",
      "Step: 228\tloss: 885.158897\n",
      "Step: 229\tloss: 886.321403\n",
      "Step: 230\tloss: 885.955478\n",
      "Step: 231\tloss: 886.115012\n",
      "Step: 232\tloss: 885.053180\n",
      "Step: 233\tloss: 886.287690\n",
      "Step: 234\tloss: 886.380863\n",
      "Step: 235\tloss: 884.763533\n",
      "Step: 236\tloss: 885.920627\n",
      "Step: 237\tloss: 885.795755\n",
      "Step: 238\tloss: 885.829053\n",
      "Step: 239\tloss: 885.710795\n",
      "Step: 240\tloss: 885.959266\n",
      "Step: 241\tloss: 886.362389\n",
      "Step: 242\tloss: 886.398098\n",
      "Step: 243\tloss: 885.329415\n",
      "Step: 244\tloss: 885.153628\n",
      "Step: 245\tloss: 886.251854\n",
      "Step: 246\tloss: 886.116789\n",
      "Step: 247\tloss: 885.322891\n",
      "Step: 248\tloss: 885.664218\n",
      "Step: 249\tloss: 885.808849\n",
      "Step: 250\tloss: 886.125211\n",
      "Step: 251\tloss: 885.969102\n",
      "Step: 252\tloss: 885.498382\n",
      "Step: 253\tloss: 886.169459\n",
      "Step: 254\tloss: 886.116687\n",
      "Step: 255\tloss: 885.933655\n",
      "Step: 256\tloss: 885.295253\n",
      "Step: 257\tloss: 886.252351\n",
      "Step: 258\tloss: 885.864638\n",
      "Step: 259\tloss: 886.302497\n",
      "Step: 260\tloss: 885.132850\n",
      "Step: 261\tloss: 885.615931\n",
      "Step: 262\tloss: 885.914465\n",
      "Step: 263\tloss: 886.255193\n",
      "Step: 264\tloss: 885.638945\n",
      "Step: 265\tloss: 886.148274\n",
      "Step: 266\tloss: 885.749491\n",
      "Step: 267\tloss: 885.975465\n",
      "Step: 268\tloss: 885.535104\n",
      "Step: 269\tloss: 886.247605\n",
      "Step: 270\tloss: 885.772516\n",
      "Step: 271\tloss: 886.322528\n",
      "Step: 272\tloss: 885.159781\n",
      "Step: 273\tloss: 887.329925\n",
      "Step: 274\tloss: 885.770240\n",
      "Step: 275\tloss: 885.181141\n",
      "Step: 276\tloss: 885.193664\n",
      "Step: 277\tloss: 886.593078\n",
      "Step: 278\tloss: 885.258466\n",
      "Step: 279\tloss: 886.057974\n",
      "Step: 280\tloss: 885.514242\n",
      "Step: 281\tloss: 886.053365\n",
      "Step: 282\tloss: 886.790033\n",
      "Step: 283\tloss: 885.284597\n",
      "Step: 284\tloss: 885.162204\n",
      "Step: 285\tloss: 886.155802\n",
      "Step: 286\tloss: 886.059330\n",
      "Step: 287\tloss: 885.626334\n",
      "Step: 288\tloss: 885.546485\n",
      "Step: 289\tloss: 886.365841\n",
      "Step: 290\tloss: 886.993566\n",
      "Step: 291\tloss: 884.919083\n",
      "Step: 292\tloss: 885.184196\n",
      "Step: 293\tloss: 886.662418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 294\tloss: 885.970917\n",
      "Step: 295\tloss: 885.249642\n",
      "Step: 296\tloss: 885.652558\n",
      "Step: 297\tloss: 886.476882\n",
      "Step: 298\tloss: 885.302112\n",
      "Step: 299\tloss: 885.914032\n",
      "Step: 300\tloss: 886.021348\n",
      "pval: 0.754031\n",
      "Training sequence #1 complete\n",
      "Beginning with training sequence #2\n",
      "Step: 301\tloss: 885.754197\n",
      "Step: 302\tloss: 888.480427\n",
      "Step: 303\tloss: 886.006512\n",
      "Step: 304\tloss: 886.239537\n",
      "Step: 305\tloss: 886.844229\n",
      "Step: 306\tloss: 886.370105\n",
      "Step: 307\tloss: 885.765668\n",
      "Step: 308\tloss: 885.759317\n",
      "Step: 309\tloss: 886.065760\n",
      "Step: 310\tloss: 886.104541\n",
      "Step: 311\tloss: 885.814132\n",
      "Step: 312\tloss: 885.549854\n",
      "Step: 313\tloss: 885.549775\n",
      "Step: 314\tloss: 885.693918\n",
      "Step: 315\tloss: 885.740031\n",
      "Step: 316\tloss: 885.641682\n",
      "Step: 317\tloss: 885.530796\n",
      "Step: 318\tloss: 885.506574\n",
      "Step: 319\tloss: 885.538244\n",
      "Step: 320\tloss: 885.544845\n",
      "pval: 0.008181\n",
      "Step: 321\tloss: 885.503336\n",
      "Step: 322\tloss: 885.458179\n",
      "Step: 323\tloss: 885.447766\n",
      "Step: 324\tloss: 885.458214\n",
      "Step: 325\tloss: 885.457418\n",
      "Step: 326\tloss: 885.441182\n",
      "Step: 327\tloss: 885.426395\n",
      "Step: 328\tloss: 885.418776\n",
      "Step: 329\tloss: 885.410001\n",
      "Step: 330\tloss: 885.398626\n",
      "pval: 0.000192\n",
      "Step: 331\tloss: 885.393239\n",
      "Step: 332\tloss: 885.394559\n",
      "Step: 333\tloss: 885.392451\n",
      "Step: 334\tloss: 885.383805\n",
      "Step: 335\tloss: 885.376924\n",
      "Step: 336\tloss: 885.375964\n",
      "Step: 337\tloss: 885.375375\n",
      "Step: 338\tloss: 885.371030\n",
      "Step: 339\tloss: 885.365597\n",
      "Step: 340\tloss: 885.362429\n",
      "pval: 0.000018\n",
      "Step: 341\tloss: 885.361516\n",
      "Step: 342\tloss: 885.361290\n",
      "Step: 343\tloss: 885.360319\n",
      "Step: 344\tloss: 885.357868\n",
      "Step: 345\tloss: 885.354892\n",
      "Step: 346\tloss: 885.353328\n",
      "Step: 347\tloss: 885.353232\n",
      "Step: 348\tloss: 885.352603\n",
      "Step: 349\tloss: 885.350796\n",
      "Step: 350\tloss: 885.349461\n",
      "pval: 0.000020\n",
      "Step: 351\tloss: 885.349426\n",
      "Step: 352\tloss: 885.349332\n",
      "Step: 353\tloss: 885.348120\n",
      "Step: 354\tloss: 885.346683\n",
      "Step: 355\tloss: 885.346220\n",
      "Step: 356\tloss: 885.346402\n",
      "Step: 357\tloss: 885.346193\n",
      "Step: 358\tloss: 885.345539\n",
      "Step: 359\tloss: 885.344999\n",
      "Step: 360\tloss: 885.344619\n",
      "pval: 0.000028\n",
      "Step: 361\tloss: 885.344269\n",
      "Step: 362\tloss: 885.344063\n",
      "Step: 363\tloss: 885.343934\n",
      "Step: 364\tloss: 885.343682\n",
      "Step: 365\tloss: 885.343449\n",
      "Step: 366\tloss: 885.343361\n",
      "Step: 367\tloss: 885.343181\n",
      "Step: 368\tloss: 885.342857\n",
      "Step: 369\tloss: 885.342713\n",
      "Step: 370\tloss: 885.342787\n",
      "pval: 0.000032\n",
      "Step: 371\tloss: 885.342728\n",
      "Step: 372\tloss: 885.342463\n",
      "Step: 373\tloss: 885.342304\n",
      "Step: 374\tloss: 885.342349\n",
      "Step: 375\tloss: 885.342337\n",
      "Step: 376\tloss: 885.342178\n",
      "Step: 377\tloss: 885.342084\n",
      "Step: 378\tloss: 885.342103\n",
      "Step: 379\tloss: 885.342060\n",
      "Step: 380\tloss: 885.341952\n",
      "pval: 0.000014\n",
      "Step: 381\tloss: 885.341917\n",
      "Step: 382\tloss: 885.341934\n",
      "Step: 383\tloss: 885.341904\n",
      "Step: 384\tloss: 885.341837\n",
      "Step: 385\tloss: 885.341797\n",
      "Step: 386\tloss: 885.341796\n",
      "Step: 387\tloss: 885.341790\n",
      "Step: 388\tloss: 885.341757\n",
      "Step: 389\tloss: 885.341727\n",
      "Step: 390\tloss: 885.341722\n",
      "pval: 0.000042\n",
      "Step: 391\tloss: 885.341712\n",
      "Step: 392\tloss: 885.341688\n",
      "Step: 393\tloss: 885.341676\n",
      "Step: 394\tloss: 885.341676\n",
      "Step: 395\tloss: 885.341667\n",
      "Step: 396\tloss: 885.341650\n",
      "Step: 397\tloss: 885.341639\n",
      "Step: 398\tloss: 885.341637\n",
      "Step: 399\tloss: 885.341634\n",
      "Step: 400\tloss: 885.341625\n",
      "pval: 0.000023\n",
      "Step: 401\tloss: 885.341618\n",
      "Step: 402\tloss: 885.341616\n",
      "Step: 403\tloss: 885.341612\n",
      "Step: 404\tloss: 885.341605\n",
      "Step: 405\tloss: 885.341603\n",
      "Step: 406\tloss: 885.341602\n",
      "Step: 407\tloss: 885.341598\n",
      "Step: 408\tloss: 885.341594\n",
      "Step: 409\tloss: 885.341593\n",
      "Step: 410\tloss: 885.341591\n",
      "pval: 0.000022\n",
      "Step: 411\tloss: 885.341588\n",
      "Step: 412\tloss: 885.341587\n",
      "Step: 413\tloss: 885.341586\n",
      "Step: 414\tloss: 885.341584\n",
      "Step: 415\tloss: 885.341583\n",
      "Step: 416\tloss: 885.341582\n",
      "Step: 417\tloss: 885.341581\n",
      "Step: 418\tloss: 885.341580\n",
      "Step: 419\tloss: 885.341579\n",
      "Step: 420\tloss: 885.341578\n",
      "pval: 0.000017\n",
      "Step: 421\tloss: 885.341577\n",
      "Step: 422\tloss: 885.341577\n",
      "Step: 423\tloss: 885.341576\n",
      "Step: 424\tloss: 885.341576\n",
      "Step: 425\tloss: 885.341576\n",
      "Step: 426\tloss: 885.341575\n",
      "Step: 427\tloss: 885.341575\n",
      "Step: 428\tloss: 885.341574\n",
      "Step: 429\tloss: 885.341574\n",
      "Step: 430\tloss: 885.341574\n",
      "pval: 0.000014\n",
      "Step: 431\tloss: 885.341574\n",
      "Step: 432\tloss: 885.341573\n",
      "Step: 433\tloss: 885.341573\n",
      "Step: 434\tloss: 885.341573\n",
      "Step: 435\tloss: 885.341573\n",
      "Step: 436\tloss: 885.341573\n",
      "Step: 437\tloss: 885.341572\n",
      "Step: 438\tloss: 885.341572\n",
      "Step: 439\tloss: 885.341572\n",
      "Step: 440\tloss: 885.341572\n",
      "pval: 0.000015\n",
      "Step: 441\tloss: 885.341572\n",
      "Step: 442\tloss: 885.341572\n",
      "Step: 443\tloss: 885.341572\n",
      "Step: 444\tloss: 885.341572\n",
      "Step: 445\tloss: 885.341572\n",
      "Step: 446\tloss: 885.341572\n",
      "Step: 447\tloss: 885.341572\n",
      "Step: 448\tloss: 885.341572\n",
      "Step: 449\tloss: 885.341572\n",
      "Step: 450\tloss: 885.341572\n",
      "pval: 0.000022\n",
      "Step: 451\tloss: 885.341572\n",
      "Step: 452\tloss: 885.341572\n",
      "Step: 453\tloss: 885.341572\n",
      "Step: 454\tloss: 885.341572\n",
      "Step: 455\tloss: 885.341572\n",
      "Step: 456\tloss: 885.341572\n",
      "Step: 457\tloss: 885.341571\n",
      "Step: 458\tloss: 885.341571\n",
      "Step: 459\tloss: 885.341571\n",
      "Step: 460\tloss: 885.341571\n",
      "pval: 0.000019\n",
      "Step: 461\tloss: 885.341571\n",
      "Step: 462\tloss: 885.341571\n",
      "Step: 463\tloss: 885.341571\n",
      "Step: 464\tloss: 885.341571\n",
      "Step: 465\tloss: 885.341571\n",
      "Step: 466\tloss: 885.341571\n",
      "Step: 467\tloss: 885.341571\n",
      "Step: 468\tloss: 885.341571\n",
      "Step: 469\tloss: 885.341571\n",
      "Step: 470\tloss: 885.341571\n",
      "pval: 0.000017\n",
      "Step: 471\tloss: 885.341571\n",
      "Step: 472\tloss: 885.341571\n",
      "Step: 473\tloss: 885.341571\n",
      "Step: 474\tloss: 885.341571\n",
      "Step: 475\tloss: 885.341571\n",
      "Step: 476\tloss: 885.341571\n",
      "Step: 477\tloss: 885.341571\n",
      "Step: 478\tloss: 885.341571\n",
      "Step: 479\tloss: 885.341571\n",
      "Step: 480\tloss: 885.341571\n",
      "pval: 0.000024\n",
      "Step: 481\tloss: 885.341571\n",
      "Step: 482\tloss: 885.341571\n",
      "Step: 483\tloss: 885.341571\n",
      "Step: 484\tloss: 885.341571\n",
      "Step: 485\tloss: 885.341571\n",
      "Step: 486\tloss: 885.341571\n",
      "Step: 487\tloss: 885.341571\n",
      "Step: 488\tloss: 885.341571\n",
      "Step: 489\tloss: 885.341571\n",
      "Step: 490\tloss: 885.341571\n",
      "pval: 0.000018\n",
      "Step: 491\tloss: 885.341571\n",
      "Step: 492\tloss: 885.341571\n",
      "Step: 493\tloss: 885.341571\n",
      "Step: 494\tloss: 885.341571\n",
      "Step: 495\tloss: 885.341571\n",
      "Step: 496\tloss: 885.341571\n",
      "Step: 497\tloss: 885.341571\n",
      "Step: 498\tloss: 885.341571\n",
      "Step: 499\tloss: 885.341571\n",
      "Step: 500\tloss: 885.341571\n",
      "pval: 0.000013\n",
      "Step: 501\tloss: 885.341571\n",
      "Step: 502\tloss: 885.341571\n",
      "Step: 503\tloss: 885.341571\n",
      "Step: 504\tloss: 885.341571\n",
      "Step: 505\tloss: 885.341571\n",
      "Step: 506\tloss: 885.341571\n",
      "Step: 507\tloss: 885.341571\n",
      "Step: 508\tloss: 885.341571\n",
      "Step: 509\tloss: 885.341571\n",
      "Step: 510\tloss: 885.341571\n",
      "pval: 0.000025\n",
      "Step: 511\tloss: 885.341571\n",
      "Step: 512\tloss: 885.341571\n",
      "Step: 513\tloss: 885.341571\n",
      "Step: 514\tloss: 885.341571\n",
      "Step: 515\tloss: 885.341571\n",
      "Step: 516\tloss: 885.341571\n",
      "Step: 517\tloss: 885.341571\n",
      "Step: 518\tloss: 885.341571\n",
      "Step: 519\tloss: 885.341571\n",
      "Step: 520\tloss: 885.341571\n",
      "pval: 0.000031\n",
      "Step: 521\tloss: 885.341571\n",
      "Step: 522\tloss: 885.341571\n",
      "Step: 523\tloss: 885.341571\n",
      "Step: 524\tloss: 885.341571\n",
      "Step: 525\tloss: 885.341571\n",
      "Step: 526\tloss: 885.341571\n",
      "Step: 527\tloss: 885.341571\n",
      "Step: 528\tloss: 885.341571\n",
      "Step: 529\tloss: 885.341571\n",
      "Step: 530\tloss: 885.341571\n",
      "pval: 0.000022\n",
      "Step: 531\tloss: 885.341571\n",
      "Step: 532\tloss: 885.341571\n",
      "Step: 533\tloss: 885.341571\n",
      "Step: 534\tloss: 885.341571\n",
      "Step: 535\tloss: 885.341571\n",
      "Step: 536\tloss: 885.341571\n",
      "Step: 537\tloss: 885.341571\n",
      "Step: 538\tloss: 885.341571\n",
      "Step: 539\tloss: 885.341571\n",
      "Step: 540\tloss: 885.341571\n",
      "pval: 0.000057\n",
      "Step: 541\tloss: 885.341571\n",
      "Step: 542\tloss: 885.341571\n",
      "Step: 543\tloss: 885.341571\n",
      "Step: 544\tloss: 885.341571\n",
      "Step: 545\tloss: 885.341571\n",
      "Step: 546\tloss: 885.341571\n",
      "Step: 547\tloss: 885.341571\n",
      "Step: 548\tloss: 885.341571\n",
      "Step: 549\tloss: 885.341571\n",
      "Step: 550\tloss: 885.341571\n",
      "pval: 0.000130\n",
      "Step: 551\tloss: 885.341571\n",
      "Step: 552\tloss: 885.341571\n",
      "Step: 553\tloss: 885.341571\n",
      "Step: 554\tloss: 885.341571\n",
      "Step: 555\tloss: 885.341571\n",
      "Step: 556\tloss: 885.341571\n",
      "Step: 557\tloss: 885.341571\n",
      "Step: 558\tloss: 885.341571\n",
      "Step: 559\tloss: 885.341571\n",
      "Step: 560\tloss: 885.341571\n",
      "pval: 0.005979\n",
      "Step: 561\tloss: 885.341571\n",
      "Step: 562\tloss: 885.341571\n",
      "Step: 563\tloss: 885.341571\n",
      "Step: 564\tloss: 885.341571\n",
      "Step: 565\tloss: 885.341571\n",
      "Step: 566\tloss: 885.341571\n",
      "Step: 567\tloss: 885.341571\n",
      "Step: 568\tloss: 885.341571\n",
      "Step: 569\tloss: 885.341571\n",
      "Step: 570\tloss: 885.341571\n",
      "pval: 0.006562\n",
      "Step: 571\tloss: 885.341571\n",
      "Step: 572\tloss: 885.341571\n",
      "Step: 573\tloss: 885.341571\n",
      "Step: 574\tloss: 885.341571\n",
      "Step: 575\tloss: 885.341571\n",
      "Step: 576\tloss: 885.341571\n",
      "Step: 577\tloss: 885.341571\n",
      "Step: 578\tloss: 885.341571\n",
      "Step: 579\tloss: 885.341571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 580\tloss: 885.341571\n",
      "pval: 0.487771\n",
      "Training sequence #2 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence(\"AUTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results\n",
    "\n",
    "The fitted parameters can be retrieved by calling the corresponding parameters of `estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_loc_params: 5, features: 100)>\n",
       "array([[ 8.799687,  8.118509,  8.956213, ...,  8.942947,  9.032373,  8.936549],\n",
       "       [ 0.134017,  0.537621,  0.698733, ...,  0.445298,  0.089941,  0.196693],\n",
       "       [-0.338728,  0.34282 ,  0.580924, ..., -0.5689  ,  0.572242,  0.673857],\n",
       "       [ 0.663064,  0.685352,  0.225317, ...,  0.242009,  0.690999, -0.132891],\n",
       "       [ 0.078703,  0.474199, -0.641201, ...,  0.035527,  0.154744, -0.284305]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "    feature_allzero    (features) bool False False False False False False ...\n",
       "  * features           (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_scale_params: 5, features: 100)>\n",
       "array([[ 2.159152, -0.021516,  1.802005, ...,  1.150372,  1.457607,  2.132369],\n",
       "       [ 0.062396, -0.060727,  0.507236, ...,  0.408981, -0.545177,  0.453194],\n",
       "       [ 0.205858,  0.271733,  0.537328, ..., -0.007023, -0.102717, -0.073561],\n",
       "       [ 0.445841, -0.246   , -0.060423, ..., -0.507981, -0.412319,  0.505199],\n",
       "       [-0.191027, -0.240782, -0.100181, ...,  0.192562,  0.01396 , -0.389601]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "    feature_allzero      (features) bool False False False False False False ...\n",
       "  * features             (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc, sim.par_link_loc)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.07\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (batchglm)",
   "language": "python",
   "name": "batchglm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
