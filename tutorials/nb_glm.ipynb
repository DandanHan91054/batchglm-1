{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)\n",
    "sim.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[ 8254,  3672,  7229, ...,  5280,  4970,  2834],\n",
       "       [ 3572,  2617,  2694, ...,  7652, 14224,  7996],\n",
       "       [ 5296,  1181,  7092, ..., 13695, 11411,  8449],\n",
       "       ...,\n",
       "       [18975, 27843,  1542, ..., 17310, 19900,  4840],\n",
       "       [11797,  2158,  5167, ..., 14480, 12826,  6834],\n",
       "       [31153,  2301,  1394, ..., 10736, 16736,  4827]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_scale, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters used to generate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'a' (design_loc_params: 5, features: 100)>\n",
       "array([[ 8.806215,  8.083441,  8.958677, ...,  8.969354,  9.082994,  8.942818],\n",
       "       [ 0.115385,  0.650169,  0.676358, ...,  0.417252,  0.072147,  0.18195 ],\n",
       "       [-0.337574,  0.362238,  0.591211, ..., -0.56396 ,  0.536657,  0.626756],\n",
       "       [ 0.6179  ,  0.681902,  0.228461, ...,  0.215373,  0.615605, -0.14967 ],\n",
       "       [ 0.07716 ,  0.439423, -0.636067, ...,  0.017376,  0.134838, -0.273974]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'b' (design_scale_params: 5, features: 100)>\n",
       "array([[ 2.197225,  0.      ,  1.791759, ...,  1.098612,  1.386294,  2.197225],\n",
       "       [ 0.035024, -0.157254,  0.467061, ...,  0.54539 , -0.552892,  0.422423],\n",
       "       [ 0.254288,  0.260412,  0.649432, ...,  0.105473, -0.117756, -0.043436],\n",
       "       [ 0.453221, -0.189692, -0.017939, ..., -0.419644, -0.31896 ,  0.360347],\n",
       "       [-0.277178, -0.259578, -0.107434, ...,  0.104903,  0.070054, -0.399334]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = sim.design_loc\n",
    "design_scale = sim.design_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(data=X, design_loc=design_loc, design_scale=design_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "RMSE of closed-form mean:\n",
      "[1.21777045e-04 1.51540194e-03 5.47320002e-04 3.70358114e-03\n",
      " 1.16360710e-03 1.26262986e-02 2.13019045e-04 6.97138934e-03\n",
      " 1.77815910e-03 2.26710994e-04 1.15609775e-03 2.51648449e-04\n",
      " 1.41579701e-03 4.93907164e-04 1.18967855e-04 6.93461858e-04\n",
      " 1.18602169e-03 2.10866936e-03 3.20415021e-03 3.57594707e-04\n",
      " 2.75920184e-03 3.46467232e-04 3.16634623e-03 1.12906172e-03\n",
      " 4.49988536e-03 7.72149491e-03 1.13673603e-03 1.33255063e-03\n",
      " 1.08078349e-04 5.88724731e-03 1.55051888e-02 1.04069028e-03\n",
      " 3.37859656e-04 2.23879080e-03 4.55242838e-04 8.41768549e-03\n",
      " 1.28355095e-03 5.97037823e-04 3.61962280e-03 6.94334802e-04\n",
      " 4.93019376e-04 4.61909142e-04 1.13183544e-02 2.10359005e-03\n",
      " 1.56217164e-03 4.51902069e-04 1.04927533e-03 1.56825573e-03\n",
      " 2.05945749e-03 1.45147367e-03 6.77560596e-04 2.13429478e-03\n",
      " 5.05332923e-04 1.00493856e-03 1.70805258e-03 2.54676581e-03\n",
      " 1.21753341e-03 5.14141976e-03 2.55480790e-03 7.94424344e-04\n",
      " 1.27736547e-03 1.20192957e-04 1.65402818e-03 7.86708778e-04\n",
      " 8.46310334e-05 1.51383529e-03 6.43679503e-03 5.76779394e-03\n",
      " 2.03335088e-03 1.99461298e-02 5.61429233e-04 1.36123483e-03\n",
      " 2.74039453e-03 6.43798539e-04 2.80987850e-04 1.36724784e-04\n",
      " 1.18175208e-02 1.64143626e-03 1.36464284e-04 4.08249451e-04\n",
      " 8.16754478e-04 1.98121846e-03 1.65755005e-03 6.10166318e-03\n",
      " 2.31143535e-03 1.57847577e-03 1.01143906e-02 1.62772280e-04\n",
      " 1.01067871e-03 1.13378080e-02 1.36075332e-02 4.03482587e-03\n",
      " 1.46298472e-03 2.01416061e-03 3.45626524e-03 5.25829497e-03\n",
      " 5.77073594e-04 3.50281269e-03 6.75201200e-04 1.83402610e-03]\n",
      "Should train mu: True\n",
      "Using closed-form MME initialization for dispersion\n",
      "RMSE of closed-form dispersion:\n",
      "[0.01287664 0.04994481 0.06191098 0.02255841 0.03324018 0.00599477\n",
      " 0.01140436 0.03190672 0.07026999 0.02096582 0.01080945 0.0076364\n",
      " 0.03581355 0.04373168 0.02438652 0.04688681 0.0194309  0.00309402\n",
      " 0.04134449 0.06069283 0.04321127 0.0097485  0.02378122 0.07492138\n",
      " 0.01761236 0.00922304 0.04619224 0.03100436 0.0235271  0.0017943\n",
      " 0.1080419  0.01973455 0.01960148 0.00250855 0.05799934 0.04795767\n",
      " 0.07942515 0.03730298 0.03088631 0.01565974 0.01092115 0.00871206\n",
      " 0.02874796 0.01391591 0.03657679 0.00970466 0.02851762 0.05237752\n",
      " 0.09039323 0.04225993 0.03699762 0.01908542 0.01023065 0.01964059\n",
      " 0.04211861 0.00682695 0.01232376 0.07077296 0.00669901 0.01192206\n",
      " 0.00142929 0.01091504 0.04497585 0.04379757 0.02482964 0.03261666\n",
      " 0.04020535 0.04188679 0.00206018 0.08920832 0.04198381 0.01483758\n",
      " 0.08794092 0.02259121 0.00398129 0.01437305 0.11775262 0.05304488\n",
      " 0.0125809  0.0116098  0.02921739 0.00976936 0.00615631 0.01590012\n",
      " 0.00734133 0.01459689 0.03592817 0.01676457 0.00969331 0.04633008\n",
      " 0.01939847 0.05433613 0.00865478 0.03391115 0.00337652 0.02724792\n",
      " 0.0307377  0.01248376 0.00652125 0.06693044]\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train:\n",
    "\n",
    "There are multiple possible training strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTO\n",
      "DEFAULT\n",
      "EXACT\n",
      "QUICK\n",
      "PRE_INITIALIZED\n"
     ]
    }
   ],
   "source": [
    "for i in estimator.TrainingStrategy:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one of them corresponds to a list of training options which will be passed to the estimator.train() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(estimator.TrainingStrategy.DEFAULT.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, when choosing the training strategy \"DEFAULT\", the following call:\n",
    "```python\n",
    "estimator.train_sequence(\"DEFAULT\")\n",
    "```\n",
    "is equal to:\n",
    "```python\n",
    "estimator.train_sequence(estimator.TrainingStrategy.DEFAULT)\n",
    "```\n",
    "is equal to:\n",
    "```python\n",
    "estimator.train(\n",
    "    convergence_criteria = 't_test',\n",
    "    learning_rate = 0.1,\n",
    "    loss_window_size = 100,\n",
    "    optim_algo = 'ADAM',\n",
    "    stop_at_loss_change = 0.05,\n",
    "    use_batching = True\n",
    ")\n",
    "estimator.train(\n",
    "    convergence_criteria = 't_test',\n",
    "    learning_rate = 0.05,\n",
    "    loss_window_size = 10,\n",
    "    optim_algo = 'GD',\n",
    "    stop_at_loss_change = 0.05,\n",
    "    use_batching = False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 885.513913\n",
      "Step: 2\tloss: 898.976127\n",
      "Step: 3\tloss: 889.199063\n",
      "Step: 4\tloss: 890.597800\n",
      "Step: 5\tloss: 890.676163\n",
      "Step: 6\tloss: 890.348473\n",
      "Step: 7\tloss: 888.962466\n",
      "Step: 8\tloss: 888.290230\n",
      "Step: 9\tloss: 888.311687\n",
      "Step: 10\tloss: 889.301279\n",
      "Step: 11\tloss: 887.976396\n",
      "Step: 12\tloss: 887.533220\n",
      "Step: 13\tloss: 887.237896\n",
      "Step: 14\tloss: 887.413583\n",
      "Step: 15\tloss: 887.046304\n",
      "Step: 16\tloss: 886.387423\n",
      "Step: 17\tloss: 887.318333\n",
      "Step: 18\tloss: 887.345315\n",
      "Step: 19\tloss: 886.430518\n",
      "Step: 20\tloss: 885.893532\n",
      "Step: 21\tloss: 886.252933\n",
      "Step: 22\tloss: 887.000912\n",
      "Step: 23\tloss: 886.164973\n",
      "Step: 24\tloss: 885.516419\n",
      "Step: 25\tloss: 885.399947\n",
      "Step: 26\tloss: 886.760356\n",
      "Step: 27\tloss: 886.788990\n",
      "Step: 28\tloss: 885.502744\n",
      "Step: 29\tloss: 886.114336\n",
      "Step: 30\tloss: 885.763258\n",
      "Step: 31\tloss: 885.582171\n",
      "Step: 32\tloss: 886.363210\n",
      "Step: 33\tloss: 886.464011\n",
      "Step: 34\tloss: 886.208091\n",
      "Step: 35\tloss: 885.418267\n",
      "Step: 36\tloss: 885.248987\n",
      "Step: 37\tloss: 886.098450\n",
      "Step: 38\tloss: 886.443648\n",
      "Step: 39\tloss: 885.249253\n",
      "Step: 40\tloss: 885.207692\n",
      "Step: 41\tloss: 886.275501\n",
      "Step: 42\tloss: 885.892441\n",
      "Step: 43\tloss: 885.101431\n",
      "Step: 44\tloss: 885.532208\n",
      "Step: 45\tloss: 885.189953\n",
      "Step: 46\tloss: 886.483929\n",
      "Step: 47\tloss: 885.844448\n",
      "Step: 48\tloss: 885.288976\n",
      "Step: 49\tloss: 886.033752\n",
      "Step: 50\tloss: 885.877834\n",
      "Step: 51\tloss: 885.262609\n",
      "Step: 52\tloss: 885.564448\n",
      "Step: 53\tloss: 886.809548\n",
      "Step: 54\tloss: 885.651337\n",
      "Step: 55\tloss: 884.964231\n",
      "Step: 56\tloss: 885.331178\n",
      "Step: 57\tloss: 885.712226\n",
      "Step: 58\tloss: 885.790924\n",
      "Step: 59\tloss: 885.546945\n",
      "Step: 60\tloss: 885.706013\n",
      "Step: 61\tloss: 885.318294\n",
      "Step: 62\tloss: 886.700413\n",
      "Step: 63\tloss: 885.356035\n",
      "Step: 64\tloss: 885.256497\n",
      "Step: 65\tloss: 885.957041\n",
      "Step: 66\tloss: 886.226514\n",
      "Step: 67\tloss: 884.953874\n",
      "Step: 68\tloss: 885.713630\n",
      "Step: 69\tloss: 886.284824\n",
      "Step: 70\tloss: 886.811733\n",
      "Step: 71\tloss: 884.916732\n",
      "Step: 72\tloss: 884.731114\n",
      "Step: 73\tloss: 885.726330\n",
      "Step: 74\tloss: 886.574139\n",
      "Step: 75\tloss: 885.889522\n",
      "Step: 76\tloss: 884.555865\n",
      "Step: 77\tloss: 886.353722\n",
      "Step: 78\tloss: 886.097547\n",
      "Step: 79\tloss: 885.073838\n",
      "Step: 80\tloss: 885.265330\n",
      "Step: 81\tloss: 885.375273\n",
      "Step: 82\tloss: 886.106321\n",
      "Step: 83\tloss: 885.303383\n",
      "Step: 84\tloss: 886.018753\n",
      "Step: 85\tloss: 886.055232\n",
      "Step: 86\tloss: 886.678678\n",
      "Step: 87\tloss: 885.249728\n",
      "Step: 88\tloss: 884.957463\n",
      "Step: 89\tloss: 885.678425\n",
      "Step: 90\tloss: 886.143267\n",
      "Step: 91\tloss: 885.378764\n",
      "Step: 92\tloss: 885.577179\n",
      "Step: 93\tloss: 886.469197\n",
      "Step: 94\tloss: 886.262913\n",
      "Step: 95\tloss: 884.829433\n",
      "Step: 96\tloss: 885.346466\n",
      "Step: 97\tloss: 886.054807\n",
      "Step: 98\tloss: 886.340502\n",
      "Step: 99\tloss: 886.013304\n",
      "Step: 100\tloss: 884.429521\n",
      "Step: 101\tloss: 885.868947\n",
      "Step: 102\tloss: 885.997614\n",
      "Step: 103\tloss: 885.704543\n",
      "Step: 104\tloss: 885.265192\n",
      "Step: 105\tloss: 886.423887\n",
      "Step: 106\tloss: 885.825850\n",
      "Step: 107\tloss: 885.305626\n",
      "Step: 108\tloss: 885.315901\n",
      "Step: 109\tloss: 885.820695\n",
      "Step: 110\tloss: 886.583452\n",
      "Step: 111\tloss: 885.041982\n",
      "Step: 112\tloss: 885.354619\n",
      "Step: 113\tloss: 886.334991\n",
      "Step: 114\tloss: 885.497668\n",
      "Step: 115\tloss: 885.349627\n",
      "Step: 116\tloss: 885.691297\n",
      "Step: 117\tloss: 885.816976\n",
      "Step: 118\tloss: 885.255328\n",
      "Step: 119\tloss: 886.065183\n",
      "Step: 120\tloss: 885.723165\n",
      "Step: 121\tloss: 885.493758\n",
      "Step: 122\tloss: 886.492555\n",
      "Step: 123\tloss: 885.572141\n",
      "Step: 124\tloss: 885.470284\n",
      "Step: 125\tloss: 886.451076\n",
      "Step: 126\tloss: 886.147512\n",
      "Step: 127\tloss: 884.991307\n",
      "Step: 128\tloss: 885.663093\n",
      "Step: 129\tloss: 886.015598\n",
      "Step: 130\tloss: 886.735470\n",
      "Step: 131\tloss: 886.158062\n",
      "Step: 132\tloss: 884.284670\n",
      "Step: 133\tloss: 886.121103\n",
      "Step: 134\tloss: 885.530606\n",
      "Step: 135\tloss: 885.417590\n",
      "Step: 136\tloss: 886.106759\n",
      "Step: 137\tloss: 886.079245\n",
      "Step: 138\tloss: 886.153492\n",
      "Step: 139\tloss: 885.351173\n",
      "Step: 140\tloss: 885.595391\n",
      "Step: 141\tloss: 886.815123\n",
      "Step: 142\tloss: 885.684422\n",
      "Step: 143\tloss: 885.243644\n",
      "Step: 144\tloss: 885.508355\n",
      "Step: 145\tloss: 885.637982\n",
      "Step: 146\tloss: 886.743502\n",
      "Step: 147\tloss: 884.531085\n",
      "Step: 148\tloss: 886.362005\n",
      "Step: 149\tloss: 886.468905\n",
      "Step: 150\tloss: 885.942340\n",
      "Step: 151\tloss: 885.410312\n",
      "Step: 152\tloss: 885.270253\n",
      "Step: 153\tloss: 885.911051\n",
      "Step: 154\tloss: 886.351204\n",
      "Step: 155\tloss: 885.055822\n",
      "Step: 156\tloss: 885.875476\n",
      "Step: 157\tloss: 887.217439\n",
      "Step: 158\tloss: 886.581170\n",
      "Step: 159\tloss: 884.963734\n",
      "Step: 160\tloss: 884.641726\n",
      "Step: 161\tloss: 886.638848\n",
      "Step: 162\tloss: 885.452679\n",
      "Step: 163\tloss: 885.929797\n",
      "Step: 164\tloss: 885.463053\n",
      "Step: 165\tloss: 886.248543\n",
      "Step: 166\tloss: 885.916307\n",
      "Step: 167\tloss: 885.658364\n",
      "Step: 168\tloss: 885.635794\n",
      "Step: 169\tloss: 885.938015\n",
      "Step: 170\tloss: 886.861065\n",
      "Step: 171\tloss: 884.751652\n",
      "Step: 172\tloss: 886.069397\n",
      "Step: 173\tloss: 885.830983\n",
      "Step: 174\tloss: 886.160683\n",
      "Step: 175\tloss: 886.575279\n",
      "Step: 176\tloss: 884.883997\n",
      "Step: 177\tloss: 886.074439\n",
      "Step: 178\tloss: 886.418480\n",
      "Step: 179\tloss: 885.074489\n",
      "Step: 180\tloss: 885.835972\n",
      "Step: 181\tloss: 885.944050\n",
      "Step: 182\tloss: 885.944512\n",
      "Step: 183\tloss: 885.703785\n",
      "Step: 184\tloss: 885.736206\n",
      "Step: 185\tloss: 886.809619\n",
      "Step: 186\tloss: 885.617069\n",
      "Step: 187\tloss: 885.398756\n",
      "Step: 188\tloss: 885.315971\n",
      "Step: 189\tloss: 885.961132\n",
      "Step: 190\tloss: 886.040078\n",
      "Step: 191\tloss: 885.374905\n",
      "Step: 192\tloss: 885.768148\n",
      "Step: 193\tloss: 887.174846\n",
      "Step: 194\tloss: 886.334036\n",
      "Step: 195\tloss: 885.489855\n",
      "Step: 196\tloss: 884.286067\n",
      "Step: 197\tloss: 886.643171\n",
      "Step: 198\tloss: 885.588283\n",
      "Step: 199\tloss: 885.284479\n",
      "Step: 200\tloss: 885.739223\n",
      "pval: 0.002675\n",
      "Step: 201\tloss: 885.948788\n",
      "Step: 202\tloss: 885.682231\n",
      "Step: 203\tloss: 885.886967\n",
      "Step: 204\tloss: 885.753952\n",
      "Step: 205\tloss: 884.974427\n",
      "Step: 206\tloss: 886.070284\n",
      "Step: 207\tloss: 885.693461\n",
      "Step: 208\tloss: 886.620687\n",
      "Step: 209\tloss: 885.790585\n",
      "Step: 210\tloss: 885.956992\n",
      "Step: 211\tloss: 885.330953\n",
      "Step: 212\tloss: 886.263774\n",
      "Step: 213\tloss: 886.486785\n",
      "Step: 214\tloss: 886.211040\n",
      "Step: 215\tloss: 886.172991\n",
      "Step: 216\tloss: 884.511788\n",
      "Step: 217\tloss: 886.354746\n",
      "Step: 218\tloss: 886.098930\n",
      "Step: 219\tloss: 885.956295\n",
      "Step: 220\tloss: 884.922427\n",
      "Step: 221\tloss: 886.771447\n",
      "Step: 222\tloss: 885.793176\n",
      "Step: 223\tloss: 885.434632\n",
      "Step: 224\tloss: 885.404149\n",
      "Step: 225\tloss: 885.901289\n",
      "Step: 226\tloss: 886.618143\n",
      "Step: 227\tloss: 885.639669\n",
      "Step: 228\tloss: 885.158897\n",
      "Step: 229\tloss: 886.321403\n",
      "Step: 230\tloss: 885.955478\n",
      "Step: 231\tloss: 886.115012\n",
      "Step: 232\tloss: 885.053180\n",
      "Step: 233\tloss: 886.287690\n",
      "Step: 234\tloss: 886.380863\n",
      "Step: 235\tloss: 884.763533\n",
      "Step: 236\tloss: 885.920627\n",
      "Step: 237\tloss: 885.795755\n",
      "Step: 238\tloss: 885.829053\n",
      "Step: 239\tloss: 885.710795\n",
      "Step: 240\tloss: 885.959266\n",
      "Step: 241\tloss: 886.362389\n",
      "Step: 242\tloss: 886.398098\n",
      "Step: 243\tloss: 885.329415\n",
      "Step: 244\tloss: 885.153628\n",
      "Step: 245\tloss: 886.251854\n",
      "Step: 246\tloss: 886.116789\n",
      "Step: 247\tloss: 885.322891\n",
      "Step: 248\tloss: 885.664218\n",
      "Step: 249\tloss: 885.808849\n",
      "Step: 250\tloss: 886.125211\n",
      "Step: 251\tloss: 885.969102\n",
      "Step: 252\tloss: 885.498382\n",
      "Step: 253\tloss: 886.169459\n",
      "Step: 254\tloss: 886.116687\n",
      "Step: 255\tloss: 885.933655\n",
      "Step: 256\tloss: 885.295253\n",
      "Step: 257\tloss: 886.252351\n",
      "Step: 258\tloss: 885.864638\n",
      "Step: 259\tloss: 886.302497\n",
      "Step: 260\tloss: 885.132850\n",
      "Step: 261\tloss: 885.615931\n",
      "Step: 262\tloss: 885.914465\n",
      "Step: 263\tloss: 886.255193\n",
      "Step: 264\tloss: 885.638945\n",
      "Step: 265\tloss: 886.148274\n",
      "Step: 266\tloss: 885.749491\n",
      "Step: 267\tloss: 885.975465\n",
      "Step: 268\tloss: 885.535104\n",
      "Step: 269\tloss: 886.247605\n",
      "Step: 270\tloss: 885.772516\n",
      "Step: 271\tloss: 886.322528\n",
      "Step: 272\tloss: 885.159781\n",
      "Step: 273\tloss: 887.329925\n",
      "Step: 274\tloss: 885.770240\n",
      "Step: 275\tloss: 885.181141\n",
      "Step: 276\tloss: 885.193664\n",
      "Step: 277\tloss: 886.593078\n",
      "Step: 278\tloss: 885.258466\n",
      "Step: 279\tloss: 886.057974\n",
      "Step: 280\tloss: 885.514242\n",
      "Step: 281\tloss: 886.053365\n",
      "Step: 282\tloss: 886.790033\n",
      "Step: 283\tloss: 885.284597\n",
      "Step: 284\tloss: 885.162204\n",
      "Step: 285\tloss: 886.155802\n",
      "Step: 286\tloss: 886.059330\n",
      "Step: 287\tloss: 885.626334\n",
      "Step: 288\tloss: 885.546485\n",
      "Step: 289\tloss: 886.365841\n",
      "Step: 290\tloss: 886.993566\n",
      "Step: 291\tloss: 884.919083\n",
      "Step: 292\tloss: 885.184196\n",
      "Step: 293\tloss: 886.662418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 294\tloss: 885.970917\n",
      "Step: 295\tloss: 885.249642\n",
      "Step: 296\tloss: 885.652558\n",
      "Step: 297\tloss: 886.476882\n",
      "Step: 298\tloss: 885.302112\n",
      "Step: 299\tloss: 885.914032\n",
      "Step: 300\tloss: 886.021348\n",
      "pval: 0.754031\n",
      "Training sequence #1 complete\n",
      "Beginning with training sequence #2\n",
      "Step: 301\tloss: 885.754197\n",
      "Step: 302\tloss: 888.480427\n",
      "Step: 303\tloss: 886.006512\n",
      "Step: 304\tloss: 886.239537\n",
      "Step: 305\tloss: 886.844229\n",
      "Step: 306\tloss: 886.370105\n",
      "Step: 307\tloss: 885.765668\n",
      "Step: 308\tloss: 885.759317\n",
      "Step: 309\tloss: 886.065760\n",
      "Step: 310\tloss: 886.104541\n",
      "Step: 311\tloss: 885.814132\n",
      "Step: 312\tloss: 885.549854\n",
      "Step: 313\tloss: 885.549775\n",
      "Step: 314\tloss: 885.693918\n",
      "Step: 315\tloss: 885.740031\n",
      "Step: 316\tloss: 885.641682\n",
      "Step: 317\tloss: 885.530796\n",
      "Step: 318\tloss: 885.506574\n",
      "Step: 319\tloss: 885.538244\n",
      "Step: 320\tloss: 885.544845\n",
      "pval: 0.008181\n",
      "Step: 321\tloss: 885.503336\n",
      "Step: 322\tloss: 885.458179\n",
      "Step: 323\tloss: 885.447766\n",
      "Step: 324\tloss: 885.458214\n",
      "Step: 325\tloss: 885.457418\n",
      "Step: 326\tloss: 885.441182\n",
      "Step: 327\tloss: 885.426395\n",
      "Step: 328\tloss: 885.418776\n",
      "Step: 329\tloss: 885.410001\n",
      "Step: 330\tloss: 885.398626\n",
      "pval: 0.000192\n",
      "Step: 331\tloss: 885.393239\n",
      "Step: 332\tloss: 885.394559\n",
      "Step: 333\tloss: 885.392451\n",
      "Step: 334\tloss: 885.383805\n",
      "Step: 335\tloss: 885.376924\n",
      "Step: 336\tloss: 885.375964\n",
      "Step: 337\tloss: 885.375375\n",
      "Step: 338\tloss: 885.371030\n",
      "Step: 339\tloss: 885.365597\n",
      "Step: 340\tloss: 885.362429\n",
      "pval: 0.000018\n",
      "Step: 341\tloss: 885.361516\n",
      "Step: 342\tloss: 885.361290\n",
      "Step: 343\tloss: 885.360319\n",
      "Step: 344\tloss: 885.357868\n",
      "Step: 345\tloss: 885.354892\n",
      "Step: 346\tloss: 885.353328\n",
      "Step: 347\tloss: 885.353232\n",
      "Step: 348\tloss: 885.352603\n",
      "Step: 349\tloss: 885.350796\n",
      "Step: 350\tloss: 885.349461\n",
      "pval: 0.000020\n",
      "Step: 351\tloss: 885.349426\n",
      "Step: 352\tloss: 885.349332\n",
      "Step: 353\tloss: 885.348120\n",
      "Step: 354\tloss: 885.346683\n",
      "Step: 355\tloss: 885.346220\n",
      "Step: 356\tloss: 885.346402\n",
      "Step: 357\tloss: 885.346193\n",
      "Step: 358\tloss: 885.345539\n",
      "Step: 359\tloss: 885.344999\n",
      "Step: 360\tloss: 885.344619\n",
      "pval: 0.000028\n",
      "Step: 361\tloss: 885.344269\n",
      "Step: 362\tloss: 885.344063\n",
      "Step: 363\tloss: 885.343934\n",
      "Step: 364\tloss: 885.343682\n",
      "Step: 365\tloss: 885.343449\n",
      "Step: 366\tloss: 885.343361\n",
      "Step: 367\tloss: 885.343181\n",
      "Step: 368\tloss: 885.342857\n",
      "Step: 369\tloss: 885.342713\n",
      "Step: 370\tloss: 885.342787\n",
      "pval: 0.000032\n",
      "Step: 371\tloss: 885.342728\n",
      "Step: 372\tloss: 885.342463\n",
      "Step: 373\tloss: 885.342304\n",
      "Step: 374\tloss: 885.342349\n",
      "Step: 375\tloss: 885.342337\n",
      "Step: 376\tloss: 885.342178\n",
      "Step: 377\tloss: 885.342084\n",
      "Step: 378\tloss: 885.342103\n",
      "Step: 379\tloss: 885.342060\n",
      "Step: 380\tloss: 885.341952\n",
      "pval: 0.000014\n",
      "Step: 381\tloss: 885.341917\n",
      "Step: 382\tloss: 885.341934\n",
      "Step: 383\tloss: 885.341904\n",
      "Step: 384\tloss: 885.341837\n",
      "Step: 385\tloss: 885.341797\n",
      "Step: 386\tloss: 885.341796\n",
      "Step: 387\tloss: 885.341790\n",
      "Step: 388\tloss: 885.341757\n",
      "Step: 389\tloss: 885.341727\n",
      "Step: 390\tloss: 885.341722\n",
      "pval: 0.000042\n",
      "Step: 391\tloss: 885.341712\n",
      "Step: 392\tloss: 885.341688\n",
      "Step: 393\tloss: 885.341676\n",
      "Step: 394\tloss: 885.341676\n",
      "Step: 395\tloss: 885.341667\n",
      "Step: 396\tloss: 885.341650\n",
      "Step: 397\tloss: 885.341639\n",
      "Step: 398\tloss: 885.341637\n",
      "Step: 399\tloss: 885.341634\n",
      "Step: 400\tloss: 885.341625\n",
      "pval: 0.000023\n",
      "Step: 401\tloss: 885.341618\n",
      "Step: 402\tloss: 885.341616\n",
      "Step: 403\tloss: 885.341612\n",
      "Step: 404\tloss: 885.341605\n",
      "Step: 405\tloss: 885.341603\n",
      "Step: 406\tloss: 885.341602\n",
      "Step: 407\tloss: 885.341598\n",
      "Step: 408\tloss: 885.341594\n",
      "Step: 409\tloss: 885.341593\n",
      "Step: 410\tloss: 885.341591\n",
      "pval: 0.000022\n",
      "Step: 411\tloss: 885.341588\n",
      "Step: 412\tloss: 885.341587\n",
      "Step: 413\tloss: 885.341586\n",
      "Step: 414\tloss: 885.341584\n",
      "Step: 415\tloss: 885.341583\n",
      "Step: 416\tloss: 885.341582\n",
      "Step: 417\tloss: 885.341581\n",
      "Step: 418\tloss: 885.341580\n",
      "Step: 419\tloss: 885.341579\n",
      "Step: 420\tloss: 885.341578\n",
      "pval: 0.000017\n",
      "Step: 421\tloss: 885.341577\n",
      "Step: 422\tloss: 885.341577\n",
      "Step: 423\tloss: 885.341576\n",
      "Step: 424\tloss: 885.341576\n",
      "Step: 425\tloss: 885.341576\n",
      "Step: 426\tloss: 885.341575\n",
      "Step: 427\tloss: 885.341575\n",
      "Step: 428\tloss: 885.341574\n",
      "Step: 429\tloss: 885.341574\n",
      "Step: 430\tloss: 885.341574\n",
      "pval: 0.000014\n",
      "Step: 431\tloss: 885.341574\n",
      "Step: 432\tloss: 885.341573\n",
      "Step: 433\tloss: 885.341573\n",
      "Step: 434\tloss: 885.341573\n",
      "Step: 435\tloss: 885.341573\n",
      "Step: 436\tloss: 885.341573\n",
      "Step: 437\tloss: 885.341572\n",
      "Step: 438\tloss: 885.341572\n",
      "Step: 439\tloss: 885.341572\n",
      "Step: 440\tloss: 885.341572\n",
      "pval: 0.000015\n",
      "Step: 441\tloss: 885.341572\n",
      "Step: 442\tloss: 885.341572\n",
      "Step: 443\tloss: 885.341572\n",
      "Step: 444\tloss: 885.341572\n",
      "Step: 445\tloss: 885.341572\n",
      "Step: 446\tloss: 885.341572\n",
      "Step: 447\tloss: 885.341572\n",
      "Step: 448\tloss: 885.341572\n",
      "Step: 449\tloss: 885.341572\n",
      "Step: 450\tloss: 885.341572\n",
      "pval: 0.000022\n",
      "Step: 451\tloss: 885.341572\n",
      "Step: 452\tloss: 885.341572\n",
      "Step: 453\tloss: 885.341572\n",
      "Step: 454\tloss: 885.341572\n",
      "Step: 455\tloss: 885.341572\n",
      "Step: 456\tloss: 885.341572\n",
      "Step: 457\tloss: 885.341571\n",
      "Step: 458\tloss: 885.341571\n",
      "Step: 459\tloss: 885.341571\n",
      "Step: 460\tloss: 885.341571\n",
      "pval: 0.000019\n",
      "Step: 461\tloss: 885.341571\n",
      "Step: 462\tloss: 885.341571\n",
      "Step: 463\tloss: 885.341571\n",
      "Step: 464\tloss: 885.341571\n",
      "Step: 465\tloss: 885.341571\n",
      "Step: 466\tloss: 885.341571\n",
      "Step: 467\tloss: 885.341571\n",
      "Step: 468\tloss: 885.341571\n",
      "Step: 469\tloss: 885.341571\n",
      "Step: 470\tloss: 885.341571\n",
      "pval: 0.000017\n",
      "Step: 471\tloss: 885.341571\n",
      "Step: 472\tloss: 885.341571\n",
      "Step: 473\tloss: 885.341571\n",
      "Step: 474\tloss: 885.341571\n",
      "Step: 475\tloss: 885.341571\n",
      "Step: 476\tloss: 885.341571\n",
      "Step: 477\tloss: 885.341571\n",
      "Step: 478\tloss: 885.341571\n",
      "Step: 479\tloss: 885.341571\n",
      "Step: 480\tloss: 885.341571\n",
      "pval: 0.000024\n",
      "Step: 481\tloss: 885.341571\n",
      "Step: 482\tloss: 885.341571\n",
      "Step: 483\tloss: 885.341571\n",
      "Step: 484\tloss: 885.341571\n",
      "Step: 485\tloss: 885.341571\n",
      "Step: 486\tloss: 885.341571\n",
      "Step: 487\tloss: 885.341571\n",
      "Step: 488\tloss: 885.341571\n",
      "Step: 489\tloss: 885.341571\n",
      "Step: 490\tloss: 885.341571\n",
      "pval: 0.000018\n",
      "Step: 491\tloss: 885.341571\n",
      "Step: 492\tloss: 885.341571\n",
      "Step: 493\tloss: 885.341571\n",
      "Step: 494\tloss: 885.341571\n",
      "Step: 495\tloss: 885.341571\n",
      "Step: 496\tloss: 885.341571\n",
      "Step: 497\tloss: 885.341571\n",
      "Step: 498\tloss: 885.341571\n",
      "Step: 499\tloss: 885.341571\n",
      "Step: 500\tloss: 885.341571\n",
      "pval: 0.000013\n",
      "Step: 501\tloss: 885.341571\n",
      "Step: 502\tloss: 885.341571\n",
      "Step: 503\tloss: 885.341571\n",
      "Step: 504\tloss: 885.341571\n",
      "Step: 505\tloss: 885.341571\n",
      "Step: 506\tloss: 885.341571\n",
      "Step: 507\tloss: 885.341571\n",
      "Step: 508\tloss: 885.341571\n",
      "Step: 509\tloss: 885.341571\n",
      "Step: 510\tloss: 885.341571\n",
      "pval: 0.000025\n",
      "Step: 511\tloss: 885.341571\n",
      "Step: 512\tloss: 885.341571\n",
      "Step: 513\tloss: 885.341571\n",
      "Step: 514\tloss: 885.341571\n",
      "Step: 515\tloss: 885.341571\n",
      "Step: 516\tloss: 885.341571\n",
      "Step: 517\tloss: 885.341571\n",
      "Step: 518\tloss: 885.341571\n",
      "Step: 519\tloss: 885.341571\n",
      "Step: 520\tloss: 885.341571\n",
      "pval: 0.000031\n",
      "Step: 521\tloss: 885.341571\n",
      "Step: 522\tloss: 885.341571\n",
      "Step: 523\tloss: 885.341571\n",
      "Step: 524\tloss: 885.341571\n",
      "Step: 525\tloss: 885.341571\n",
      "Step: 526\tloss: 885.341571\n",
      "Step: 527\tloss: 885.341571\n",
      "Step: 528\tloss: 885.341571\n",
      "Step: 529\tloss: 885.341571\n",
      "Step: 530\tloss: 885.341571\n",
      "pval: 0.000022\n",
      "Step: 531\tloss: 885.341571\n",
      "Step: 532\tloss: 885.341571\n",
      "Step: 533\tloss: 885.341571\n",
      "Step: 534\tloss: 885.341571\n",
      "Step: 535\tloss: 885.341571\n",
      "Step: 536\tloss: 885.341571\n",
      "Step: 537\tloss: 885.341571\n",
      "Step: 538\tloss: 885.341571\n",
      "Step: 539\tloss: 885.341571\n",
      "Step: 540\tloss: 885.341571\n",
      "pval: 0.000057\n",
      "Step: 541\tloss: 885.341571\n",
      "Step: 542\tloss: 885.341571\n",
      "Step: 543\tloss: 885.341571\n",
      "Step: 544\tloss: 885.341571\n",
      "Step: 545\tloss: 885.341571\n",
      "Step: 546\tloss: 885.341571\n",
      "Step: 547\tloss: 885.341571\n",
      "Step: 548\tloss: 885.341571\n",
      "Step: 549\tloss: 885.341571\n",
      "Step: 550\tloss: 885.341571\n",
      "pval: 0.000130\n",
      "Step: 551\tloss: 885.341571\n",
      "Step: 552\tloss: 885.341571\n",
      "Step: 553\tloss: 885.341571\n",
      "Step: 554\tloss: 885.341571\n",
      "Step: 555\tloss: 885.341571\n",
      "Step: 556\tloss: 885.341571\n",
      "Step: 557\tloss: 885.341571\n",
      "Step: 558\tloss: 885.341571\n",
      "Step: 559\tloss: 885.341571\n",
      "Step: 560\tloss: 885.341571\n",
      "pval: 0.005979\n",
      "Step: 561\tloss: 885.341571\n",
      "Step: 562\tloss: 885.341571\n",
      "Step: 563\tloss: 885.341571\n",
      "Step: 564\tloss: 885.341571\n",
      "Step: 565\tloss: 885.341571\n",
      "Step: 566\tloss: 885.341571\n",
      "Step: 567\tloss: 885.341571\n",
      "Step: 568\tloss: 885.341571\n",
      "Step: 569\tloss: 885.341571\n",
      "Step: 570\tloss: 885.341571\n",
      "pval: 0.006562\n",
      "Step: 571\tloss: 885.341571\n",
      "Step: 572\tloss: 885.341571\n",
      "Step: 573\tloss: 885.341571\n",
      "Step: 574\tloss: 885.341571\n",
      "Step: 575\tloss: 885.341571\n",
      "Step: 576\tloss: 885.341571\n",
      "Step: 577\tloss: 885.341571\n",
      "Step: 578\tloss: 885.341571\n",
      "Step: 579\tloss: 885.341571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 580\tloss: 885.341571\n",
      "pval: 0.487771\n",
      "Training sequence #2 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence(\"AUTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results\n",
    "\n",
    "The fitted parameters can be retrieved by calling the corresponding parameters of `estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_loc_params: 5, features: 100)>\n",
       "array([[ 8.799687,  8.118509,  8.956213, ...,  8.942947,  9.032373,  8.936549],\n",
       "       [ 0.134017,  0.537621,  0.698733, ...,  0.445298,  0.089941,  0.196693],\n",
       "       [-0.338728,  0.34282 ,  0.580924, ..., -0.5689  ,  0.572242,  0.673857],\n",
       "       [ 0.663064,  0.685352,  0.225317, ...,  0.242009,  0.690999, -0.132891],\n",
       "       [ 0.078703,  0.474199, -0.641201, ...,  0.035527,  0.154744, -0.284305]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "    feature_allzero    (features) bool False False False False False False ...\n",
       "  * features           (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_scale_params: 5, features: 100)>\n",
       "array([[ 2.159152, -0.021516,  1.802005, ...,  1.150372,  1.457607,  2.132369],\n",
       "       [ 0.062396, -0.060727,  0.507236, ...,  0.408981, -0.545177,  0.453194],\n",
       "       [ 0.205858,  0.271733,  0.537328, ..., -0.007023, -0.102717, -0.073561],\n",
       "       [ 0.445841, -0.246   , -0.060423, ..., -0.507981, -0.412319,  0.505199],\n",
       "       [-0.191027, -0.240782, -0.100181, ...,  0.192562,  0.01396 , -0.389601]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "    feature_allzero      (features) bool False False False False False False ...\n",
       "  * features             (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc, sim.par_link_loc)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.07\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
