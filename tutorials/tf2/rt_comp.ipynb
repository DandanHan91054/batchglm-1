{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Runtime Comparisons on Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook checks the performance of feature batching vs standard training. It makes use of data from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5746044/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import warnings\n",
    "import scipy\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm\n",
    "import scanpy as sc\n",
    "\n",
    "np.warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"diffxpy\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '/home/mario/PSC19/'\n",
    "out_dir = '/home/mario/PSC19/'\n",
    "matrix_file = 'riesenfeld17_il25nmu_counts.mtx'\n",
    "annot_file = 'riesenfeld17_il25nmu_annot.csv'\n",
    "genes_file = 'riesenfeld17_il25nmu_genes.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures = True  # specify whether figures should be printed.\n",
    "save_figures = False # specify whether figures should be saved.\n",
    "\n",
    "quick_scale = True  # determine whether the variance model should be trained\n",
    "autograd = False     # specfify whether to use closed form gradients or TensorFlow's built-in autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Input Data for BatchGLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Counts Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy's mmread function takes a while here, approx. 90 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35670, 13711)\n"
     ]
    }
   ],
   "source": [
    "x_orig = scipy.io.mmread(in_dir+matrix_file).tocsc()\n",
    "x = x_orig.transpose().toarray()\n",
    "x = x.astype(dtype='int')\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take three samples with the highest 10000, 5000 and 1000 expressed genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35670, 10000) (35670, 5000) (35670, 1000)\n"
     ]
    }
   ],
   "source": [
    "counts = np.sum(x, axis=0)\n",
    "counts_sorted = counts.argsort()\n",
    "highest_10000 = counts_sorted[-10000:]\n",
    "highest_5000 = counts_sorted[-5000:]\n",
    "highest_1000 = counts_sorted[-1000:]\n",
    "\n",
    "x_10000 = x[:,highest_10000]\n",
    "x_5000 = x[:,highest_5000]\n",
    "x_1000 = x[:,highest_1000]\n",
    "print(x_10000.shape, x_5000.shape, x_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35670 observations are way too much to do general runtimes analyses at this point in time. We downsample to `n_obs` observations by taking random row indices from x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000) (10000, 5000) (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "n_obs = 10000\n",
    "idx = np.random.choice(x.shape[0], n_obs, replace=False)\n",
    "x = x[idx]\n",
    "x_10000 = x_10000[idx]\n",
    "x_5000 = x_5000[idx]\n",
    "x_1000 = x_1000[idx]\n",
    "print(x_10000.shape, x_5000.shape, x_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Design matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read annotation csv data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cell</th>\n",
       "      <th>sample</th>\n",
       "      <th>cond</th>\n",
       "      <th>rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30907</th>\n",
       "      <td>NMU_rep1.TTCGGTCTCAGTGCAT</td>\n",
       "      <td>NMU_rep1.TTCGGTCTCAGTGCAT</td>\n",
       "      <td>NMU_rep1</td>\n",
       "      <td>NMU</td>\n",
       "      <td>rep1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27531</th>\n",
       "      <td>NMU_rep1.CATTATCCACATCCGG</td>\n",
       "      <td>NMU_rep1.CATTATCCACATCCGG</td>\n",
       "      <td>NMU_rep1</td>\n",
       "      <td>NMU</td>\n",
       "      <td>rep1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24888</th>\n",
       "      <td>NMU_IL25_rep2.GTGGGTCAGTTAAGTG</td>\n",
       "      <td>NMU_IL25_rep2.GTGGGTCAGTTAAGTG</td>\n",
       "      <td>NMU_IL25_rep2</td>\n",
       "      <td>NMU_IL25</td>\n",
       "      <td>rep2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31030</th>\n",
       "      <td>NMU_rep1.TTGTAGGTCGCGTAGC</td>\n",
       "      <td>NMU_rep1.TTGTAGGTCGCGTAGC</td>\n",
       "      <td>NMU_rep1</td>\n",
       "      <td>NMU</td>\n",
       "      <td>rep1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13180</th>\n",
       "      <td>control_rep2.CAGATCACAGTAGAGC</td>\n",
       "      <td>control_rep2.CAGATCACAGTAGAGC</td>\n",
       "      <td>control_rep2</td>\n",
       "      <td>control</td>\n",
       "      <td>rep2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Unnamed: 0                            cell  \\\n",
       "30907       NMU_rep1.TTCGGTCTCAGTGCAT       NMU_rep1.TTCGGTCTCAGTGCAT   \n",
       "27531       NMU_rep1.CATTATCCACATCCGG       NMU_rep1.CATTATCCACATCCGG   \n",
       "24888  NMU_IL25_rep2.GTGGGTCAGTTAAGTG  NMU_IL25_rep2.GTGGGTCAGTTAAGTG   \n",
       "31030       NMU_rep1.TTGTAGGTCGCGTAGC       NMU_rep1.TTGTAGGTCGCGTAGC   \n",
       "13180   control_rep2.CAGATCACAGTAGAGC   control_rep2.CAGATCACAGTAGAGC   \n",
       "\n",
       "              sample      cond   rep  \n",
       "30907       NMU_rep1       NMU  rep1  \n",
       "27531       NMU_rep1       NMU  rep1  \n",
       "24888  NMU_IL25_rep2  NMU_IL25  rep2  \n",
       "31030       NMU_rep1       NMU  rep1  \n",
       "13180   control_rep2   control  rep2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_data = pd.read_csv(in_dir+annot_file)\n",
    "annot_data= annot_data.loc[idx]\n",
    "annot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NMU_rep1', 'NMU_IL25_rep2', 'control_rep2', 'control_rep1',\n",
       "       'NMU_IL25_rep1', 'NMU_rep2', 'IL25_rep1', 'IL25_rep2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_data['sample'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NMU', 'NMU_IL25', 'control', 'IL25'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_data['cond'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data consists of a total of `n_obs` observations, with 4 conditions, each of which is given in two batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = np.ones(n_obs)\n",
    "cond_IL25 = np.array(annot_data['cond'] == 'IL25', dtype='int')\n",
    "cond_NMU = np.array(annot_data['cond'] == 'NMU', dtype='int')\n",
    "cond_NMU_IL25 = np.array(annot_data['cond'] == 'NMU_IL25', dtype='int')\n",
    "#batch = np.array(annot_data['rep'] == 'rep2', dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design matrix can be given by an intercept, representing condition control and batch 0 + 3 condition columns and a batch column.\n",
    "It has the shape n_obs x (intercept, IL25, NMU, IL25_NMU, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]]\n",
      "(10000, 4)\n"
     ]
    }
   ],
   "source": [
    "design_loc = np.stack((intercept, cond_IL25, cond_NMU, cond_NMU_IL25), axis=-1)\n",
    "print(design_loc)\n",
    "print(design_loc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`design_loc` and `design_scale` are the same in this case, so we just assign it here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_scale = design_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Size Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "size_factors_10000 = np.sum(x_10000, axis=1)/(n_obs)\n",
    "size_factors_10000 = size_factors_10000 / np.mean(size_factors_10000)\n",
    "size_factors_5000 = np.sum(x_5000, axis=1)/(n_obs)\n",
    "size_factors_5000 = size_factors_5000 / np.mean(size_factors_5000)\n",
    "size_factors_1000 = np.sum(x_1000, axis=1)/(n_obs)\n",
    "size_factors_1000 = size_factors_1000 / np.mean(size_factors_1000)\n",
    "print(size_factors_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for zeros to identify potential columns with only zeros (features without any counts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'size_factors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b4b1d2bbd36c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnon_zero_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_factors\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_zero_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_zero_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnon_zero_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'size_factors' is not defined"
     ]
    }
   ],
   "source": [
    "non_zero_idx = np.where(size_factors > 0)[0]\n",
    "print(non_zero_idx)\n",
    "print(x.shape)\n",
    "if len(non_zero_idx) < len(size_factors):\n",
    "    x = x[:,non_zero_idx]\n",
    "    size_factors = size_factors[non_zero_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty array, thus all features have at least 1 count. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_10000 = glm.models.glm_nb.InputDataGLM(data=x_10000, design_loc=design_loc, design_scale=design_scale, size_factors=size_factors_10000)\n",
    "input_data_5000 = glm.models.glm_nb.InputDataGLM(data=x_5000, design_loc=design_loc, design_scale=design_scale, size_factors=size_factors_5000)\n",
    "input_data_1000 = glm.models.glm_nb.InputDataGLM(data=x_1000, design_loc=design_loc, design_scale=design_scale, size_factors=size_factors_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models with IRLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "step 0\n",
      "Step: 3 loss: 17037.118601371214, converged 162, updated 1000, (logs: 162, grad: 0, x_step: 0)\n",
      "Step: 4 loss: 17037.031380757515, converged 799, updated 838, (logs: 637, grad: 0, x_step: 0)\n",
      "Step: 5 loss: 17037.026009157198, converged 971, updated 201, (logs: 172, grad: 0, x_step: 0)\n",
      "Step: 6 loss: 17037.02595248672, converged 996, updated 29, (logs: 25, grad: 0, x_step: 0)\n",
      "Step: 7 loss: 17037.025952475637, converged 998, updated 4, (logs: 2, grad: 0, x_step: 0)\n",
      "Step: 8 loss: 17037.025952475607, converged 1000, updated 2, (logs: 2, grad: 0, x_step: 0)\n",
      "(10000,)\n",
      "step 0\n",
      "Step: 3 loss: 17037.118601371214, converged 162, updated 1000, (logs: 162, grad: 0, x_step: 0)\n",
      "Step: 4 loss: 17037.031380757515, converged 799, updated 838, (logs: 637, grad: 0, x_step: 0)\n",
      "Step: 5 loss: 17037.02600915722, converged 971, updated 201, (logs: 172, grad: 0, x_step: 0)\n",
      "Step: 6 loss: 17037.025952486765, converged 996, updated 29, (logs: 25, grad: 0, x_step: 0)\n",
      "Step: 7 loss: 17037.025952475717, converged 998, updated 4, (logs: 2, grad: 0, x_step: 0)\n",
      "Step: 8 loss: 17037.025952475684, converged 1000, updated 2, (logs: 2, grad: 0, x_step: 0)\n",
      "(10000,)\n",
      "step 0\n",
      "Step: 2 loss: 43233.326998074226, converged 1, updated 5000, (logs: 1, grad: 0, x_step: 0)\n",
      "Step: 3 loss: 43230.56291099936, converged 740, updated 4999, (logs: 739, grad: 0, x_step: 0)\n",
      "Step: 4 loss: 43230.196866782586, converged 4052, updated 4260, (logs: 3312, grad: 0, x_step: 0)\n",
      "Step: 5 loss: 43230.141830867906, converged 4837, updated 948, (logs: 785, grad: 0, x_step: 0)\n",
      "Step: 6 loss: 43230.129904168076, converged 4949, updated 163, (logs: 112, grad: 0, x_step: 0)\n",
      "Step: 7 loss: 43230.126306014674, converged 4979, updated 51, (logs: 29, grad: 1, x_step: 0)\n",
      "Step: 8 loss: 43230.12506977636, converged 4993, updated 21, (logs: 14, grad: 0, x_step: 0)\n",
      "Step: 9 loss: 43230.12461524944, converged 4996, updated 7, (logs: 3, grad: 0, x_step: 0)\n",
      "step 10\n",
      "step 20\n",
      "Step: 22 loss: 43230.12434934241, converged 4999, updated 4, (logs: 3, grad: 0, x_step: 0)\n",
      "Step: 23 loss: 43230.12434934203, converged 5000, updated 1, (logs: 1, grad: 0, x_step: 0)\n",
      "(10000,)\n",
      "step 0\n",
      "Step: 2 loss: 43233.326998074226, converged 1, updated 5000, (logs: 1, grad: 0, x_step: 0)\n",
      "Step: 3 loss: 43230.56291099936, converged 740, updated 4999, (logs: 739, grad: 0, x_step: 0)\n",
      "Step: 4 loss: 43230.19686678259, converged 4052, updated 4260, (logs: 3312, grad: 0, x_step: 0)\n",
      "Step: 5 loss: 43230.141830867986, converged 4837, updated 948, (logs: 785, grad: 0, x_step: 0)\n",
      "Step: 6 loss: 43230.12990416823, converged 4949, updated 163, (logs: 112, grad: 0, x_step: 0)\n",
      "Step: 7 loss: 43230.12630601487, converged 4979, updated 51, (logs: 29, grad: 1, x_step: 0)\n",
      "Step: 8 loss: 43230.125069776564, converged 4993, updated 21, (logs: 14, grad: 0, x_step: 0)\n",
      "Step: 9 loss: 43230.12461524964, converged 4996, updated 7, (logs: 3, grad: 0, x_step: 0)\n",
      "step 10\n",
      "step 20\n",
      "Step: 22 loss: 43230.12434934261, converged 4999, updated 4, (logs: 3, grad: 0, x_step: 0)\n",
      "Step: 23 loss: 43230.124349342455, converged 5000, updated 1, (logs: 1, grad: 0, x_step: 0)\n",
      "(10000,)\n",
      "step 0\n",
      "Step: 3 loss: 66945.0472954082, converged 1166, updated 10000, (logs: 1166, grad: 0, x_step: 0)\n",
      "Step: 4 loss: 66944.79289854769, converged 7239, updated 8834, (logs: 6073, grad: 0, x_step: 0)\n",
      "Step: 5 loss: 66944.76116980464, converged 9404, updated 2761, (logs: 2155, grad: 10, x_step: 0)\n",
      "Step: 6 loss: 66944.74785739624, converged 9821, updated 596, (logs: 366, grad: 51, x_step: 0)\n",
      "Step: 7 loss: 66944.74236216818, converged 9940, updated 179, (logs: 93, grad: 26, x_step: 0)\n",
      "Step: 8 loss: 66944.7403517707, converged 9979, updated 60, (logs: 34, grad: 5, x_step: 0)\n",
      "Step: 9 loss: 66944.7396093859, converged 9984, updated 21, (logs: 5, grad: 0, x_step: 0)\n",
      "step 10\n",
      "step 20\n",
      "Step: 21 loss: 66944.73917516679, converged 9990, updated 16, (logs: 6, grad: 0, x_step: 0)\n",
      "Step: 22 loss: 66944.7391751651, converged 9998, updated 10, (logs: 8, grad: 0, x_step: 0)\n",
      "Step: 23 loss: 66944.73917516456, converged 10000, updated 2, (logs: 2, grad: 0, x_step: 0)\n",
      "(10000,)\n",
      "step 0\n",
      "Step: 3 loss: 66945.0472954082, converged 1166, updated 10000, (logs: 1166, grad: 0, x_step: 0)\n",
      "Step: 4 loss: 66944.79289854772, converged 7239, updated 8834, (logs: 6073, grad: 0, x_step: 0)\n",
      "Step: 5 loss: 66944.76116980475, converged 9404, updated 2761, (logs: 2155, grad: 10, x_step: 0)\n",
      "Step: 6 loss: 66944.74785718223, converged 9821, updated 596, (logs: 366, grad: 51, x_step: 0)\n",
      "Step: 7 loss: 66944.74236106763, converged 9940, updated 179, (logs: 93, grad: 26, x_step: 0)\n",
      "Step: 8 loss: 66944.74035044282, converged 9979, updated 60, (logs: 34, grad: 5, x_step: 0)\n",
      "Step: 9 loss: 66944.7396080579, converged 9984, updated 21, (logs: 5, grad: 0, x_step: 0)\n",
      "step 10\n",
      "step 20\n",
      "Step: 21 loss: 66944.7391738388, converged 9990, updated 16, (logs: 6, grad: 0, x_step: 0)\n",
      "Step: 22 loss: 66944.73917383734, converged 9998, updated 10, (logs: 8, grad: 0, x_step: 0)\n",
      "Step: 23 loss: 66944.73917383715, converged 10000, updated 2, (logs: 2, grad: 0, x_step: 0)\n",
      "standard [ 17.93252039 234.23506975 494.75194597]\n",
      "feature_batched [ 10.70412803  58.57747984 128.36713672]\n"
     ]
    }
   ],
   "source": [
    "times_standard_irls = np.zeros(3)\n",
    "times_fb_irls = np.zeros(3)\n",
    "for i, input_data in enumerate([input_data_1000, input_data_5000, input_data_10000]):\n",
    "    \n",
    "    estimator = glm.models.glm_nb.Estimator(input_data, init_a = \"standard\", init_b = \"standard\", quick_scale=quick_scale)\n",
    "    estimator.initialize()\n",
    "    \n",
    "    t0_analytic = time.time()\n",
    "    estimator.train(batched_model=False, batch_size=1000, optimizer=\"irls_gd_tr\", learning_rate=1e-2, \n",
    "                convergence_criteria=\"all_converged\", stopping_criteria=30, autograd=autograd, featurewise=False)\n",
    "    t1_analytic = time.time()\n",
    "    times_standard_irls[i] = t1_analytic - t0_analytic\n",
    "    \n",
    "    del estimator\n",
    "    estimator2 = glm.models.glm_nb.Estimator(input_data, \n",
    "                init_a = \"standard\", init_b = \"standard\", quick_scale=quick_scale)\n",
    "    estimator2.initialize()\n",
    "    \n",
    "    t0_tf = time.time()\n",
    "    estimator2.train(batched_model=False, batch_size=1000, optimizer=\"irls_gd_tr\", learning_rate=1e-2,\n",
    "                convergence_criteria=\"all_converged\", stopping_criteria=30, autograd=autograd, featurewise=True)\n",
    "    t1_tf = time.time()\n",
    "    times_fb_irls[i] = t1_tf - t0_tf\n",
    "    del estimator2\n",
    "print('standard', times_standard_irls)\n",
    "print('featurewise', times_fb_irls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models with ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION: Fitting with ADAM takes considerably longer than using IRLS. Runtimes for 10000 obs and 10000 features are expected to run many hours, probably longer than one day (tested with Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz using 8 parallel computations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_standard_adam = np.zeros(3)\n",
    "times_fb_adam = np.zeros(3)\n",
    "for i, input_data in enumerate([input_data_1000]):\n",
    "    \n",
    "    estimator = glm.models.glm_nb.Estimator(input_data, init_a = \"standard\", init_b = \"standard\", quick_scale=quick_scale)\n",
    "    estimator.initialize()\n",
    "    \n",
    "    t0_analytic = time.time()\n",
    "    estimator.train(batched_model=False, batch_size=1000, optimizer=\"adam\", learning_rate=1e-2, \n",
    "                convergence_criteria=\"all_converged\", stopping_criteria=30, autograd=autograd, featurewise=False)\n",
    "    t1_analytic = time.time()\n",
    "    times_standard_adam[i] = t1_analytic - t0_analytic\n",
    "    \n",
    "    del estimator\n",
    "    estimator2 = glm.models.glm_nb.Estimator(input_data, \n",
    "                init_a = \"standard\", init_b = \"standard\", quick_scale=quick_scale)\n",
    "    estimator.initialize()\n",
    "    \n",
    "    t0_tf = time.time()\n",
    "    estimator2.train(batched_model=False, batch_size=1000, optimizer=\"adam\", learning_rate=1e-2,\n",
    "                convergence_criteria=\"all_converged\", stopping_criteria=30, autograd=autograd, featurewise=True)\n",
    "    t1_tf = time.time()\n",
    "    times_fb_adam[i] = t1_tf - t0_tf\n",
    "    del estimator2\n",
    "print('standard', times_standard_adam)\n",
    "print('featurewise', times_fb_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_figures:# data to plot\n",
    "    n_groups = 3\n",
    "\n",
    "    # create plot\n",
    "    fig, ax = plt.subplots(figsize=(3.2,4.4))\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, times_standard_irls, bar_width,\n",
    "    alpha=opacity,\n",
    "    color='b',\n",
    "    label='standard')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, times_fb_irls, bar_width,\n",
    "    alpha=opacity,\n",
    "    color='g',\n",
    "    label='featurewise')\n",
    "\n",
    "    plt.xlabel('number of observations/features')\n",
    "    plt.ylabel('runtime in sec')\n",
    "    #plt.title('Scores by person')\n",
    "    plt.xticks(index + bar_width, ('20000/10000', '10000/5000', '2000/2000'))\n",
    "    plt.legend()\n",
    "\n",
    "    if save_figure:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/home/mario/PSC19/batchglm_tf2/irls_runtimes.pdf', bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_figures:\n",
    "    \n",
    "    # data to plot\n",
    "    n_groups = 3\n",
    "\n",
    "    # create plot\n",
    "    fig, ax = plt.subplots(figsize=(3.2,4.4))\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, times_standard_adam, bar_width,\n",
    "    alpha=opacity,\n",
    "    color='b',\n",
    "    label='standard')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, times_fb_adam, bar_width,\n",
    "    alpha=opacity,\n",
    "    color='g',\n",
    "    label='featurewise')\n",
    "\n",
    "    plt.xlabel('number of observations/features')\n",
    "    plt.ylabel('runtime in sec')\n",
    "    #plt.title('Scores by person')\n",
    "    plt.xticks(index + bar_width, ('20000/10000', '10000/5000', '2000/2000'))\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    if save_figures:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir+'adam_runtimes.pdf', bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
