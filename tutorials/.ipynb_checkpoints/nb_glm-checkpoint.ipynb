{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)\n",
    "sim.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[   32,   416,  6105, ...,  8969,  1636,   663],\n",
       "       [   23,   626, 10337, ...,  4622,  1413,   777],\n",
       "       [   26,   508,  2510, ...,  8820,  2166,  1007],\n",
       "       ...,\n",
       "       [   30,   198,  8707, ...,  8151,  1100,   616],\n",
       "       [   36,   156,  3285, ...,  8939,  1053,   625],\n",
       "       [   38,   247,  8099, ...,  6965,  1415,   644]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_scale, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters used to generate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'a' (design_loc_params: 5, features: 100)>\n",
       "array([[ 3.453986,  5.866545,  8.830487, ...,  9.0674  ,  7.869775,  6.942665],\n",
       "       [-0.485149, -0.186729,  0.520302, ...,  0.344557,  0.171454,  0.352811],\n",
       "       [ 0.64733 ,  0.184957, -0.163139, ...,  0.385477,  0.190301,  0.375933],\n",
       "       [ 0.314313, -0.173535, -0.202074, ...,  0.383422, -0.363319, -0.523276],\n",
       "       [-0.333154, -0.132375,  0.374714, ..., -0.410275, -0.292073,  0.10605 ]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'b' (design_scale_params: 5, features: 100)>\n",
       "array([[ 2.302585,  1.386294,  1.94591 , ...,  1.098612,  2.302585,  2.079442],\n",
       "       [-0.115073,  0.056259,  0.019004, ...,  0.462091,  0.305876, -0.525032],\n",
       "       [ 0.161889,  0.680545,  0.508986, ..., -0.576983,  0.517051,  0.403571],\n",
       "       [ 0.692426,  0.322277,  0.581509, ..., -0.212946,  0.136586,  0.521936],\n",
       "       [ 0.305526,  0.166326,  0.28729 , ...,  0.581877,  0.06642 ,  0.420869]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = sim.design_loc\n",
    "design_scale = sim.design_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(data=X, design_loc=design_loc, design_scale=design_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: True\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train:\n",
    "\n",
    "There are multiple possible training strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTO\n",
      "DEFAULT\n",
      "EXACT\n",
      "QUICK\n",
      "PRE_INITIALIZED\n"
     ]
    }
   ],
   "source": [
    "for i in estimator.TrainingStrategy:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one of them corresponds to a list of training options which will be passed to the estimator.train() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(estimator.TrainingStrategy.DEFAULT.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, when choosing the training strategy \"DEFAULT\", the following call:\n",
    "```python\n",
    "estimator.train_sequence(\"DEFAULT\")\n",
    "```\n",
    "is equal to:\n",
    "```python\n",
    "estimator.train_sequence(estimator.TrainingStrategy.DEFAULT)\n",
    "```\n",
    "is equal to:\n",
    "```python\n",
    "estimator.train(\n",
    "    convergence_criteria = 't_test',\n",
    "    learning_rate = 0.1,\n",
    "    loss_window_size = 100,\n",
    "    optim_algo = 'ADAM',\n",
    "    stop_at_loss_change = 0.05,\n",
    "    use_batching = True\n",
    ")\n",
    "estimator.train(\n",
    "    convergence_criteria = 't_test',\n",
    "    learning_rate = 0.05,\n",
    "    loss_window_size = 10,\n",
    "    optim_algo = 'GD',\n",
    "    stop_at_loss_change = 0.05,\n",
    "    use_batching = False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 870.819283\n",
      "Step: 2\tloss: 886.954227\n",
      "Step: 3\tloss: 877.969386\n",
      "Step: 4\tloss: 879.416220\n",
      "Step: 5\tloss: 875.629420\n",
      "Step: 6\tloss: 878.777180\n",
      "Step: 7\tloss: 878.018419\n",
      "Step: 8\tloss: 876.890313\n",
      "Step: 9\tloss: 873.899672\n",
      "Step: 10\tloss: 875.953812\n",
      "Step: 11\tloss: 878.074777\n",
      "Step: 12\tloss: 876.909827\n",
      "Step: 13\tloss: 872.009427\n",
      "Step: 14\tloss: 874.448934\n",
      "Step: 15\tloss: 875.445427\n",
      "Step: 16\tloss: 875.966209\n",
      "Step: 17\tloss: 872.381678\n",
      "Step: 18\tloss: 873.777388\n",
      "Step: 19\tloss: 875.611500\n",
      "Step: 20\tloss: 875.068247\n",
      "Step: 21\tloss: 871.718600\n",
      "Step: 22\tloss: 874.416605\n",
      "Step: 23\tloss: 874.773107\n",
      "Step: 24\tloss: 873.968636\n",
      "Step: 25\tloss: 871.079997\n",
      "Step: 26\tloss: 873.275232\n",
      "Step: 27\tloss: 875.230056\n",
      "Step: 28\tloss: 874.240742\n",
      "Step: 29\tloss: 870.215909\n",
      "Step: 30\tloss: 873.794882\n",
      "Step: 31\tloss: 874.273801\n",
      "Step: 32\tloss: 874.980534\n",
      "Step: 33\tloss: 870.907554\n",
      "Step: 34\tloss: 873.696283\n",
      "Step: 35\tloss: 874.294932\n",
      "Step: 36\tloss: 873.767583\n",
      "Step: 37\tloss: 870.756128\n",
      "Step: 38\tloss: 872.852450\n",
      "Step: 39\tloss: 874.427652\n",
      "Step: 40\tloss: 874.512540\n",
      "Step: 41\tloss: 870.640891\n",
      "Step: 42\tloss: 872.489970\n",
      "Step: 43\tloss: 874.553594\n",
      "Step: 44\tloss: 874.611144\n",
      "Step: 45\tloss: 870.339595\n",
      "Step: 46\tloss: 873.835041\n",
      "Step: 47\tloss: 873.291327\n",
      "Step: 48\tloss: 874.897320\n",
      "Step: 49\tloss: 870.154987\n",
      "Step: 50\tloss: 872.864180\n",
      "Step: 51\tloss: 874.890345\n",
      "Step: 52\tloss: 874.416591\n",
      "Step: 53\tloss: 871.006433\n",
      "Step: 54\tloss: 872.144715\n",
      "Step: 55\tloss: 874.726212\n",
      "Step: 56\tloss: 874.283225\n",
      "Step: 57\tloss: 870.559905\n",
      "Step: 58\tloss: 872.395168\n",
      "Step: 59\tloss: 874.978726\n",
      "Step: 60\tloss: 874.319578\n",
      "Step: 61\tloss: 870.684302\n",
      "Step: 62\tloss: 873.768050\n",
      "Step: 63\tloss: 874.068089\n",
      "Step: 64\tloss: 873.669553\n",
      "Step: 65\tloss: 871.214540\n",
      "Step: 66\tloss: 872.765640\n",
      "Step: 67\tloss: 874.839236\n",
      "Step: 68\tloss: 873.358628\n",
      "Step: 69\tloss: 870.739820\n",
      "Step: 70\tloss: 873.145078\n",
      "Step: 71\tloss: 874.152284\n",
      "Step: 72\tloss: 874.122608\n",
      "Step: 73\tloss: 870.835266\n",
      "Step: 74\tloss: 872.972957\n",
      "Step: 75\tloss: 874.503672\n",
      "Step: 76\tloss: 873.960053\n",
      "Step: 77\tloss: 871.167750\n",
      "Step: 78\tloss: 872.778620\n",
      "Step: 79\tloss: 873.995919\n",
      "Step: 80\tloss: 874.354522\n",
      "Step: 81\tloss: 870.768888\n",
      "Step: 82\tloss: 873.744815\n",
      "Step: 83\tloss: 873.813900\n",
      "Step: 84\tloss: 874.048194\n",
      "Step: 85\tloss: 870.846924\n",
      "Step: 86\tloss: 873.134872\n",
      "Step: 87\tloss: 874.295616\n",
      "Step: 88\tloss: 874.176323\n",
      "Step: 89\tloss: 870.675683\n",
      "Step: 90\tloss: 873.524638\n",
      "Step: 91\tloss: 874.092618\n",
      "Step: 92\tloss: 874.127733\n",
      "Step: 93\tloss: 871.317233\n",
      "Step: 94\tloss: 873.739889\n",
      "Step: 95\tloss: 873.533527\n",
      "Step: 96\tloss: 873.805002\n",
      "Step: 97\tloss: 871.209870\n",
      "Step: 98\tloss: 873.265705\n",
      "Step: 99\tloss: 874.022205\n",
      "Step: 100\tloss: 873.901245\n",
      "Step: 101\tloss: 870.550892\n",
      "Step: 102\tloss: 873.538904\n",
      "Step: 103\tloss: 874.004596\n",
      "Step: 104\tloss: 874.335319\n",
      "Step: 105\tloss: 870.727217\n",
      "Step: 106\tloss: 872.660687\n",
      "Step: 107\tloss: 874.665649\n",
      "Step: 108\tloss: 874.418822\n",
      "Step: 109\tloss: 870.924672\n",
      "Step: 110\tloss: 872.525484\n",
      "Step: 111\tloss: 874.392557\n",
      "Step: 112\tloss: 874.743708\n",
      "Step: 113\tloss: 871.054145\n",
      "Step: 114\tloss: 873.353530\n",
      "Step: 115\tloss: 874.621660\n",
      "Step: 116\tloss: 873.726984\n",
      "Step: 117\tloss: 871.119784\n",
      "Step: 118\tloss: 872.807481\n",
      "Step: 119\tloss: 874.027930\n",
      "Step: 120\tloss: 874.899571\n",
      "Step: 121\tloss: 870.986161\n",
      "Step: 122\tloss: 873.690313\n",
      "Step: 123\tloss: 874.339511\n",
      "Step: 124\tloss: 873.941674\n",
      "Step: 125\tloss: 870.703227\n",
      "Step: 126\tloss: 873.159512\n",
      "Step: 127\tloss: 875.284023\n",
      "Step: 128\tloss: 873.896691\n",
      "Step: 129\tloss: 871.098113\n",
      "Step: 130\tloss: 872.422523\n",
      "Step: 131\tloss: 874.800246\n",
      "Step: 132\tloss: 874.731686\n",
      "Step: 133\tloss: 870.905213\n",
      "Step: 134\tloss: 872.470159\n",
      "Step: 135\tloss: 875.449717\n",
      "Step: 136\tloss: 874.216712\n",
      "Step: 137\tloss: 871.093914\n",
      "Step: 138\tloss: 872.887942\n",
      "Step: 139\tloss: 874.008057\n",
      "Step: 140\tloss: 874.893625\n",
      "Step: 141\tloss: 871.073309\n",
      "Step: 142\tloss: 873.118283\n",
      "Step: 143\tloss: 874.109784\n",
      "Step: 144\tloss: 874.493772\n",
      "Step: 145\tloss: 870.150903\n",
      "Step: 146\tloss: 873.902363\n",
      "Step: 147\tloss: 874.690120\n",
      "Step: 148\tloss: 874.020912\n",
      "Step: 149\tloss: 871.065081\n",
      "Step: 150\tloss: 872.733352\n",
      "Step: 151\tloss: 874.478476\n",
      "Step: 152\tloss: 874.509936\n",
      "Step: 153\tloss: 870.614555\n",
      "Step: 154\tloss: 873.333496\n",
      "Step: 155\tloss: 874.767462\n",
      "Step: 156\tloss: 873.991345\n",
      "Step: 157\tloss: 871.133918\n",
      "Step: 158\tloss: 872.834649\n",
      "Step: 159\tloss: 874.044586\n",
      "Step: 160\tloss: 874.543188\n",
      "Step: 161\tloss: 870.783721\n",
      "Step: 162\tloss: 873.125700\n",
      "Step: 163\tloss: 873.800921\n",
      "Step: 164\tloss: 874.831353\n",
      "Step: 165\tloss: 871.290835\n",
      "Step: 166\tloss: 872.076478\n",
      "Step: 167\tloss: 874.924190\n",
      "Step: 168\tloss: 874.280690\n",
      "Step: 169\tloss: 870.673528\n",
      "Step: 170\tloss: 873.014773\n",
      "Step: 171\tloss: 874.447804\n",
      "Step: 172\tloss: 874.479430\n",
      "Step: 173\tloss: 870.606504\n",
      "Step: 174\tloss: 873.405573\n",
      "Step: 175\tloss: 874.747915\n",
      "Step: 176\tloss: 874.059654\n",
      "Step: 177\tloss: 871.226565\n",
      "Step: 178\tloss: 873.258315\n",
      "Step: 179\tloss: 873.878615\n",
      "Step: 180\tloss: 874.547863\n",
      "Step: 181\tloss: 870.395746\n",
      "Step: 182\tloss: 873.461644\n",
      "Step: 183\tloss: 874.094560\n",
      "Step: 184\tloss: 874.862654\n",
      "Step: 185\tloss: 870.886362\n",
      "Step: 186\tloss: 872.917973\n",
      "Step: 187\tloss: 874.806470\n",
      "Step: 188\tloss: 874.131044\n",
      "Step: 189\tloss: 870.511743\n",
      "Step: 190\tloss: 873.190622\n",
      "Step: 191\tloss: 874.381568\n",
      "Step: 192\tloss: 874.507343\n",
      "Step: 193\tloss: 870.945548\n",
      "Step: 194\tloss: 873.005511\n",
      "Step: 195\tloss: 874.727319\n",
      "Step: 196\tloss: 874.197028\n",
      "Step: 197\tloss: 871.301964\n",
      "Step: 198\tloss: 873.535706\n",
      "Step: 199\tloss: 874.250716\n",
      "Step: 200\tloss: 873.831552\n",
      "pval: 0.021207\n",
      "Step: 201\tloss: 870.550390\n",
      "Step: 202\tloss: 874.555599\n",
      "Step: 203\tloss: 874.067689\n",
      "Step: 204\tloss: 873.654290\n",
      "Step: 205\tloss: 870.195306\n",
      "Step: 206\tloss: 873.452520\n",
      "Step: 207\tloss: 874.858825\n",
      "Step: 208\tloss: 874.439849\n",
      "Step: 209\tloss: 870.964987\n",
      "Step: 210\tloss: 873.046549\n",
      "Step: 211\tloss: 874.718724\n",
      "Step: 212\tloss: 874.206776\n",
      "Step: 213\tloss: 871.223971\n",
      "Step: 214\tloss: 873.595623\n",
      "Step: 215\tloss: 874.411752\n",
      "Step: 216\tloss: 873.733844\n",
      "Step: 217\tloss: 872.093111\n",
      "Step: 218\tloss: 872.703273\n",
      "Step: 219\tloss: 873.926447\n",
      "Step: 220\tloss: 874.271780\n",
      "Step: 221\tloss: 870.972247\n",
      "Step: 222\tloss: 872.979951\n",
      "Step: 223\tloss: 874.632008\n",
      "Step: 224\tloss: 874.495705\n",
      "Step: 225\tloss: 870.769182\n",
      "Step: 226\tloss: 873.230472\n",
      "Step: 227\tloss: 875.122516\n",
      "Step: 228\tloss: 873.860150\n",
      "Step: 229\tloss: 870.848692\n",
      "Step: 230\tloss: 872.885808\n",
      "Step: 231\tloss: 874.449508\n",
      "Step: 232\tloss: 874.792874\n",
      "Step: 233\tloss: 870.240548\n",
      "Step: 234\tloss: 873.011892\n",
      "Step: 235\tloss: 874.492051\n",
      "Step: 236\tloss: 875.296848\n",
      "Step: 237\tloss: 870.755667\n",
      "Step: 238\tloss: 873.008137\n",
      "Step: 239\tloss: 874.624365\n",
      "Step: 240\tloss: 874.713135\n",
      "Step: 241\tloss: 871.145348\n",
      "Step: 242\tloss: 873.913285\n",
      "Step: 243\tloss: 874.276560\n",
      "Step: 244\tloss: 873.800432\n",
      "Step: 245\tloss: 870.377334\n",
      "Step: 246\tloss: 873.296309\n",
      "Step: 247\tloss: 874.789862\n",
      "Step: 248\tloss: 874.557186\n",
      "Step: 249\tloss: 871.223261\n",
      "Step: 250\tloss: 873.385071\n",
      "Step: 251\tloss: 875.241237\n",
      "Step: 252\tloss: 873.367462\n",
      "Step: 253\tloss: 871.153000\n",
      "Step: 254\tloss: 873.321217\n",
      "Step: 255\tloss: 874.005639\n",
      "Step: 256\tloss: 874.719971\n",
      "Step: 257\tloss: 870.927109\n",
      "Step: 258\tloss: 873.085619\n",
      "Step: 259\tloss: 874.644768\n",
      "Step: 260\tloss: 874.381829\n",
      "Step: 261\tloss: 870.372723\n",
      "Step: 262\tloss: 873.735819\n",
      "Step: 263\tloss: 874.049287\n",
      "Step: 264\tloss: 874.877423\n",
      "Step: 265\tloss: 871.255158\n",
      "Step: 266\tloss: 872.984669\n",
      "Step: 267\tloss: 874.486916\n",
      "Step: 268\tloss: 874.216953\n",
      "Step: 269\tloss: 870.684067\n",
      "Step: 270\tloss: 873.318804\n",
      "Step: 271\tloss: 874.503743\n",
      "Step: 272\tloss: 874.463340\n",
      "Step: 273\tloss: 871.359838\n",
      "Step: 274\tloss: 873.409103\n",
      "Step: 275\tloss: 874.321510\n",
      "Step: 276\tloss: 873.745978\n",
      "Step: 277\tloss: 870.693268\n",
      "Step: 278\tloss: 873.454804\n",
      "Step: 279\tloss: 874.503820\n",
      "Step: 280\tloss: 874.189042\n",
      "Step: 281\tloss: 871.244224\n",
      "Step: 282\tloss: 872.821983\n",
      "Step: 283\tloss: 874.588074\n",
      "Step: 284\tloss: 874.191991\n",
      "Step: 285\tloss: 871.525850\n",
      "Step: 286\tloss: 872.905843\n",
      "Step: 287\tloss: 873.890929\n",
      "Step: 288\tloss: 874.587698\n",
      "Step: 289\tloss: 871.237297\n",
      "Step: 290\tloss: 872.875093\n",
      "Step: 291\tloss: 874.292493\n",
      "Step: 292\tloss: 874.742259\n",
      "Step: 293\tloss: 870.542074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 294\tloss: 873.554922\n",
      "Step: 295\tloss: 874.425155\n",
      "Step: 296\tloss: 874.733963\n",
      "Step: 297\tloss: 871.142162\n",
      "Step: 298\tloss: 873.126753\n",
      "Step: 299\tloss: 874.777332\n",
      "Step: 300\tloss: 874.216714\n",
      "pval: 0.619993\n",
      "Training sequence #1 complete\n",
      "Beginning with training sequence #2\n",
      "Step: 301\tloss: 873.109344\n",
      "Step: 302\tloss: 876.143956\n",
      "Step: 303\tloss: 873.395818\n",
      "Step: 304\tloss: 873.607083\n",
      "Step: 305\tloss: 874.349209\n",
      "Step: 306\tloss: 873.857454\n",
      "Step: 307\tloss: 873.158390\n",
      "Step: 308\tloss: 873.155839\n",
      "Step: 309\tloss: 873.523994\n",
      "Step: 310\tloss: 873.544317\n",
      "Step: 311\tloss: 873.182074\n",
      "Step: 312\tloss: 872.904408\n",
      "Step: 313\tloss: 872.953067\n",
      "Step: 314\tloss: 873.129368\n",
      "Step: 315\tloss: 873.159188\n",
      "Step: 316\tloss: 873.032171\n",
      "Step: 317\tloss: 872.913430\n",
      "Step: 318\tloss: 872.898050\n",
      "Step: 319\tloss: 872.934936\n",
      "Step: 320\tloss: 872.935527\n",
      "pval: 0.009443\n",
      "Step: 321\tloss: 872.886982\n",
      "Step: 322\tloss: 872.842937\n",
      "Step: 323\tloss: 872.838025\n",
      "Step: 324\tloss: 872.849736\n",
      "Step: 325\tloss: 872.845569\n",
      "Step: 326\tloss: 872.826084\n",
      "Step: 327\tloss: 872.808648\n",
      "Step: 328\tloss: 872.798780\n",
      "Step: 329\tloss: 872.791370\n",
      "Step: 330\tloss: 872.784706\n",
      "pval: 0.000220\n",
      "Step: 331\tloss: 872.780874\n",
      "Step: 332\tloss: 872.778599\n",
      "Step: 333\tloss: 872.773409\n",
      "Step: 334\tloss: 872.764844\n",
      "Step: 335\tloss: 872.758327\n",
      "Step: 336\tloss: 872.756868\n",
      "Step: 337\tloss: 872.756357\n",
      "Step: 338\tloss: 872.752554\n",
      "Step: 339\tloss: 872.747019\n",
      "Step: 340\tloss: 872.743713\n",
      "pval: 0.000017\n",
      "Step: 341\tloss: 872.743327\n",
      "Step: 342\tloss: 872.742833\n",
      "Step: 343\tloss: 872.739943\n",
      "Step: 344\tloss: 872.736298\n",
      "Step: 345\tloss: 872.734807\n",
      "Step: 346\tloss: 872.735198\n",
      "Step: 347\tloss: 872.734667\n",
      "Step: 348\tloss: 872.732221\n",
      "Step: 349\tloss: 872.729873\n",
      "Step: 350\tloss: 872.729396\n",
      "pval: 0.000029\n",
      "Step: 351\tloss: 872.729818\n",
      "Step: 352\tloss: 872.729169\n",
      "Step: 353\tloss: 872.727546\n",
      "Step: 354\tloss: 872.726606\n",
      "Step: 355\tloss: 872.726625\n",
      "Step: 356\tloss: 872.726364\n",
      "Step: 357\tloss: 872.725466\n",
      "Step: 358\tloss: 872.724775\n",
      "Step: 359\tloss: 872.724641\n",
      "Step: 360\tloss: 872.724549\n",
      "pval: 0.000041\n",
      "Step: 361\tloss: 872.724142\n",
      "Step: 362\tloss: 872.723634\n",
      "Step: 363\tloss: 872.723368\n",
      "Step: 364\tloss: 872.723305\n",
      "Step: 365\tloss: 872.723132\n",
      "Step: 366\tloss: 872.722793\n",
      "Step: 367\tloss: 872.722562\n",
      "Step: 368\tloss: 872.722539\n",
      "Step: 369\tloss: 872.722461\n",
      "Step: 370\tloss: 872.722205\n",
      "pval: 0.000041\n",
      "Step: 371\tloss: 872.722029\n",
      "Step: 372\tloss: 872.722039\n",
      "Step: 373\tloss: 872.721978\n",
      "Step: 374\tloss: 872.721792\n",
      "Step: 375\tloss: 872.721715\n",
      "Step: 376\tloss: 872.721725\n",
      "Step: 377\tloss: 872.721623\n",
      "Step: 378\tloss: 872.721503\n",
      "Step: 379\tloss: 872.721513\n",
      "Step: 380\tloss: 872.721515\n",
      "pval: 0.000018\n",
      "Step: 381\tloss: 872.721408\n",
      "Step: 382\tloss: 872.721339\n",
      "Step: 383\tloss: 872.721358\n",
      "Step: 384\tloss: 872.721336\n",
      "Step: 385\tloss: 872.721265\n",
      "Step: 386\tloss: 872.721247\n",
      "Step: 387\tloss: 872.721258\n",
      "Step: 388\tloss: 872.721223\n",
      "Step: 389\tloss: 872.721180\n",
      "Step: 390\tloss: 872.721175\n",
      "pval: 0.000012\n",
      "Step: 391\tloss: 872.721169\n",
      "Step: 392\tloss: 872.721146\n",
      "Step: 393\tloss: 872.721137\n",
      "Step: 394\tloss: 872.721134\n",
      "Step: 395\tloss: 872.721116\n",
      "Step: 396\tloss: 872.721101\n",
      "Step: 397\tloss: 872.721097\n",
      "Step: 398\tloss: 872.721090\n",
      "Step: 399\tloss: 872.721082\n",
      "Step: 400\tloss: 872.721080\n",
      "pval: 0.000017\n",
      "Step: 401\tloss: 872.721074\n",
      "Step: 402\tloss: 872.721064\n",
      "Step: 403\tloss: 872.721061\n",
      "Step: 404\tloss: 872.721059\n",
      "Step: 405\tloss: 872.721053\n",
      "Step: 406\tloss: 872.721050\n",
      "Step: 407\tloss: 872.721050\n",
      "Step: 408\tloss: 872.721045\n",
      "Step: 409\tloss: 872.721042\n",
      "Step: 410\tloss: 872.721041\n",
      "pval: 0.000024\n",
      "Step: 411\tloss: 872.721039\n",
      "Step: 412\tloss: 872.721036\n",
      "Step: 413\tloss: 872.721035\n",
      "Step: 414\tloss: 872.721034\n",
      "Step: 415\tloss: 872.721032\n",
      "Step: 416\tloss: 872.721031\n",
      "Step: 417\tloss: 872.721030\n",
      "Step: 418\tloss: 872.721029\n",
      "Step: 419\tloss: 872.721029\n",
      "Step: 420\tloss: 872.721027\n",
      "pval: 0.000023\n",
      "Step: 421\tloss: 872.721026\n",
      "Step: 422\tloss: 872.721026\n",
      "Step: 423\tloss: 872.721025\n",
      "Step: 424\tloss: 872.721024\n",
      "Step: 425\tloss: 872.721025\n",
      "Step: 426\tloss: 872.721024\n",
      "Step: 427\tloss: 872.721023\n",
      "Step: 428\tloss: 872.721023\n",
      "Step: 429\tloss: 872.721023\n",
      "Step: 430\tloss: 872.721022\n",
      "pval: 0.000018\n",
      "Step: 431\tloss: 872.721022\n",
      "Step: 432\tloss: 872.721022\n",
      "Step: 433\tloss: 872.721022\n",
      "Step: 434\tloss: 872.721022\n",
      "Step: 435\tloss: 872.721021\n",
      "Step: 436\tloss: 872.721021\n",
      "Step: 437\tloss: 872.721021\n",
      "Step: 438\tloss: 872.721021\n",
      "Step: 439\tloss: 872.721021\n",
      "Step: 440\tloss: 872.721021\n",
      "pval: 0.000021\n",
      "Step: 441\tloss: 872.721021\n",
      "Step: 442\tloss: 872.721021\n",
      "Step: 443\tloss: 872.721021\n",
      "Step: 444\tloss: 872.721021\n",
      "Step: 445\tloss: 872.721020\n",
      "Step: 446\tloss: 872.721020\n",
      "Step: 447\tloss: 872.721020\n",
      "Step: 448\tloss: 872.721020\n",
      "Step: 449\tloss: 872.721020\n",
      "Step: 450\tloss: 872.721020\n",
      "pval: 0.000024\n",
      "Step: 451\tloss: 872.721020\n",
      "Step: 452\tloss: 872.721020\n",
      "Step: 453\tloss: 872.721020\n",
      "Step: 454\tloss: 872.721020\n",
      "Step: 455\tloss: 872.721020\n",
      "Step: 456\tloss: 872.721020\n",
      "Step: 457\tloss: 872.721020\n",
      "Step: 458\tloss: 872.721020\n",
      "Step: 459\tloss: 872.721020\n",
      "Step: 460\tloss: 872.721020\n",
      "pval: 0.000024\n",
      "Step: 461\tloss: 872.721020\n",
      "Step: 462\tloss: 872.721020\n",
      "Step: 463\tloss: 872.721020\n",
      "Step: 464\tloss: 872.721020\n",
      "Step: 465\tloss: 872.721020\n",
      "Step: 466\tloss: 872.721020\n",
      "Step: 467\tloss: 872.721020\n",
      "Step: 468\tloss: 872.721020\n",
      "Step: 469\tloss: 872.721020\n",
      "Step: 470\tloss: 872.721020\n",
      "pval: 0.000015\n",
      "Step: 471\tloss: 872.721020\n",
      "Step: 472\tloss: 872.721020\n",
      "Step: 473\tloss: 872.721020\n",
      "Step: 474\tloss: 872.721020\n",
      "Step: 475\tloss: 872.721020\n",
      "Step: 476\tloss: 872.721020\n",
      "Step: 477\tloss: 872.721020\n",
      "Step: 478\tloss: 872.721020\n",
      "Step: 479\tloss: 872.721020\n",
      "Step: 480\tloss: 872.721020\n",
      "pval: 0.000014\n",
      "Step: 481\tloss: 872.721020\n",
      "Step: 482\tloss: 872.721020\n",
      "Step: 483\tloss: 872.721020\n",
      "Step: 484\tloss: 872.721020\n",
      "Step: 485\tloss: 872.721020\n",
      "Step: 486\tloss: 872.721020\n",
      "Step: 487\tloss: 872.721020\n",
      "Step: 488\tloss: 872.721020\n",
      "Step: 489\tloss: 872.721020\n",
      "Step: 490\tloss: 872.721020\n",
      "pval: 0.000021\n",
      "Step: 491\tloss: 872.721020\n",
      "Step: 492\tloss: 872.721020\n",
      "Step: 493\tloss: 872.721020\n",
      "Step: 494\tloss: 872.721020\n",
      "Step: 495\tloss: 872.721020\n",
      "Step: 496\tloss: 872.721020\n",
      "Step: 497\tloss: 872.721020\n",
      "Step: 498\tloss: 872.721020\n",
      "Step: 499\tloss: 872.721020\n",
      "Step: 500\tloss: 872.721020\n",
      "pval: 0.000019\n",
      "Step: 501\tloss: 872.721020\n",
      "Step: 502\tloss: 872.721020\n",
      "Step: 503\tloss: 872.721020\n",
      "Step: 504\tloss: 872.721020\n",
      "Step: 505\tloss: 872.721020\n",
      "Step: 506\tloss: 872.721020\n",
      "Step: 507\tloss: 872.721020\n",
      "Step: 508\tloss: 872.721020\n",
      "Step: 509\tloss: 872.721020\n",
      "Step: 510\tloss: 872.721020\n",
      "pval: 0.000018\n",
      "Step: 511\tloss: 872.721020\n",
      "Step: 512\tloss: 872.721020\n",
      "Step: 513\tloss: 872.721020\n",
      "Step: 514\tloss: 872.721020\n",
      "Step: 515\tloss: 872.721020\n",
      "Step: 516\tloss: 872.721020\n",
      "Step: 517\tloss: 872.721020\n",
      "Step: 518\tloss: 872.721020\n",
      "Step: 519\tloss: 872.721020\n",
      "Step: 520\tloss: 872.721020\n",
      "pval: 0.000014\n",
      "Step: 521\tloss: 872.721020\n",
      "Step: 522\tloss: 872.721020\n",
      "Step: 523\tloss: 872.721020\n",
      "Step: 524\tloss: 872.721020\n",
      "Step: 525\tloss: 872.721020\n",
      "Step: 526\tloss: 872.721020\n",
      "Step: 527\tloss: 872.721020\n",
      "Step: 528\tloss: 872.721020\n",
      "Step: 529\tloss: 872.721020\n",
      "Step: 530\tloss: 872.721020\n",
      "pval: 0.000014\n",
      "Step: 531\tloss: 872.721020\n",
      "Step: 532\tloss: 872.721020\n",
      "Step: 533\tloss: 872.721020\n",
      "Step: 534\tloss: 872.721020\n",
      "Step: 535\tloss: 872.721020\n",
      "Step: 536\tloss: 872.721020\n",
      "Step: 537\tloss: 872.721020\n",
      "Step: 538\tloss: 872.721020\n",
      "Step: 539\tloss: 872.721020\n",
      "Step: 540\tloss: 872.721020\n",
      "pval: 0.000015\n",
      "Step: 541\tloss: 872.721020\n",
      "Step: 542\tloss: 872.721020\n",
      "Step: 543\tloss: 872.721020\n",
      "Step: 544\tloss: 872.721020\n",
      "Step: 545\tloss: 872.721020\n",
      "Step: 546\tloss: 872.721020\n",
      "Step: 547\tloss: 872.721020\n",
      "Step: 548\tloss: 872.721020\n",
      "Step: 549\tloss: 872.721020\n",
      "Step: 550\tloss: 872.721020\n",
      "pval: 0.000010\n",
      "Step: 551\tloss: 872.721020\n",
      "Step: 552\tloss: 872.721020\n",
      "Step: 553\tloss: 872.721020\n",
      "Step: 554\tloss: 872.721020\n",
      "Step: 555\tloss: 872.721020\n",
      "Step: 556\tloss: 872.721020\n",
      "Step: 557\tloss: 872.721020\n",
      "Step: 558\tloss: 872.721020\n",
      "Step: 559\tloss: 872.721020\n",
      "Step: 560\tloss: 872.721020\n",
      "pval: 0.007744\n",
      "Step: 561\tloss: 872.721020\n",
      "Step: 562\tloss: 872.721020\n",
      "Step: 563\tloss: 872.721020\n",
      "Step: 564\tloss: 872.721020\n",
      "Step: 565\tloss: 872.721020\n",
      "Step: 566\tloss: 872.721020\n",
      "Step: 567\tloss: 872.721020\n",
      "Step: 568\tloss: 872.721020\n",
      "Step: 569\tloss: 872.721020\n",
      "Step: 570\tloss: 872.721020\n",
      "pval: 0.078706\n",
      "Training sequence #2 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence(\"AUTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results\n",
    "\n",
    "The fitted parameters can be retrieved by calling the corresponding parameters of `estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_loc_params: 5, features: 100)>\n",
       "array([[ 3.459502,  5.869586,  8.818344, ...,  9.101997,  7.861408,  6.944665],\n",
       "       [-0.447363, -0.159992,  0.538026, ...,  0.336276,  0.183903,  0.351782],\n",
       "       [ 0.663368,  0.178523, -0.150581, ...,  0.342808,  0.207144,  0.376436],\n",
       "       [ 0.334399, -0.200837, -0.191472, ...,  0.388376, -0.373587, -0.510122],\n",
       "       [-0.356504, -0.160303,  0.377781, ..., -0.429275, -0.298389,  0.101208]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "    feature_allzero    (features) bool False False False False False False ...\n",
       "  * features           (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_scale_params: 5, features: 100)>\n",
       "array([[ 2.239549,  1.369592,  2.05534 , ...,  1.12565 ,  2.352958,  2.034455],\n",
       "       [ 0.006636,  0.145781, -0.060395, ...,  0.463318,  0.317246, -0.464744],\n",
       "       [ 0.278803,  0.754612,  0.341749, ..., -0.430314,  0.52776 ,  0.419028],\n",
       "       [ 0.612083,  0.302963,  0.403214, ..., -0.22296 ,  0.194152,  0.457432],\n",
       "       [ 0.318217,  0.142252,  0.34286 , ...,  0.518943,  0.036133,  0.461486]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U14 'Intercept' 'batch[T.1]' ...\n",
       "    feature_allzero      (features) bool False False False False False False ...\n",
       "  * features             (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc, sim.par_link_loc)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.07\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
