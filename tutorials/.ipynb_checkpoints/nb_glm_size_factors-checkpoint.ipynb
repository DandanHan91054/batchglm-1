{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=1000)\n",
    "sim.generate()\n",
    "X = sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superimpose library size effects on counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_factors = numpy.random.normal(loc=1, scale=0.5, size=X.shape[0]) # draw random factors\n",
    "size_factors[size_factors < 0.2] = 0.2 # threshold\n",
    "X = np.round(X*np.repeat(np.expand_dims(size_factors, axis=1), axis=1, repeats=X.shape[1])) # scale counts and round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size factor scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.036100913313054886"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(X, axis=1).values/np.mean(np.sum(X, axis=1)).values - size_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add continuous covariate to desing loc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_loc = sim.design_loc\n",
    "design_loc = np.hstack([design_loc.values, np.expand_dims(numpy.random.normal(loc=1, scale=1, size=design_loc.shape[0]), axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=size_factors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, init_a='standard', init_b='standard')\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 9349.023429\n",
      "Step: 2\tloss: 9348.346032\n",
      "Step: 3\tloss: 9305.726775\n",
      "Step: 4\tloss: 9262.448597\n",
      "Step: 5\tloss: 9203.921256\n",
      "Step: 6\tloss: 9200.577417\n",
      "Step: 7\tloss: 9051.681319\n",
      "Step: 8\tloss: 9059.095941\n",
      "Step: 9\tloss: 9047.526901\n",
      "Step: 10\tloss: 9038.011104\n",
      "Step: 11\tloss: 8959.259093\n",
      "Step: 12\tloss: 8918.269985\n",
      "Step: 13\tloss: 8930.100143\n",
      "Step: 14\tloss: 8938.436552\n",
      "Step: 15\tloss: 8917.905964\n",
      "Step: 16\tloss: 8840.571024\n",
      "Step: 17\tloss: 8885.640464\n",
      "Step: 18\tloss: 8886.688097\n",
      "Step: 19\tloss: 8818.634583\n",
      "Step: 20\tloss: 8872.522261\n",
      "Step: 21\tloss: 8877.302940\n",
      "Step: 22\tloss: 8847.278586\n",
      "Step: 23\tloss: 8826.096943\n",
      "Step: 24\tloss: 8840.475552\n",
      "Step: 25\tloss: 8833.890737\n",
      "Step: 26\tloss: 8843.111580\n",
      "Step: 27\tloss: 8853.530884\n",
      "Step: 28\tloss: 8823.257045\n",
      "Step: 29\tloss: 8832.820885\n",
      "Step: 30\tloss: 8798.619325\n",
      "Step: 31\tloss: 8837.103206\n",
      "Step: 32\tloss: 8850.206108\n",
      "Step: 33\tloss: 8823.719159\n",
      "Step: 34\tloss: 8803.179267\n",
      "Step: 35\tloss: 8843.558876\n",
      "Step: 36\tloss: 8809.229781\n",
      "Step: 37\tloss: 8809.495391\n",
      "Step: 38\tloss: 8793.819811\n",
      "Step: 39\tloss: 8858.126514\n",
      "Step: 40\tloss: 8789.590417\n",
      "Step: 41\tloss: 8824.381401\n",
      "Step: 42\tloss: 8814.798838\n",
      "Step: 43\tloss: 8791.599450\n",
      "Step: 44\tloss: 8801.748707\n",
      "Step: 45\tloss: 8808.948933\n",
      "Step: 46\tloss: 8818.767924\n",
      "Step: 47\tloss: 8805.822827\n",
      "Step: 48\tloss: 8786.019018\n",
      "Step: 49\tloss: 8780.017206\n",
      "Step: 50\tloss: 8790.209478\n",
      "Step: 51\tloss: 8833.026337\n",
      "Step: 52\tloss: 8805.411059\n",
      "Step: 53\tloss: 8810.547083\n",
      "Step: 54\tloss: 8776.613548\n",
      "Step: 55\tloss: 8805.050996\n",
      "Step: 56\tloss: 8805.797058\n",
      "Step: 57\tloss: 8782.242097\n",
      "Step: 58\tloss: 8819.986619\n",
      "Step: 59\tloss: 8812.794151\n",
      "Step: 60\tloss: 8774.162095\n",
      "Step: 61\tloss: 8792.111032\n",
      "Step: 62\tloss: 8795.525152\n",
      "Step: 63\tloss: 8793.334992\n",
      "Step: 64\tloss: 8800.705229\n",
      "Step: 65\tloss: 8824.226697\n",
      "Step: 66\tloss: 8768.702284\n",
      "Step: 67\tloss: 8806.988903\n",
      "Step: 68\tloss: 8775.338851\n",
      "Step: 69\tloss: 8812.211876\n",
      "Step: 70\tloss: 8807.599299\n",
      "Step: 71\tloss: 8777.741968\n",
      "Step: 72\tloss: 8772.390563\n",
      "Step: 73\tloss: 8807.197106\n",
      "Step: 74\tloss: 8785.785681\n",
      "Step: 75\tloss: 8768.952930\n",
      "Step: 76\tloss: 8803.598381\n",
      "Step: 77\tloss: 8779.829319\n",
      "Step: 78\tloss: 8814.926574\n",
      "Step: 79\tloss: 8772.469467\n",
      "Step: 80\tloss: 8794.278441\n",
      "Step: 81\tloss: 8822.996453\n",
      "Step: 82\tloss: 8783.714326\n",
      "Step: 83\tloss: 8782.285549\n",
      "Step: 84\tloss: 8768.820249\n",
      "Step: 85\tloss: 8777.299481\n",
      "Step: 86\tloss: 8796.058095\n",
      "Step: 87\tloss: 8783.856623\n",
      "Step: 88\tloss: 8797.758730\n",
      "Step: 89\tloss: 8806.175080\n",
      "Step: 90\tloss: 8789.362911\n",
      "Step: 91\tloss: 8765.120974\n",
      "Step: 92\tloss: 8791.238628\n",
      "Step: 93\tloss: 8837.357210\n",
      "Step: 94\tloss: 8791.585281\n",
      "Step: 95\tloss: 8770.049350\n",
      "Step: 96\tloss: 8750.578726\n",
      "Step: 97\tloss: 8759.908205\n",
      "Step: 98\tloss: 8781.466248\n",
      "Step: 99\tloss: 8836.457044\n",
      "Step: 100\tloss: 8769.616016\n",
      "Step: 101\tloss: 8791.984348\n",
      "Step: 102\tloss: 8791.824413\n",
      "Step: 103\tloss: 8794.044441\n",
      "Step: 104\tloss: 8767.397924\n",
      "Step: 105\tloss: 8784.542515\n",
      "Step: 106\tloss: 8785.129460\n",
      "Step: 107\tloss: 8781.810710\n",
      "Step: 108\tloss: 8792.491161\n",
      "Step: 109\tloss: 8777.934813\n",
      "Step: 110\tloss: 8811.221053\n",
      "Step: 111\tloss: 8768.381990\n",
      "Step: 112\tloss: 8784.727917\n",
      "Step: 113\tloss: 8808.666373\n",
      "Step: 114\tloss: 8787.735912\n",
      "Step: 115\tloss: 8774.516124\n",
      "Step: 116\tloss: 8770.070498\n",
      "Step: 117\tloss: 8787.877917\n",
      "Step: 118\tloss: 8810.286351\n",
      "Step: 119\tloss: 8750.137814\n",
      "Step: 120\tloss: 8791.645672\n",
      "Step: 121\tloss: 8766.483314\n",
      "Step: 122\tloss: 8749.782357\n",
      "Step: 123\tloss: 8794.746830\n",
      "Step: 124\tloss: 8828.076604\n",
      "Step: 125\tloss: 8760.367790\n",
      "Step: 126\tloss: 8805.650843\n",
      "Step: 127\tloss: 8775.554901\n",
      "Step: 128\tloss: 8796.955575\n",
      "Step: 129\tloss: 8773.755261\n",
      "Step: 130\tloss: 8796.618128\n",
      "Step: 131\tloss: 8805.234212\n",
      "Step: 132\tloss: 8761.855673\n",
      "Step: 133\tloss: 8776.596419\n",
      "Step: 134\tloss: 8802.039323\n",
      "Step: 135\tloss: 8769.230968\n",
      "Step: 136\tloss: 8788.684188\n",
      "Step: 137\tloss: 8782.768270\n",
      "Step: 138\tloss: 8789.192577\n",
      "Step: 139\tloss: 8826.872559\n",
      "Step: 140\tloss: 8737.346276\n",
      "Step: 141\tloss: 8781.102689\n",
      "Step: 142\tloss: 8798.181744\n",
      "Step: 143\tloss: 8773.709002\n",
      "Step: 144\tloss: 8782.727840\n",
      "Step: 145\tloss: 8808.480850\n",
      "Step: 146\tloss: 8771.293029\n",
      "Step: 147\tloss: 8773.132378\n",
      "Step: 148\tloss: 8782.366494\n",
      "Step: 149\tloss: 8743.888338\n",
      "Step: 150\tloss: 8815.406685\n",
      "Step: 151\tloss: 8754.364623\n",
      "Step: 152\tloss: 8821.361963\n",
      "Step: 153\tloss: 8789.702738\n",
      "Step: 154\tloss: 8783.972828\n",
      "Step: 155\tloss: 8801.184259\n",
      "Step: 156\tloss: 8759.425962\n",
      "Step: 157\tloss: 8754.343698\n",
      "Step: 158\tloss: 8793.325433\n",
      "Step: 159\tloss: 8767.237517\n",
      "Step: 160\tloss: 8819.343968\n",
      "Step: 161\tloss: 8807.533894\n",
      "Step: 162\tloss: 8764.177811\n",
      "Step: 163\tloss: 8766.465589\n",
      "Step: 164\tloss: 8796.654776\n",
      "Step: 165\tloss: 8809.710377\n",
      "Step: 166\tloss: 8756.896104\n",
      "Step: 167\tloss: 8750.216973\n",
      "Step: 168\tloss: 8817.858142\n",
      "Step: 169\tloss: 8773.585997\n",
      "Step: 170\tloss: 8815.506962\n",
      "Step: 171\tloss: 8755.119576\n",
      "Step: 172\tloss: 8790.372581\n",
      "Step: 173\tloss: 8770.963825\n",
      "Step: 174\tloss: 8791.859621\n",
      "Step: 175\tloss: 8773.771828\n",
      "Step: 176\tloss: 8797.682253\n",
      "Step: 177\tloss: 8782.981605\n",
      "Step: 178\tloss: 8787.458409\n",
      "Step: 179\tloss: 8796.955404\n",
      "Step: 180\tloss: 8766.763330\n",
      "Step: 181\tloss: 8787.526924\n",
      "Step: 182\tloss: 8785.458565\n",
      "Step: 183\tloss: 8744.676378\n",
      "Step: 184\tloss: 8816.775487\n",
      "Step: 185\tloss: 8795.275952\n",
      "Step: 186\tloss: 8804.563826\n",
      "Step: 187\tloss: 8770.087519\n",
      "Step: 188\tloss: 8764.153858\n",
      "Step: 189\tloss: 8792.839571\n",
      "Step: 190\tloss: 8800.676170\n",
      "Step: 191\tloss: 8776.412866\n",
      "Step: 192\tloss: 8764.342731\n",
      "Step: 193\tloss: 8764.928246\n",
      "Step: 194\tloss: 8795.724251\n",
      "Step: 195\tloss: 8768.066690\n",
      "Step: 196\tloss: 8805.621542\n",
      "Step: 197\tloss: 8772.259463\n",
      "Step: 198\tloss: 8784.681831\n",
      "Step: 199\tloss: 8770.869461\n",
      "Step: 200\tloss: 8807.007768\n",
      "pval: 0.000000\n",
      "Step: 201\tloss: 8765.753728\n",
      "Step: 202\tloss: 8806.237796\n",
      "Step: 203\tloss: 8788.737606\n",
      "Step: 204\tloss: 8773.814648\n",
      "Step: 205\tloss: 8789.985185\n",
      "Step: 206\tloss: 8757.457232\n",
      "Step: 207\tloss: 8799.229056\n",
      "Step: 208\tloss: 8787.807095\n",
      "Step: 209\tloss: 8764.272427\n",
      "Step: 210\tloss: 8777.740023\n",
      "Step: 211\tloss: 8795.316227\n",
      "Step: 212\tloss: 8796.951644\n",
      "Step: 213\tloss: 8779.403993\n",
      "Step: 214\tloss: 8741.337119\n",
      "Step: 215\tloss: 8809.600481\n",
      "Step: 216\tloss: 8804.044460\n",
      "Step: 217\tloss: 8762.912171\n",
      "Step: 218\tloss: 8791.102883\n",
      "Step: 219\tloss: 8806.018930\n",
      "Step: 220\tloss: 8774.655721\n",
      "Step: 221\tloss: 8753.454715\n",
      "Step: 222\tloss: 8791.725217\n",
      "Step: 223\tloss: 8806.831648\n",
      "Step: 224\tloss: 8783.101605\n",
      "Step: 225\tloss: 8811.741116\n",
      "Step: 226\tloss: 8762.925813\n",
      "Step: 227\tloss: 8789.393547\n",
      "Step: 228\tloss: 8771.162483\n",
      "Step: 229\tloss: 8816.985336\n",
      "Step: 230\tloss: 8752.020105\n",
      "Step: 231\tloss: 8790.022930\n",
      "Step: 232\tloss: 8775.639048\n",
      "Step: 233\tloss: 8761.271057\n",
      "Step: 234\tloss: 8800.297042\n",
      "Step: 235\tloss: 8784.400208\n",
      "Step: 236\tloss: 8788.576956\n",
      "Step: 237\tloss: 8806.683063\n",
      "Step: 238\tloss: 8776.335907\n",
      "Step: 239\tloss: 8798.866281\n",
      "Step: 240\tloss: 8752.335473\n",
      "Step: 241\tloss: 8789.701887\n",
      "Step: 242\tloss: 8777.087126\n",
      "Step: 243\tloss: 8777.213626\n",
      "Step: 244\tloss: 8789.909883\n",
      "Step: 245\tloss: 8770.029515\n",
      "Step: 246\tloss: 8805.895203\n",
      "Step: 247\tloss: 8764.552926\n",
      "Step: 248\tloss: 8794.017181\n",
      "Step: 249\tloss: 8780.202257\n",
      "Step: 250\tloss: 8814.147522\n",
      "Step: 251\tloss: 8755.849612\n",
      "Step: 252\tloss: 8784.325247\n",
      "Step: 253\tloss: 8756.815706\n",
      "Step: 254\tloss: 8811.798726\n",
      "Step: 255\tloss: 8772.238803\n",
      "Step: 256\tloss: 8794.133322\n",
      "Step: 257\tloss: 8840.424448\n",
      "Step: 258\tloss: 8806.245684\n",
      "Step: 259\tloss: 8720.987503\n",
      "Step: 260\tloss: 8766.809495\n",
      "Step: 261\tloss: 8801.954823\n",
      "Step: 262\tloss: 8758.911551\n",
      "Step: 263\tloss: 8765.365496\n",
      "Step: 264\tloss: 8808.403394\n",
      "Step: 265\tloss: 8789.479148\n",
      "Step: 266\tloss: 8769.264111\n",
      "Step: 267\tloss: 8798.034913\n",
      "Step: 268\tloss: 8777.918754\n",
      "Step: 269\tloss: 8725.967730\n",
      "Step: 270\tloss: 8794.875534\n",
      "Step: 271\tloss: 8813.610336\n",
      "Step: 272\tloss: 8800.594298\n",
      "Step: 273\tloss: 8809.968003\n",
      "Step: 274\tloss: 8784.020336\n",
      "Step: 275\tloss: 8788.493968\n",
      "Step: 276\tloss: 8752.763491\n",
      "Step: 277\tloss: 8768.766704\n",
      "Step: 278\tloss: 8775.584086\n",
      "Step: 279\tloss: 8767.314149\n",
      "Step: 280\tloss: 8823.798180\n",
      "Step: 281\tloss: 8754.817059\n",
      "Step: 282\tloss: 8790.239336\n",
      "Step: 283\tloss: 8759.718348\n",
      "Step: 284\tloss: 8830.389523\n",
      "Step: 285\tloss: 8775.271235\n",
      "Step: 286\tloss: 8790.690158\n",
      "Step: 287\tloss: 8779.812038\n",
      "Step: 288\tloss: 8789.395476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 289\tloss: 8754.385319\n",
      "Step: 290\tloss: 8804.990925\n",
      "Step: 291\tloss: 8770.171753\n",
      "Step: 292\tloss: 8805.411745\n",
      "Step: 293\tloss: 8800.644576\n",
      "Step: 294\tloss: 8723.383024\n",
      "Step: 295\tloss: 8796.725462\n",
      "Step: 296\tloss: 8814.324475\n",
      "Step: 297\tloss: 8786.904414\n",
      "Step: 298\tloss: 8752.297882\n",
      "Step: 299\tloss: 8781.539296\n",
      "Step: 300\tloss: 8814.298923\n",
      "pval: 0.433574\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc[:-1,:].values, sim.par_link_loc.values)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.07\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=sim.design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, init_a='standard', init_b='standard')\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 9523.144088\n",
      "Step: 2\tloss: 9478.396673\n",
      "Step: 3\tloss: 9413.858916\n",
      "Step: 4\tloss: 9362.095233\n",
      "Step: 5\tloss: 9356.345579\n",
      "Step: 6\tloss: 9336.932951\n",
      "Step: 7\tloss: 9357.801661\n",
      "Step: 8\tloss: 9333.369691\n",
      "Step: 9\tloss: 9326.729463\n",
      "Step: 10\tloss: 9358.242468\n",
      "Step: 11\tloss: 9368.139463\n",
      "Step: 12\tloss: 9286.259570\n",
      "Step: 13\tloss: 9312.619368\n",
      "Step: 14\tloss: 9327.936684\n",
      "Step: 15\tloss: 9290.155924\n",
      "Step: 16\tloss: 9341.765085\n",
      "Step: 17\tloss: 9296.375203\n",
      "Step: 18\tloss: 9318.889652\n",
      "Step: 19\tloss: 9329.637868\n",
      "Step: 20\tloss: 9308.188994\n",
      "Step: 21\tloss: 9321.345349\n",
      "Step: 22\tloss: 9299.253636\n",
      "Step: 23\tloss: 9287.535792\n",
      "Step: 24\tloss: 9320.655017\n",
      "Step: 25\tloss: 9309.188216\n",
      "Step: 26\tloss: 9308.537540\n",
      "Step: 27\tloss: 9284.790006\n",
      "Step: 28\tloss: 9313.015483\n",
      "Step: 29\tloss: 9305.465790\n",
      "Step: 30\tloss: 9333.375514\n",
      "Step: 31\tloss: 9287.157703\n",
      "Step: 32\tloss: 9284.538404\n",
      "Step: 33\tloss: 9312.136113\n",
      "Step: 34\tloss: 9310.497171\n",
      "Step: 35\tloss: 9272.854122\n",
      "Step: 36\tloss: 9309.251946\n",
      "Step: 37\tloss: 9306.476747\n",
      "Step: 38\tloss: 9291.749048\n",
      "Step: 39\tloss: 9297.864005\n",
      "Step: 40\tloss: 9302.692761\n",
      "Step: 41\tloss: 9267.301428\n",
      "Step: 42\tloss: 9325.671064\n",
      "Step: 43\tloss: 9292.631619\n",
      "Step: 44\tloss: 9311.083986\n",
      "Step: 45\tloss: 9301.937149\n",
      "Step: 46\tloss: 9289.002715\n",
      "Step: 47\tloss: 9306.920910\n",
      "Step: 48\tloss: 9298.906235\n",
      "Step: 49\tloss: 9328.214755\n",
      "Step: 50\tloss: 9283.624848\n",
      "Step: 51\tloss: 9303.940422\n",
      "Step: 52\tloss: 9280.802438\n",
      "Step: 53\tloss: 9279.332746\n",
      "Step: 54\tloss: 9291.658436\n",
      "Step: 55\tloss: 9310.032801\n",
      "Step: 56\tloss: 9312.002401\n",
      "Step: 57\tloss: 9308.765460\n",
      "Step: 58\tloss: 9304.998927\n",
      "Step: 59\tloss: 9297.867304\n",
      "Step: 60\tloss: 9280.857410\n",
      "Step: 61\tloss: 9315.764578\n",
      "Step: 62\tloss: 9309.335138\n",
      "Step: 63\tloss: 9294.744910\n",
      "Step: 64\tloss: 9272.328621\n",
      "Step: 65\tloss: 9264.865594\n",
      "Step: 66\tloss: 9304.265945\n",
      "Step: 67\tloss: 9333.205315\n",
      "Step: 68\tloss: 9289.964549\n",
      "Step: 69\tloss: 9288.954727\n",
      "Step: 70\tloss: 9299.882444\n",
      "Step: 71\tloss: 9306.695381\n",
      "Step: 72\tloss: 9297.672810\n",
      "Step: 73\tloss: 9311.295811\n",
      "Step: 74\tloss: 9285.808041\n",
      "Step: 75\tloss: 9297.211052\n",
      "Step: 76\tloss: 9297.516465\n",
      "Step: 77\tloss: 9288.855113\n",
      "Step: 78\tloss: 9323.027569\n",
      "Step: 79\tloss: 9297.270262\n",
      "Step: 80\tloss: 9284.004483\n",
      "Step: 81\tloss: 9308.398228\n",
      "Step: 82\tloss: 9265.946490\n",
      "Step: 83\tloss: 9294.198136\n",
      "Step: 84\tloss: 9329.114909\n",
      "Step: 85\tloss: 9294.964201\n",
      "Step: 86\tloss: 9303.809631\n",
      "Step: 87\tloss: 9290.489157\n",
      "Step: 88\tloss: 9305.053439\n",
      "Step: 89\tloss: 9335.232745\n",
      "Step: 90\tloss: 9273.691974\n",
      "Step: 91\tloss: 9283.556679\n",
      "Step: 92\tloss: 9299.731792\n",
      "Step: 93\tloss: 9281.842257\n",
      "Step: 94\tloss: 9301.967590\n",
      "Step: 95\tloss: 9307.835844\n",
      "Step: 96\tloss: 9303.275803\n",
      "Step: 97\tloss: 9265.294665\n",
      "Step: 98\tloss: 9325.805422\n",
      "Step: 99\tloss: 9298.841812\n",
      "Step: 100\tloss: 9303.928486\n",
      "Step: 101\tloss: 9287.432280\n",
      "Step: 102\tloss: 9298.816311\n",
      "Step: 103\tloss: 9314.558201\n",
      "Step: 104\tloss: 9292.689257\n",
      "Step: 105\tloss: 9292.792352\n",
      "Step: 106\tloss: 9326.339230\n",
      "Step: 107\tloss: 9306.847258\n",
      "Step: 108\tloss: 9270.675566\n",
      "Step: 109\tloss: 9308.379943\n",
      "Step: 110\tloss: 9270.612532\n",
      "Step: 111\tloss: 9308.227809\n",
      "Step: 112\tloss: 9310.738845\n",
      "Step: 113\tloss: 9299.528683\n",
      "Step: 114\tloss: 9271.970241\n",
      "Step: 115\tloss: 9328.381058\n",
      "Step: 116\tloss: 9298.469347\n",
      "Step: 117\tloss: 9300.229690\n",
      "Step: 118\tloss: 9296.245817\n",
      "Step: 119\tloss: 9300.800931\n",
      "Step: 120\tloss: 9298.380382\n",
      "Step: 121\tloss: 9293.744596\n",
      "Step: 122\tloss: 9329.000979\n",
      "Step: 123\tloss: 9294.959102\n",
      "Step: 124\tloss: 9279.084626\n",
      "Step: 125\tloss: 9272.232433\n",
      "Step: 126\tloss: 9341.344424\n",
      "Step: 127\tloss: 9280.095044\n",
      "Step: 128\tloss: 9304.056692\n",
      "Step: 129\tloss: 9297.009706\n",
      "Step: 130\tloss: 9283.652110\n",
      "Step: 131\tloss: 9323.638330\n",
      "Step: 132\tloss: 9290.967357\n",
      "Step: 133\tloss: 9298.614629\n",
      "Step: 134\tloss: 9303.294998\n",
      "Step: 135\tloss: 9289.065274\n",
      "Step: 136\tloss: 9306.771867\n",
      "Step: 137\tloss: 9315.843352\n",
      "Step: 138\tloss: 9298.942083\n",
      "Step: 139\tloss: 9281.130041\n",
      "Step: 140\tloss: 9299.959473\n",
      "Step: 141\tloss: 9296.765859\n",
      "Step: 142\tloss: 9313.272157\n",
      "Step: 143\tloss: 9274.778896\n",
      "Step: 144\tloss: 9309.848603\n",
      "Step: 145\tloss: 9253.618270\n",
      "Step: 146\tloss: 9330.681367\n",
      "Step: 147\tloss: 9286.625284\n",
      "Step: 148\tloss: 9324.633488\n",
      "Step: 149\tloss: 9297.656320\n",
      "Step: 150\tloss: 9345.853651\n",
      "Step: 151\tloss: 9272.201221\n",
      "Step: 152\tloss: 9282.985835\n",
      "Step: 153\tloss: 9280.465491\n",
      "Step: 154\tloss: 9319.190842\n",
      "Step: 155\tloss: 9323.864513\n",
      "Step: 156\tloss: 9276.350592\n",
      "Step: 157\tloss: 9295.534591\n",
      "Step: 158\tloss: 9294.340461\n",
      "Step: 159\tloss: 9302.990684\n",
      "Step: 160\tloss: 9301.910046\n",
      "Step: 161\tloss: 9302.282971\n",
      "Step: 162\tloss: 9301.317798\n",
      "Step: 163\tloss: 9292.613797\n",
      "Step: 164\tloss: 9298.828307\n",
      "Step: 165\tloss: 9297.380142\n",
      "Step: 166\tloss: 9334.700897\n",
      "Step: 167\tloss: 9281.146029\n",
      "Step: 168\tloss: 9283.036408\n",
      "Step: 169\tloss: 9308.086862\n",
      "Step: 170\tloss: 9301.586796\n",
      "Step: 171\tloss: 9302.456643\n",
      "Step: 172\tloss: 9281.365803\n",
      "Step: 173\tloss: 9297.092861\n",
      "Step: 174\tloss: 9319.241205\n",
      "Step: 175\tloss: 9289.357852\n",
      "Step: 176\tloss: 9290.452111\n",
      "Step: 177\tloss: 9256.993378\n",
      "Step: 178\tloss: 9347.254798\n",
      "Step: 179\tloss: 9295.387788\n",
      "Step: 180\tloss: 9295.647737\n",
      "Step: 181\tloss: 9294.592855\n",
      "Step: 182\tloss: 9294.332383\n",
      "Step: 183\tloss: 9286.838303\n",
      "Step: 184\tloss: 9317.081624\n",
      "Step: 185\tloss: 9308.260099\n",
      "Step: 186\tloss: 9305.791338\n",
      "Step: 187\tloss: 9275.575847\n",
      "Step: 188\tloss: 9304.168154\n",
      "Step: 189\tloss: 9289.063880\n",
      "Step: 190\tloss: 9293.889917\n",
      "Step: 191\tloss: 9311.717291\n",
      "Step: 192\tloss: 9300.147904\n",
      "Step: 193\tloss: 9282.974965\n",
      "Step: 194\tloss: 9325.017049\n",
      "Step: 195\tloss: 9302.712251\n",
      "Step: 196\tloss: 9281.837459\n",
      "Step: 197\tloss: 9284.598304\n",
      "Step: 198\tloss: 9308.801742\n",
      "Step: 199\tloss: 9319.329505\n",
      "Step: 200\tloss: 9283.884867\n",
      "pval: 0.003583\n",
      "Step: 201\tloss: 9298.448638\n",
      "Step: 202\tloss: 9306.197448\n",
      "Step: 203\tloss: 9286.727027\n",
      "Step: 204\tloss: 9300.851439\n",
      "Step: 205\tloss: 9291.907541\n",
      "Step: 206\tloss: 9299.677697\n",
      "Step: 207\tloss: 9307.490512\n",
      "Step: 208\tloss: 9296.498230\n",
      "Step: 209\tloss: 9272.855404\n",
      "Step: 210\tloss: 9322.495793\n",
      "Step: 211\tloss: 9285.158723\n",
      "Step: 212\tloss: 9314.233614\n",
      "Step: 213\tloss: 9294.423883\n",
      "Step: 214\tloss: 9309.587720\n",
      "Step: 215\tloss: 9270.444565\n",
      "Step: 216\tloss: 9319.639411\n",
      "Step: 217\tloss: 9266.519902\n",
      "Step: 218\tloss: 9288.503513\n",
      "Step: 219\tloss: 9314.004866\n",
      "Step: 220\tloss: 9324.791163\n",
      "Step: 221\tloss: 9296.898897\n",
      "Step: 222\tloss: 9301.557039\n",
      "Step: 223\tloss: 9294.476992\n",
      "Step: 224\tloss: 9308.035730\n",
      "Step: 225\tloss: 9293.740304\n",
      "Step: 226\tloss: 9269.886192\n",
      "Step: 227\tloss: 9338.359520\n",
      "Step: 228\tloss: 9302.507772\n",
      "Step: 229\tloss: 9316.937896\n",
      "Step: 230\tloss: 9314.697905\n",
      "Step: 231\tloss: 9291.375959\n",
      "Step: 232\tloss: 9286.087860\n",
      "Step: 233\tloss: 9305.273691\n",
      "Step: 234\tloss: 9299.518118\n",
      "Step: 235\tloss: 9261.708989\n",
      "Step: 236\tloss: 9335.383530\n",
      "Step: 237\tloss: 9285.588384\n",
      "Step: 238\tloss: 9275.416751\n",
      "Step: 239\tloss: 9302.998545\n",
      "Step: 240\tloss: 9335.587224\n",
      "Step: 241\tloss: 9305.274491\n",
      "Step: 242\tloss: 9301.005633\n",
      "Step: 243\tloss: 9289.832594\n",
      "Step: 244\tloss: 9298.250323\n",
      "Step: 245\tloss: 9281.575557\n",
      "Step: 246\tloss: 9256.287411\n",
      "Step: 247\tloss: 9360.052234\n",
      "Step: 248\tloss: 9299.494437\n",
      "Step: 249\tloss: 9296.408466\n",
      "Step: 250\tloss: 9298.365981\n",
      "Step: 251\tloss: 9282.710094\n",
      "Step: 252\tloss: 9322.601827\n",
      "Step: 253\tloss: 9282.325207\n",
      "Step: 254\tloss: 9296.258016\n",
      "Step: 255\tloss: 9314.916633\n",
      "Step: 256\tloss: 9300.299287\n",
      "Step: 257\tloss: 9296.687019\n",
      "Step: 258\tloss: 9291.785242\n",
      "Step: 259\tloss: 9312.886982\n",
      "Step: 260\tloss: 9294.473725\n",
      "Step: 261\tloss: 9291.980401\n",
      "Step: 262\tloss: 9316.398674\n",
      "Step: 263\tloss: 9307.261952\n",
      "Step: 264\tloss: 9279.956927\n",
      "Step: 265\tloss: 9287.746411\n",
      "Step: 266\tloss: 9301.371135\n",
      "Step: 267\tloss: 9305.507932\n",
      "Step: 268\tloss: 9300.074205\n",
      "Step: 269\tloss: 9317.250272\n",
      "Step: 270\tloss: 9281.687069\n",
      "Step: 271\tloss: 9292.891113\n",
      "Step: 272\tloss: 9310.919744\n",
      "Step: 273\tloss: 9292.564949\n",
      "Step: 274\tloss: 9341.334787\n",
      "Step: 275\tloss: 9264.958837\n",
      "Step: 276\tloss: 9299.951467\n",
      "Step: 277\tloss: 9336.228730\n",
      "Step: 278\tloss: 9278.083410\n",
      "Step: 279\tloss: 9281.517917\n",
      "Step: 280\tloss: 9302.214064\n",
      "Step: 281\tloss: 9297.803850\n",
      "Step: 282\tloss: 9326.754397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 283\tloss: 9292.741937\n",
      "Step: 284\tloss: 9280.745204\n",
      "Step: 285\tloss: 9291.840491\n",
      "Step: 286\tloss: 9328.143576\n",
      "Step: 287\tloss: 9297.440166\n",
      "Step: 288\tloss: 9283.101632\n",
      "Step: 289\tloss: 9267.981149\n",
      "Step: 290\tloss: 9327.463614\n",
      "Step: 291\tloss: 9307.681920\n",
      "Step: 292\tloss: 9290.215485\n",
      "Step: 293\tloss: 9292.076662\n",
      "Step: 294\tloss: 9297.269904\n",
      "Step: 295\tloss: 9295.472917\n",
      "Step: 296\tloss: 9310.090950\n",
      "Step: 297\tloss: 9293.645186\n",
      "Step: 298\tloss: 9314.240113\n",
      "Step: 299\tloss: 9274.333341\n",
      "Step: 300\tloss: 9312.695285\n",
      "pval: 0.567980\n",
      "Training sequence #1 complete\n",
      "Beginning with training sequence #2\n",
      "Step: 301\tloss: 9297.443807\n",
      "Step: 302\tloss: 9307.641087\n",
      "Step: 303\tloss: 9298.315078\n",
      "Step: 304\tloss: 9300.108662\n",
      "Step: 305\tloss: 9301.767079\n",
      "Step: 306\tloss: 9299.784890\n",
      "Step: 307\tloss: 9297.809597\n",
      "Step: 308\tloss: 9297.983685\n",
      "Step: 309\tloss: 9299.090942\n",
      "Step: 310\tloss: 9299.081192\n",
      "Step: 311\tloss: 9297.957163\n",
      "Step: 312\tloss: 9297.089310\n",
      "Step: 313\tloss: 9297.128566\n",
      "Step: 314\tloss: 9297.582223\n",
      "Step: 315\tloss: 9297.769621\n",
      "Step: 316\tloss: 9297.539266\n",
      "Step: 317\tloss: 9297.185289\n",
      "Step: 318\tloss: 9297.010042\n",
      "Step: 319\tloss: 9297.028352\n",
      "Step: 320\tloss: 9297.047325\n",
      "pval: 0.009633\n",
      "Step: 321\tloss: 9296.949293\n",
      "Step: 322\tloss: 9296.812026\n",
      "Step: 323\tloss: 9296.760921\n",
      "Step: 324\tloss: 9296.795720\n",
      "Step: 325\tloss: 9296.820131\n",
      "Step: 326\tloss: 9296.778593\n",
      "Step: 327\tloss: 9296.703726\n",
      "Step: 328\tloss: 9296.648302\n",
      "Step: 329\tloss: 9296.620353\n",
      "Step: 330\tloss: 9296.597802\n",
      "pval: 0.000126\n",
      "Step: 331\tloss: 9296.573957\n",
      "Step: 332\tloss: 9296.562359\n",
      "Step: 333\tloss: 9296.563092\n",
      "Step: 334\tloss: 9296.556694\n",
      "Step: 335\tloss: 9296.534623\n",
      "Step: 336\tloss: 9296.509536\n",
      "Step: 337\tloss: 9296.494950\n",
      "Step: 338\tloss: 9296.488562\n",
      "Step: 339\tloss: 9296.480814\n",
      "Step: 340\tloss: 9296.470079\n",
      "pval: 0.000018\n",
      "Step: 341\tloss: 9296.461753\n",
      "Step: 342\tloss: 9296.458401\n",
      "Step: 343\tloss: 9296.456123\n",
      "Step: 344\tloss: 9296.450150\n",
      "Step: 345\tloss: 9296.440818\n",
      "Step: 346\tloss: 9296.432464\n",
      "Step: 347\tloss: 9296.428040\n",
      "Step: 348\tloss: 9296.426729\n",
      "Step: 349\tloss: 9296.425740\n",
      "Step: 350\tloss: 9296.422851\n",
      "pval: 0.000015\n",
      "Step: 351\tloss: 9296.418361\n",
      "Step: 352\tloss: 9296.414657\n",
      "Step: 353\tloss: 9296.412894\n",
      "Step: 354\tloss: 9296.411320\n",
      "Step: 355\tloss: 9296.408525\n",
      "Step: 356\tloss: 9296.405984\n",
      "Step: 357\tloss: 9296.405314\n",
      "Step: 358\tloss: 9296.405199\n",
      "Step: 359\tloss: 9296.403723\n",
      "Step: 360\tloss: 9296.401442\n",
      "pval: 0.000018\n",
      "Step: 361\tloss: 9296.399966\n",
      "Step: 362\tloss: 9296.399481\n",
      "Step: 363\tloss: 9296.398965\n",
      "Step: 364\tloss: 9296.398079\n",
      "Step: 365\tloss: 9296.397289\n",
      "Step: 366\tloss: 9296.396740\n",
      "Step: 367\tloss: 9296.396178\n",
      "Step: 368\tloss: 9296.395530\n",
      "Step: 369\tloss: 9296.394967\n",
      "Step: 370\tloss: 9296.394576\n",
      "pval: 0.000015\n",
      "Step: 371\tloss: 9296.394250\n",
      "Step: 372\tloss: 9296.393927\n",
      "Step: 373\tloss: 9296.393628\n",
      "Step: 374\tloss: 9296.393310\n",
      "Step: 375\tloss: 9296.392987\n",
      "Step: 376\tloss: 9296.392756\n",
      "Step: 377\tloss: 9296.392606\n",
      "Step: 378\tloss: 9296.392420\n",
      "Step: 379\tloss: 9296.392200\n",
      "Step: 380\tloss: 9296.392045\n",
      "pval: 0.000011\n",
      "Step: 381\tloss: 9296.391932\n",
      "Step: 382\tloss: 9296.391796\n",
      "Step: 383\tloss: 9296.391661\n",
      "Step: 384\tloss: 9296.391577\n",
      "Step: 385\tloss: 9296.391489\n",
      "Step: 386\tloss: 9296.391378\n",
      "Step: 387\tloss: 9296.391302\n",
      "Step: 388\tloss: 9296.391254\n",
      "Step: 389\tloss: 9296.391185\n",
      "Step: 390\tloss: 9296.391114\n",
      "pval: 0.000020\n",
      "Step: 391\tloss: 9296.391073\n",
      "Step: 392\tloss: 9296.391035\n",
      "Step: 393\tloss: 9296.390978\n",
      "Step: 394\tloss: 9296.390934\n",
      "Step: 395\tloss: 9296.390915\n",
      "Step: 396\tloss: 9296.390892\n",
      "Step: 397\tloss: 9296.390852\n",
      "Step: 398\tloss: 9296.390820\n",
      "Step: 399\tloss: 9296.390805\n",
      "Step: 400\tloss: 9296.390788\n",
      "pval: 0.000020\n",
      "Step: 401\tloss: 9296.390763\n",
      "Step: 402\tloss: 9296.390746\n",
      "Step: 403\tloss: 9296.390738\n",
      "Step: 404\tloss: 9296.390722\n",
      "Step: 405\tloss: 9296.390703\n",
      "Step: 406\tloss: 9296.390695\n",
      "Step: 407\tloss: 9296.390690\n",
      "Step: 408\tloss: 9296.390680\n",
      "Step: 409\tloss: 9296.390673\n",
      "Step: 410\tloss: 9296.390676\n",
      "pval: 0.000022\n",
      "Step: 411\tloss: 9296.390688\n",
      "Step: 412\tloss: 9296.390728\n",
      "Step: 413\tloss: 9296.390797\n",
      "Step: 414\tloss: 9296.390826\n",
      "Step: 415\tloss: 9296.390713\n",
      "Step: 416\tloss: 9296.390630\n",
      "Step: 417\tloss: 9296.390700\n",
      "Step: 418\tloss: 9296.390725\n",
      "Step: 419\tloss: 9296.390638\n",
      "Step: 420\tloss: 9296.390636\n",
      "pval: 0.492025\n",
      "Training sequence #2 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence(\"AUTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.04\n",
      "Root mean squared deviation of scale:    0.46\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc, sim.par_link_loc)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.05\n",
      "Root mean squared deviation of scale:    1.05\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is evident that the dispersion (the variance mdoel) is badly estimated if the size-factor are not accounted for as they represent unaccounted confoudning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
