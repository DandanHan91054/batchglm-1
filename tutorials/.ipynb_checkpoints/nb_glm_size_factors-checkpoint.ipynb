{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=1000)\n",
    "sim.generate()\n",
    "X = sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superimpose library size effects on counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_factors = numpy.random.normal(loc=1, scale=0.5, size=X.shape[0]) # draw random factors\n",
    "size_factors[size_factors < 0.2] = 0.2 # threshold\n",
    "X = np.round(X*np.repeat(np.expand_dims(size_factors, axis=1), axis=1, repeats=X.shape[1])) # scale counts and round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size factor scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.015730194737318067"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(X, axis=1).values/np.mean(np.sum(X, axis=1)).values - size_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add continuous covariate to desing_loc so that location model cannot be perfectly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_loc = sim.design_loc\n",
    "design_loc = np.hstack([design_loc.values, np.expand_dims(numpy.random.normal(loc=1, scale=1, size=design_loc.shape[0]), axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=size_factors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(\n",
    "    input_data, \n",
    "    init_a='standard', \n",
    "    init_b='standard',\n",
    "    batch_size=500\n",
    ")\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 9254.189149\n",
      "Step: 2\tloss: 9292.989550\n",
      "Step: 3\tloss: 9167.053270\n",
      "Step: 4\tloss: 9052.905853\n",
      "Step: 5\tloss: 8967.553832\n",
      "Step: 6\tloss: 8959.901128\n",
      "Step: 7\tloss: 8874.794233\n",
      "Step: 8\tloss: 8854.083242\n",
      "Step: 9\tloss: 8852.239760\n",
      "Step: 10\tloss: 8777.278185\n",
      "Step: 11\tloss: 8845.530370\n",
      "Step: 12\tloss: 8820.528019\n",
      "Step: 13\tloss: 8818.555588\n",
      "Step: 14\tloss: 8812.567318\n",
      "Step: 15\tloss: 8762.291693\n",
      "Step: 16\tloss: 8797.929816\n",
      "Step: 17\tloss: 8840.081402\n",
      "Step: 18\tloss: 8745.417729\n",
      "Step: 19\tloss: 8792.725467\n",
      "Step: 20\tloss: 8736.065210\n",
      "Step: 21\tloss: 8732.579664\n",
      "Step: 22\tloss: 8785.134898\n",
      "Step: 23\tloss: 8720.633028\n",
      "Step: 24\tloss: 8782.008826\n",
      "Step: 25\tloss: 8752.383262\n",
      "Step: 26\tloss: 8733.759706\n",
      "Step: 27\tloss: 8751.284042\n",
      "Step: 28\tloss: 8746.472003\n",
      "Step: 29\tloss: 8768.293886\n",
      "Step: 30\tloss: 8732.775239\n",
      "Step: 31\tloss: 8738.495989\n",
      "Step: 32\tloss: 8711.262123\n",
      "Step: 33\tloss: 8718.596630\n",
      "Step: 34\tloss: 8750.232652\n",
      "Step: 35\tloss: 8720.690091\n",
      "Step: 36\tloss: 8725.805557\n",
      "Step: 37\tloss: 8697.602008\n",
      "Step: 38\tloss: 8756.369849\n",
      "Step: 39\tloss: 8724.338734\n",
      "Step: 40\tloss: 8711.509786\n",
      "Step: 41\tloss: 8696.868554\n",
      "Step: 42\tloss: 8726.091721\n",
      "Step: 43\tloss: 8707.077530\n",
      "Step: 44\tloss: 8746.975852\n",
      "Step: 45\tloss: 8691.774777\n",
      "Step: 46\tloss: 8702.250122\n",
      "Step: 47\tloss: 8704.278937\n",
      "Step: 48\tloss: 8768.973257\n",
      "Step: 49\tloss: 8705.703990\n",
      "Step: 50\tloss: 8749.307343\n",
      "Step: 51\tloss: 8728.526871\n",
      "Step: 52\tloss: 8673.591843\n",
      "Step: 53\tloss: 8714.567012\n",
      "Step: 54\tloss: 8698.056579\n",
      "Step: 55\tloss: 8691.914615\n",
      "Step: 56\tloss: 8744.965709\n",
      "Step: 57\tloss: 8677.514891\n",
      "Step: 58\tloss: 8696.833084\n",
      "Step: 59\tloss: 8717.773210\n",
      "Step: 60\tloss: 8751.660094\n",
      "Step: 61\tloss: 8694.411296\n",
      "Step: 62\tloss: 8702.855706\n",
      "Step: 63\tloss: 8718.987185\n",
      "Step: 64\tloss: 8723.703392\n",
      "Step: 65\tloss: 8721.458360\n",
      "Step: 66\tloss: 8718.794734\n",
      "Step: 67\tloss: 8707.982234\n",
      "Step: 68\tloss: 8688.733475\n",
      "Step: 69\tloss: 8687.133019\n",
      "Step: 70\tloss: 8708.568051\n",
      "Step: 71\tloss: 8729.111199\n",
      "Step: 72\tloss: 8711.125034\n",
      "Step: 73\tloss: 8721.669259\n",
      "Step: 74\tloss: 8701.567774\n",
      "Step: 75\tloss: 8741.328068\n",
      "Step: 76\tloss: 8671.131235\n",
      "Step: 77\tloss: 8687.251747\n",
      "Step: 78\tloss: 8703.769863\n",
      "Step: 79\tloss: 8687.602253\n",
      "Step: 80\tloss: 8755.856556\n",
      "Step: 81\tloss: 8692.067604\n",
      "Step: 82\tloss: 8726.036195\n",
      "Step: 83\tloss: 8700.350908\n",
      "Step: 84\tloss: 8716.205624\n",
      "Step: 85\tloss: 8720.951390\n",
      "Step: 86\tloss: 8691.135904\n",
      "Step: 87\tloss: 8718.452548\n",
      "Step: 88\tloss: 8704.160581\n",
      "Step: 89\tloss: 8700.648437\n",
      "Step: 90\tloss: 8704.742954\n",
      "Step: 91\tloss: 8709.849688\n",
      "Step: 92\tloss: 8719.971579\n",
      "Step: 93\tloss: 8735.953916\n",
      "Step: 94\tloss: 8661.252025\n",
      "Step: 95\tloss: 8739.875952\n",
      "Step: 96\tloss: 8697.984315\n",
      "Step: 97\tloss: 8719.201365\n",
      "Step: 98\tloss: 8709.380685\n",
      "Step: 99\tloss: 8683.280826\n",
      "Step: 100\tloss: 8723.017198\n",
      "Step: 101\tloss: 8705.406950\n",
      "Step: 102\tloss: 8708.978014\n",
      "Step: 103\tloss: 8728.206108\n",
      "Step: 104\tloss: 8692.376771\n",
      "Step: 105\tloss: 8701.022993\n",
      "Step: 106\tloss: 8708.284090\n",
      "Step: 107\tloss: 8735.563580\n",
      "Step: 108\tloss: 8689.583339\n",
      "Step: 109\tloss: 8673.334454\n",
      "Step: 110\tloss: 8733.352845\n",
      "Step: 111\tloss: 8730.591527\n",
      "Step: 112\tloss: 8697.091024\n",
      "Step: 113\tloss: 8712.429564\n",
      "Step: 114\tloss: 8714.569668\n",
      "Step: 115\tloss: 8695.609474\n",
      "Step: 116\tloss: 8712.788833\n",
      "Step: 117\tloss: 8671.374446\n",
      "Step: 118\tloss: 8727.056995\n",
      "Step: 119\tloss: 8711.105353\n",
      "Step: 120\tloss: 8725.749394\n",
      "Step: 121\tloss: 8673.217268\n",
      "Step: 122\tloss: 8720.680536\n",
      "Step: 123\tloss: 8739.505368\n",
      "Step: 124\tloss: 8703.546142\n",
      "Step: 125\tloss: 8724.532253\n",
      "Step: 126\tloss: 8721.425258\n",
      "Step: 127\tloss: 8705.744094\n",
      "Step: 128\tloss: 8685.305656\n",
      "Step: 129\tloss: 8714.309687\n",
      "Step: 130\tloss: 8681.213330\n",
      "Step: 131\tloss: 8751.012737\n",
      "Step: 132\tloss: 8691.188515\n",
      "Step: 133\tloss: 8711.915851\n",
      "Step: 134\tloss: 8703.695404\n",
      "Step: 135\tloss: 8708.356603\n",
      "Step: 136\tloss: 8713.536034\n",
      "Step: 137\tloss: 8685.083519\n",
      "Step: 138\tloss: 8750.533292\n",
      "Step: 139\tloss: 8683.365248\n",
      "Step: 140\tloss: 8720.913187\n",
      "Step: 141\tloss: 8700.044337\n",
      "Step: 142\tloss: 8700.313525\n",
      "Step: 143\tloss: 8739.467785\n",
      "Step: 144\tloss: 8699.843293\n",
      "Step: 145\tloss: 8696.226426\n",
      "Step: 146\tloss: 8718.272822\n",
      "Step: 147\tloss: 8726.053751\n",
      "Step: 148\tloss: 8699.548153\n",
      "Step: 149\tloss: 8696.187996\n",
      "Step: 150\tloss: 8686.717598\n",
      "Step: 151\tloss: 8721.465723\n",
      "Step: 152\tloss: 8735.173078\n",
      "Step: 153\tloss: 8674.054611\n",
      "Step: 154\tloss: 8686.321949\n",
      "Step: 155\tloss: 8755.703843\n",
      "Step: 156\tloss: 8723.937795\n",
      "Step: 157\tloss: 8727.913080\n",
      "Step: 158\tloss: 8670.493131\n",
      "Step: 159\tloss: 8692.857722\n",
      "Step: 160\tloss: 8748.256494\n",
      "Step: 161\tloss: 8729.490542\n",
      "Step: 162\tloss: 8703.285175\n",
      "Step: 163\tloss: 8705.483628\n",
      "Step: 164\tloss: 8701.490747\n",
      "Step: 165\tloss: 8701.034683\n",
      "Step: 166\tloss: 8711.469646\n",
      "Step: 167\tloss: 8711.543862\n",
      "Step: 168\tloss: 8716.210028\n",
      "Step: 169\tloss: 8727.232491\n",
      "Step: 170\tloss: 8715.929582\n",
      "Step: 171\tloss: 8690.930875\n",
      "Step: 172\tloss: 8706.176274\n",
      "Step: 173\tloss: 8718.578066\n",
      "Step: 174\tloss: 8685.527212\n",
      "Step: 175\tloss: 8710.335207\n",
      "Step: 176\tloss: 8724.763299\n",
      "Step: 177\tloss: 8736.050609\n",
      "Step: 178\tloss: 8717.206535\n",
      "Step: 179\tloss: 8692.046372\n",
      "Step: 180\tloss: 8694.133108\n",
      "Step: 181\tloss: 8708.821952\n",
      "Step: 182\tloss: 8694.667809\n",
      "Step: 183\tloss: 8728.983431\n",
      "Step: 184\tloss: 8707.301618\n",
      "Step: 185\tloss: 8714.721886\n",
      "Step: 186\tloss: 8709.911586\n",
      "Step: 187\tloss: 8687.364088\n",
      "Step: 188\tloss: 8727.870449\n",
      "Step: 189\tloss: 8686.778613\n",
      "Step: 190\tloss: 8704.707104\n",
      "Step: 191\tloss: 8734.603928\n",
      "Step: 192\tloss: 8713.774840\n",
      "Step: 193\tloss: 8700.875916\n",
      "Step: 194\tloss: 8726.656728\n",
      "Step: 195\tloss: 8696.141088\n",
      "Step: 196\tloss: 8717.558464\n",
      "Step: 197\tloss: 8679.738512\n",
      "Step: 198\tloss: 8735.985949\n",
      "Step: 199\tloss: 8685.484825\n",
      "Step: 200\tloss: 8740.252733\n",
      "pval: 0.000031\n",
      "Step: 201\tloss: 8685.933583\n",
      "Step: 202\tloss: 8729.535126\n",
      "Step: 203\tloss: 8716.660333\n",
      "Step: 204\tloss: 8709.160144\n",
      "Step: 205\tloss: 8721.967901\n",
      "Step: 206\tloss: 8707.034561\n",
      "Step: 207\tloss: 8688.717652\n",
      "Step: 208\tloss: 8725.210725\n",
      "Step: 209\tloss: 8649.036828\n",
      "Step: 210\tloss: 8725.554716\n",
      "Step: 211\tloss: 8740.981480\n",
      "Step: 212\tloss: 8727.970259\n",
      "Step: 213\tloss: 8716.467428\n",
      "Step: 214\tloss: 8668.447726\n",
      "Step: 215\tloss: 8708.943614\n",
      "Step: 216\tloss: 8749.165414\n",
      "Step: 217\tloss: 8714.879846\n",
      "Step: 218\tloss: 8688.998716\n",
      "Step: 219\tloss: 8743.809467\n",
      "Step: 220\tloss: 8697.318499\n",
      "Step: 221\tloss: 8689.751437\n",
      "Step: 222\tloss: 8707.829022\n",
      "Step: 223\tloss: 8720.223119\n",
      "Step: 224\tloss: 8726.162551\n",
      "Step: 225\tloss: 8707.945048\n",
      "Step: 226\tloss: 8689.661112\n",
      "Step: 227\tloss: 8733.562289\n",
      "Step: 228\tloss: 8712.222466\n",
      "Step: 229\tloss: 8718.822459\n",
      "Step: 230\tloss: 8738.846886\n",
      "Step: 231\tloss: 8708.654425\n",
      "Step: 232\tloss: 8677.277692\n",
      "Step: 233\tloss: 8681.372569\n",
      "Step: 234\tloss: 8718.712305\n",
      "Step: 235\tloss: 8720.429562\n",
      "Step: 236\tloss: 8722.866405\n",
      "Step: 237\tloss: 8714.007758\n",
      "Step: 238\tloss: 8772.570201\n",
      "Step: 239\tloss: 8685.844356\n",
      "Step: 240\tloss: 8671.801557\n",
      "Step: 241\tloss: 8694.357841\n",
      "Step: 242\tloss: 8713.257852\n",
      "Step: 243\tloss: 8744.839883\n",
      "Step: 244\tloss: 8692.150932\n",
      "Step: 245\tloss: 8688.224909\n",
      "Step: 246\tloss: 8719.156757\n",
      "Step: 247\tloss: 8708.698208\n",
      "Step: 248\tloss: 8729.164849\n",
      "Step: 249\tloss: 8705.081043\n",
      "Step: 250\tloss: 8746.446832\n",
      "Step: 251\tloss: 8664.563914\n",
      "Step: 252\tloss: 8727.831423\n",
      "Step: 253\tloss: 8708.360948\n",
      "Step: 254\tloss: 8733.558449\n",
      "Step: 255\tloss: 8705.277011\n",
      "Step: 256\tloss: 8696.861381\n",
      "Step: 257\tloss: 8713.702984\n",
      "Step: 258\tloss: 8691.846730\n",
      "Step: 259\tloss: 8691.367778\n",
      "Step: 260\tloss: 8748.214927\n",
      "Step: 261\tloss: 8704.343788\n",
      "Step: 262\tloss: 8685.577736\n",
      "Step: 263\tloss: 8734.208661\n",
      "Step: 264\tloss: 8719.761126\n",
      "Step: 265\tloss: 8689.119063\n",
      "Step: 266\tloss: 8721.567861\n",
      "Step: 267\tloss: 8711.068214\n",
      "Step: 268\tloss: 8722.853600\n",
      "Step: 269\tloss: 8695.473272\n",
      "Step: 270\tloss: 8699.854429\n",
      "Step: 271\tloss: 8722.321709\n",
      "Step: 272\tloss: 8725.730694\n",
      "Step: 273\tloss: 8693.225512\n",
      "Step: 274\tloss: 8715.977314\n",
      "Step: 275\tloss: 8718.706763\n",
      "Step: 276\tloss: 8715.287645\n",
      "Step: 277\tloss: 8708.799073\n",
      "Step: 278\tloss: 8701.820775\n",
      "Step: 279\tloss: 8693.039456\n",
      "Step: 280\tloss: 8739.055475\n",
      "Step: 281\tloss: 8714.024147\n",
      "Step: 282\tloss: 8700.885207\n",
      "Step: 283\tloss: 8694.395941\n",
      "Step: 284\tloss: 8733.433495\n",
      "Step: 285\tloss: 8677.796281\n",
      "Step: 286\tloss: 8715.830957\n",
      "Step: 287\tloss: 8722.393929\n",
      "Step: 288\tloss: 8727.750449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 289\tloss: 8712.210471\n",
      "Step: 290\tloss: 8724.768196\n",
      "Step: 291\tloss: 8699.936695\n",
      "Step: 292\tloss: 8708.175842\n",
      "Step: 293\tloss: 8709.959639\n",
      "Step: 294\tloss: 8709.060281\n",
      "Step: 295\tloss: 8677.341841\n",
      "Step: 296\tloss: 8748.233477\n",
      "Step: 297\tloss: 8702.330357\n",
      "Step: 298\tloss: 8711.757124\n",
      "Step: 299\tloss: 8697.965955\n",
      "Step: 300\tloss: 8732.572436\n",
      "pval: 0.680317\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc[:-1,:].values, sim.par_link_loc.values)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.05\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=sim.design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(\n",
    "    input_data, \n",
    "    init_a='standard', \n",
    "    init_b='standard',\n",
    "    batch_size=500\n",
    ")\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 9458.421593\n",
      "Step: 2\tloss: 9398.465752\n",
      "Step: 3\tloss: 9344.372833\n",
      "Step: 4\tloss: 9314.525149\n",
      "Step: 5\tloss: 9268.994495\n",
      "Step: 6\tloss: 9270.309770\n",
      "Step: 7\tloss: 9306.905255\n",
      "Step: 8\tloss: 9281.843394\n",
      "Step: 9\tloss: 9244.021592\n",
      "Step: 10\tloss: 9282.948936\n",
      "Step: 11\tloss: 9290.082151\n",
      "Step: 12\tloss: 9254.104585\n",
      "Step: 13\tloss: 9279.192735\n",
      "Step: 14\tloss: 9253.488935\n",
      "Step: 15\tloss: 9273.220736\n",
      "Step: 16\tloss: 9215.139687\n",
      "Step: 17\tloss: 9272.192973\n",
      "Step: 18\tloss: 9241.510418\n",
      "Step: 19\tloss: 9243.610566\n",
      "Step: 20\tloss: 9244.566488\n",
      "Step: 21\tloss: 9230.141529\n",
      "Step: 22\tloss: 9228.657503\n",
      "Step: 23\tloss: 9211.238731\n",
      "Step: 24\tloss: 9306.095295\n",
      "Step: 25\tloss: 9207.797747\n",
      "Step: 26\tloss: 9235.119875\n",
      "Step: 27\tloss: 9258.727445\n",
      "Step: 28\tloss: 9263.214895\n",
      "Step: 29\tloss: 9237.738408\n",
      "Step: 30\tloss: 9226.959142\n",
      "Step: 31\tloss: 9241.612661\n",
      "Step: 32\tloss: 9250.036840\n",
      "Step: 33\tloss: 9211.303033\n",
      "Step: 34\tloss: 9247.577982\n",
      "Step: 35\tloss: 9266.457253\n",
      "Step: 36\tloss: 9229.035313\n",
      "Step: 37\tloss: 9228.480019\n",
      "Step: 38\tloss: 9219.640859\n",
      "Step: 39\tloss: 9257.577275\n",
      "Step: 40\tloss: 9244.532450\n",
      "Step: 41\tloss: 9216.995822\n",
      "Step: 42\tloss: 9245.863875\n",
      "Step: 43\tloss: 9246.911285\n",
      "Step: 44\tloss: 9236.671958\n",
      "Step: 45\tloss: 9232.575500\n",
      "Step: 46\tloss: 9237.511689\n",
      "Step: 47\tloss: 9242.766641\n",
      "Step: 48\tloss: 9232.451911\n",
      "Step: 49\tloss: 9212.061022\n",
      "Step: 50\tloss: 9225.997872\n",
      "Step: 51\tloss: 9238.941746\n",
      "Step: 52\tloss: 9271.157195\n",
      "Step: 53\tloss: 9213.254725\n",
      "Step: 54\tloss: 9250.881590\n",
      "Step: 55\tloss: 9243.654577\n",
      "Step: 56\tloss: 9237.779202\n",
      "Step: 57\tloss: 9219.735235\n",
      "Step: 58\tloss: 9245.068720\n",
      "Step: 59\tloss: 9219.545984\n",
      "Step: 60\tloss: 9265.223295\n",
      "Step: 61\tloss: 9227.535875\n",
      "Step: 62\tloss: 9202.700327\n",
      "Step: 63\tloss: 9225.253665\n",
      "Step: 64\tloss: 9288.775803\n",
      "Step: 65\tloss: 9234.489947\n",
      "Step: 66\tloss: 9226.118749\n",
      "Step: 67\tloss: 9232.453292\n",
      "Step: 68\tloss: 9253.621157\n",
      "Step: 69\tloss: 9222.135939\n",
      "Step: 70\tloss: 9255.025276\n",
      "Step: 71\tloss: 9233.867575\n",
      "Step: 72\tloss: 9232.062757\n",
      "Step: 73\tloss: 9230.195139\n",
      "Step: 74\tloss: 9209.210449\n",
      "Step: 75\tloss: 9261.469201\n",
      "Step: 76\tloss: 9242.533688\n",
      "Step: 77\tloss: 9254.801070\n",
      "Step: 78\tloss: 9202.949523\n",
      "Step: 79\tloss: 9244.339931\n",
      "Step: 80\tloss: 9241.878413\n",
      "Step: 81\tloss: 9245.577174\n",
      "Step: 82\tloss: 9244.526596\n",
      "Step: 83\tloss: 9223.025008\n",
      "Step: 84\tloss: 9231.361997\n",
      "Step: 85\tloss: 9221.325078\n",
      "Step: 86\tloss: 9225.017740\n",
      "Step: 87\tloss: 9216.223652\n",
      "Step: 88\tloss: 9282.176324\n",
      "Step: 89\tloss: 9200.984551\n",
      "Step: 90\tloss: 9248.595128\n",
      "Step: 91\tloss: 9228.017332\n",
      "Step: 92\tloss: 9267.711858\n",
      "Step: 93\tloss: 9215.218602\n",
      "Step: 94\tloss: 9182.717676\n",
      "Step: 95\tloss: 9294.644341\n",
      "Step: 96\tloss: 9251.924728\n",
      "Step: 97\tloss: 9215.629728\n",
      "Step: 98\tloss: 9247.136633\n",
      "Step: 99\tloss: 9246.397873\n",
      "Step: 100\tloss: 9234.999467\n",
      "Step: 101\tloss: 9241.500416\n",
      "Step: 102\tloss: 9221.019670\n",
      "Step: 103\tloss: 9242.820355\n",
      "Step: 104\tloss: 9237.539251\n",
      "Step: 105\tloss: 9203.770309\n",
      "Step: 106\tloss: 9270.648044\n",
      "Step: 107\tloss: 9243.590504\n",
      "Step: 108\tloss: 9226.181912\n",
      "Step: 109\tloss: 9231.227146\n",
      "Step: 110\tloss: 9234.322542\n",
      "Step: 111\tloss: 9231.834615\n",
      "Step: 112\tloss: 9244.638843\n",
      "Step: 113\tloss: 9201.630790\n",
      "Step: 114\tloss: 9234.521096\n",
      "Step: 115\tloss: 9247.344981\n",
      "Step: 116\tloss: 9263.653334\n",
      "Step: 117\tloss: 9218.478636\n",
      "Step: 118\tloss: 9229.194323\n",
      "Step: 119\tloss: 9261.616206\n",
      "Step: 120\tloss: 9233.880701\n",
      "Step: 121\tloss: 9239.004680\n",
      "Step: 122\tloss: 9244.991791\n",
      "Step: 123\tloss: 9243.632571\n",
      "Step: 124\tloss: 9216.330172\n",
      "Step: 125\tloss: 9225.066909\n",
      "Step: 126\tloss: 9242.033625\n",
      "Step: 127\tloss: 9258.504014\n",
      "Step: 128\tloss: 9217.298769\n",
      "Step: 129\tloss: 9231.457555\n",
      "Step: 130\tloss: 9227.957377\n",
      "Step: 131\tloss: 9251.791884\n",
      "Step: 132\tloss: 9232.744836\n",
      "Step: 133\tloss: 9224.129540\n",
      "Step: 134\tloss: 9242.486810\n",
      "Step: 135\tloss: 9254.117076\n",
      "Step: 136\tloss: 9223.022143\n",
      "Step: 137\tloss: 9220.400841\n",
      "Step: 138\tloss: 9242.609661\n",
      "Step: 139\tloss: 9240.577894\n",
      "Step: 140\tloss: 9240.941211\n",
      "Step: 141\tloss: 9197.530298\n",
      "Step: 142\tloss: 9246.569425\n",
      "Step: 143\tloss: 9239.551379\n",
      "Step: 144\tloss: 9261.310430\n",
      "Step: 145\tloss: 9223.193983\n",
      "Step: 146\tloss: 9235.502864\n",
      "Step: 147\tloss: 9256.043401\n",
      "Step: 148\tloss: 9230.311255\n",
      "Step: 149\tloss: 9261.433570\n",
      "Step: 150\tloss: 9219.412481\n",
      "Step: 151\tloss: 9223.583681\n",
      "Step: 152\tloss: 9239.018993\n",
      "Step: 153\tloss: 9238.841845\n",
      "Step: 154\tloss: 9228.187049\n",
      "Step: 155\tloss: 9260.534702\n",
      "Step: 156\tloss: 9219.464686\n",
      "Step: 157\tloss: 9223.138080\n",
      "Step: 158\tloss: 9243.474596\n",
      "Step: 159\tloss: 9231.431893\n",
      "Step: 160\tloss: 9245.557959\n",
      "Step: 161\tloss: 9212.320804\n",
      "Step: 162\tloss: 9230.754855\n",
      "Step: 163\tloss: 9224.715013\n",
      "Step: 164\tloss: 9277.069181\n",
      "Step: 165\tloss: 9211.591804\n",
      "Step: 166\tloss: 9245.174150\n",
      "Step: 167\tloss: 9222.885267\n",
      "Step: 168\tloss: 9268.848848\n",
      "Step: 169\tloss: 9194.831135\n",
      "Step: 170\tloss: 9243.157083\n",
      "Step: 171\tloss: 9259.509766\n",
      "Step: 172\tloss: 9246.572474\n",
      "Step: 173\tloss: 9229.709837\n",
      "Step: 174\tloss: 9233.277939\n",
      "Step: 175\tloss: 9227.297967\n",
      "Step: 176\tloss: 9255.297126\n",
      "Step: 177\tloss: 9258.440020\n",
      "Step: 178\tloss: 9225.116592\n",
      "Step: 179\tloss: 9257.293619\n",
      "Step: 180\tloss: 9206.400621\n",
      "Step: 181\tloss: 9206.736217\n",
      "Step: 182\tloss: 9257.285083\n",
      "Step: 183\tloss: 9206.035052\n",
      "Step: 184\tloss: 9275.649156\n",
      "Step: 185\tloss: 9205.672476\n",
      "Step: 186\tloss: 9236.916341\n",
      "Step: 187\tloss: 9258.989990\n",
      "Step: 188\tloss: 9242.909575\n",
      "Step: 189\tloss: 9215.775259\n",
      "Step: 190\tloss: 9218.124399\n",
      "Step: 191\tloss: 9255.413415\n",
      "Step: 192\tloss: 9255.457698\n",
      "Step: 193\tloss: 9219.719683\n",
      "Step: 194\tloss: 9217.359819\n",
      "Step: 195\tloss: 9249.740100\n",
      "Step: 196\tloss: 9257.087342\n",
      "Step: 197\tloss: 9200.628935\n",
      "Step: 198\tloss: 9249.434531\n",
      "Step: 199\tloss: 9250.779323\n",
      "Step: 200\tloss: 9242.703690\n",
      "pval: 0.004252\n",
      "Step: 201\tloss: 9228.223208\n",
      "Step: 202\tloss: 9231.197344\n",
      "Step: 203\tloss: 9231.129537\n",
      "Step: 204\tloss: 9257.837538\n",
      "Step: 205\tloss: 9225.510828\n",
      "Step: 206\tloss: 9229.222723\n",
      "Step: 207\tloss: 9245.231027\n",
      "Step: 208\tloss: 9246.323818\n",
      "Step: 209\tloss: 9206.525948\n",
      "Step: 210\tloss: 9234.846155\n",
      "Step: 211\tloss: 9261.819350\n",
      "Step: 212\tloss: 9248.242385\n",
      "Step: 213\tloss: 9204.218791\n",
      "Step: 214\tloss: 9232.968533\n",
      "Step: 215\tloss: 9270.215548\n",
      "Step: 216\tloss: 9241.368299\n",
      "Step: 217\tloss: 9218.962980\n",
      "Step: 218\tloss: 9252.258795\n",
      "Step: 219\tloss: 9266.411995\n",
      "Step: 220\tloss: 9209.218365\n",
      "Step: 221\tloss: 9228.145437\n",
      "Step: 222\tloss: 9245.360088\n",
      "Step: 223\tloss: 9236.593157\n",
      "Step: 224\tloss: 9235.991615\n",
      "Step: 225\tloss: 9223.806877\n",
      "Step: 226\tloss: 9209.352349\n",
      "Step: 227\tloss: 9249.528056\n",
      "Step: 228\tloss: 9264.660078\n",
      "Step: 229\tloss: 9229.819324\n",
      "Step: 230\tloss: 9239.581231\n",
      "Step: 231\tloss: 9262.281022\n",
      "Step: 232\tloss: 9217.744125\n",
      "Step: 233\tloss: 9229.090900\n",
      "Step: 234\tloss: 9217.145829\n",
      "Step: 235\tloss: 9252.064004\n",
      "Step: 236\tloss: 9248.904439\n",
      "Step: 237\tloss: 9214.484194\n",
      "Step: 238\tloss: 9258.085694\n",
      "Step: 239\tloss: 9230.898365\n",
      "Step: 240\tloss: 9244.973037\n",
      "Step: 241\tloss: 9207.646848\n",
      "Step: 242\tloss: 9239.737108\n",
      "Step: 243\tloss: 9254.778714\n",
      "Step: 244\tloss: 9242.729557\n",
      "Step: 245\tloss: 9221.666682\n",
      "Step: 246\tloss: 9246.088022\n",
      "Step: 247\tloss: 9249.445332\n",
      "Step: 248\tloss: 9227.742513\n",
      "Step: 249\tloss: 9216.967627\n",
      "Step: 250\tloss: 9234.140318\n",
      "Step: 251\tloss: 9227.133549\n",
      "Step: 252\tloss: 9267.372806\n",
      "Step: 253\tloss: 9213.484142\n",
      "Step: 254\tloss: 9224.508218\n",
      "Step: 255\tloss: 9263.923173\n",
      "Step: 256\tloss: 9246.197205\n",
      "Step: 257\tloss: 9205.913952\n",
      "Step: 258\tloss: 9229.523541\n",
      "Step: 259\tloss: 9244.070259\n",
      "Step: 260\tloss: 9270.938506\n",
      "Step: 261\tloss: 9223.393410\n",
      "Step: 262\tloss: 9216.872964\n",
      "Step: 263\tloss: 9244.984165\n",
      "Step: 264\tloss: 9264.386374\n",
      "Step: 265\tloss: 9223.027456\n",
      "Step: 266\tloss: 9227.564648\n",
      "Step: 267\tloss: 9231.848534\n",
      "Step: 268\tloss: 9266.910902\n",
      "Step: 269\tloss: 9206.916649\n",
      "Step: 270\tloss: 9230.365915\n",
      "Step: 271\tloss: 9247.224225\n",
      "Step: 272\tloss: 9259.156400\n",
      "Step: 273\tloss: 9229.790390\n",
      "Step: 274\tloss: 9206.983409\n",
      "Step: 275\tloss: 9249.500725\n",
      "Step: 276\tloss: 9262.183157\n",
      "Step: 277\tloss: 9184.242779\n",
      "Step: 278\tloss: 9225.617403\n",
      "Step: 279\tloss: 9272.731252\n",
      "Step: 280\tloss: 9263.536696\n",
      "Step: 281\tloss: 9205.474038\n",
      "Step: 282\tloss: 9249.726591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 283\tloss: 9265.220195\n",
      "Step: 284\tloss: 9226.671381\n",
      "Step: 285\tloss: 9236.661232\n",
      "Step: 286\tloss: 9225.021000\n",
      "Step: 287\tloss: 9245.541541\n",
      "Step: 288\tloss: 9239.235795\n",
      "Step: 289\tloss: 9229.505751\n",
      "Step: 290\tloss: 9237.045226\n",
      "Step: 291\tloss: 9233.214329\n",
      "Step: 292\tloss: 9248.371921\n",
      "Step: 293\tloss: 9254.696389\n",
      "Step: 294\tloss: 9231.841992\n",
      "Step: 295\tloss: 9269.496156\n",
      "Step: 296\tloss: 9198.846293\n",
      "Step: 297\tloss: 9233.606520\n",
      "Step: 298\tloss: 9251.327885\n",
      "Step: 299\tloss: 9223.690749\n",
      "Step: 300\tloss: 9238.603239\n",
      "pval: 0.620044\n",
      "Training sequence #1 complete\n",
      "Beginning with training sequence #2\n",
      "Step: 301\tloss: 9235.765752\n",
      "Step: 302\tloss: 9243.441434\n",
      "Step: 303\tloss: 9236.422914\n",
      "Step: 304\tloss: 9236.156504\n",
      "Step: 305\tloss: 9238.364877\n",
      "Step: 306\tloss: 9236.652104\n",
      "Step: 307\tloss: 9234.901043\n",
      "Step: 308\tloss: 9235.401243\n",
      "Step: 309\tloss: 9236.423933\n",
      "Step: 310\tloss: 9236.156357\n",
      "Step: 311\tloss: 9235.055447\n",
      "Step: 312\tloss: 9234.500501\n",
      "Step: 313\tloss: 9234.888904\n",
      "Step: 314\tloss: 9235.356004\n",
      "Step: 315\tloss: 9235.177452\n",
      "Step: 316\tloss: 9234.672312\n",
      "Step: 317\tloss: 9234.458102\n",
      "Step: 318\tloss: 9234.626682\n",
      "Step: 319\tloss: 9234.789496\n",
      "Step: 320\tloss: 9234.671616\n",
      "pval: 0.008513\n",
      "Step: 321\tloss: 9234.419106\n",
      "Step: 322\tloss: 9234.331342\n",
      "Step: 323\tloss: 9234.447403\n",
      "Step: 324\tloss: 9234.525959\n",
      "Step: 325\tloss: 9234.425959\n",
      "Step: 326\tloss: 9234.284051\n",
      "Step: 327\tloss: 9234.263393\n",
      "Step: 328\tloss: 9234.325138\n",
      "Step: 329\tloss: 9234.333069\n",
      "Step: 330\tloss: 9234.263868\n",
      "pval: 0.000250\n",
      "Step: 331\tloss: 9234.211639\n",
      "Step: 332\tloss: 9234.226266\n",
      "Step: 333\tloss: 9234.247413\n",
      "Step: 334\tloss: 9234.219922\n",
      "Step: 335\tloss: 9234.179447\n",
      "Step: 336\tloss: 9234.176199\n",
      "Step: 337\tloss: 9234.192294\n",
      "Step: 338\tloss: 9234.184848\n",
      "Step: 339\tloss: 9234.158297\n",
      "Step: 340\tloss: 9234.147340\n",
      "pval: 0.000043\n",
      "Step: 341\tloss: 9234.155229\n",
      "Step: 342\tloss: 9234.154569\n",
      "Step: 343\tloss: 9234.139264\n",
      "Step: 344\tloss: 9234.130677\n",
      "Step: 345\tloss: 9234.135052\n",
      "Step: 346\tloss: 9234.135292\n",
      "Step: 347\tloss: 9234.125210\n",
      "Step: 348\tloss: 9234.118547\n",
      "Step: 349\tloss: 9234.121084\n",
      "Step: 350\tloss: 9234.121930\n",
      "pval: 0.000035\n",
      "Step: 351\tloss: 9234.115752\n",
      "Step: 352\tloss: 9234.111030\n",
      "Step: 353\tloss: 9234.112151\n",
      "Step: 354\tloss: 9234.112465\n",
      "Step: 355\tloss: 9234.108655\n",
      "Step: 356\tloss: 9234.105880\n",
      "Step: 357\tloss: 9234.106666\n",
      "Step: 358\tloss: 9234.106620\n",
      "Step: 359\tloss: 9234.103968\n",
      "Step: 360\tloss: 9234.102333\n",
      "pval: 0.000042\n",
      "Step: 361\tloss: 9234.102812\n",
      "Step: 362\tloss: 9234.102458\n",
      "Step: 363\tloss: 9234.100782\n",
      "Step: 364\tloss: 9234.100104\n",
      "Step: 365\tloss: 9234.100316\n",
      "Step: 366\tloss: 9234.099654\n",
      "Step: 367\tloss: 9234.098634\n",
      "Step: 368\tloss: 9234.098519\n",
      "Step: 369\tloss: 9234.098539\n",
      "Step: 370\tloss: 9234.097891\n",
      "pval: 0.000022\n",
      "Step: 371\tloss: 9234.097384\n",
      "Step: 372\tloss: 9234.097408\n",
      "Step: 373\tloss: 9234.097209\n",
      "Step: 374\tloss: 9234.096727\n",
      "Step: 375\tloss: 9234.096572\n",
      "Step: 376\tloss: 9234.096601\n",
      "Step: 377\tloss: 9234.096353\n",
      "Step: 378\tloss: 9234.096064\n",
      "Step: 379\tloss: 9234.096021\n",
      "Step: 380\tloss: 9234.095966\n",
      "pval: 0.000030\n",
      "Step: 381\tloss: 9234.095775\n",
      "Step: 382\tloss: 9234.095667\n",
      "Step: 383\tloss: 9234.095638\n",
      "Step: 384\tloss: 9234.095538\n",
      "Step: 385\tloss: 9234.095428\n",
      "Step: 386\tloss: 9234.095387\n",
      "Step: 387\tloss: 9234.095337\n",
      "Step: 388\tloss: 9234.095265\n",
      "Step: 389\tloss: 9234.095228\n",
      "Step: 390\tloss: 9234.095191\n",
      "pval: 0.000014\n",
      "Step: 391\tloss: 9234.095129\n",
      "Step: 392\tloss: 9234.095098\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence(\"QUICK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc, sim.par_link_loc)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the dispersion (the variance mdoel) is badly estimated if the size-factor are not accounted for as they represent unaccounted confoudning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
