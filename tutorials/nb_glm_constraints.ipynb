{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import xarray as xa\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect confounding occurs frequently in differential expression assays, often if biological replicates cannot be spread acrodd conditions: This is often the case with animals or patients. Perfect confoudnding implies that the corresponding design matrix is not full rank and the model underdetermined. This can be circumvented by certain tricks which essentially regress repplicates to reference replicates. We believe that this is firstly undesirable as the condition coefficients depend on the identity of the reference replicates and accordingly on the ordering of the replicates, which has no experiental meaning and is purely a result of sample labels. Secondly, such tricks may be hard to come up with in hard cases such as presented in example 2. Here, we show how one can solve both problems by constraining parameterse in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have 4 biological replicates (animals, patients, cell culture replicates etc.) in a treatment experiment: 2 in each condition (treated, untreated). Accordingly, there is perfect confounding at this level. We circumvent this by constraining the biological replicate coefficients to not model mean trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 2000\n",
    "dmat = np.zeros([ncells, 6])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1\n",
    "dmat[500:1000,2] = 1 # bio rep 2\n",
    "dmat[1000:1500,3] = 1 # bio rep 3\n",
    "dmat[1500:2000,4] = 1 # bio rep 4\n",
    "dmat[1000:2000,5] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[ 4541,  9711,  4887, ..., 16581,  9323,   344],\n",
       "       [ 5039,  4544,   550, ...,  4949, 10234,   337],\n",
       "       [ 5788, 12647,   748, ...,  4174, 10603,   301],\n",
       "       ...,\n",
       "       [ 5427, 23449,  1359, ..., 13546,  4714,   178],\n",
       "       [ 4767, 10963,  2410, ...,  7389,  7637,   197],\n",
       "       [ 4062,  8821,  4397, ..., 11444,  7165,   311]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters used to generate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'a' (design_loc_params: 6, features: 100)>\n",
       "array([[ 8.742622,  9.019872,  7.773027, ...,  9.056196,  9.08451 ,  5.675544],\n",
       "       [-0.214252,  0.049093, -0.598358, ...,  0.565863,  0.349302,  0.185456],\n",
       "       [-0.012214,  0.122214, -0.462221, ...,  0.493109, -0.287578,  0.411753],\n",
       "       [ 0.226343,  0.381476, -0.035095, ..., -0.680083, -0.082377,  0.580288],\n",
       "       [ 0.613135,  0.475471,  0.294625, ...,  0.401823,  0.132764,  0.61665 ],\n",
       "       [-0.540422, -0.313562,  0.167923, ...,  0.683239, -0.328986, -0.49114 ]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'b' (design_scale_params: 6, features: 100)>\n",
       "array([[ 1.098612,  0.693147,  0.693147, ...,  0.693147,  1.94591 ,  2.197225],\n",
       "       [ 0.293187, -0.118444, -0.404836, ...,  0.119574,  0.295343,  0.087177],\n",
       "       [ 0.228909,  0.056135,  0.038628, ..., -0.252425,  0.490918,  0.682672],\n",
       "       [ 0.369396,  0.624545,  0.204821, ...,  0.434492,  0.64766 ,  0.572951],\n",
       "       [ 0.047528,  0.033225,  0.011649, ...,  0.265999, -0.257833,  0.26869 ],\n",
       "       [ 0.583855,  0.521627, -0.225258, ...,  0.395389, -0.030503, -0.692789]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build constraints based on sets of parameters that have to sum to zero. Each of these constraints is enforced by binding one of these parameters to the rest of the set. Such a constraint is encoded by assigning a 1 to each parameter in the set and a -1 to to the dependent parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([2, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset.\n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 2\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale)\n",
    "input_data.constraints_loc = constraints_loc\n",
    "input_data.constraints_scale = constraints_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: False\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 885.863276\n",
      "Step: 2\tloss: 886.500053\n",
      "Step: 3\tloss: 887.294536\n",
      "Step: 4\tloss: 886.676804\n",
      "Step: 5\tloss: 886.248121\n",
      "Step: 6\tloss: 886.380830\n",
      "Step: 7\tloss: 886.337496\n",
      "Step: 8\tloss: 887.146266\n",
      "Step: 9\tloss: 885.562496\n",
      "Step: 10\tloss: 886.097614\n",
      "Step: 11\tloss: 887.182414\n",
      "Step: 12\tloss: 887.011029\n",
      "Step: 13\tloss: 885.804114\n",
      "Step: 14\tloss: 886.415423\n",
      "Step: 15\tloss: 886.570063\n",
      "Step: 16\tloss: 887.015245\n",
      "Step: 17\tloss: 885.765268\n",
      "Step: 18\tloss: 886.203099\n",
      "Step: 19\tloss: 886.577780\n",
      "Step: 20\tloss: 887.203632\n",
      "Step: 21\tloss: 885.881685\n",
      "Step: 22\tloss: 885.678160\n",
      "Step: 23\tloss: 887.355229\n",
      "Step: 24\tloss: 886.782114\n",
      "Step: 25\tloss: 886.200820\n",
      "Step: 26\tloss: 886.626420\n",
      "Step: 27\tloss: 886.667075\n",
      "Step: 28\tloss: 886.183109\n",
      "Step: 29\tloss: 886.188782\n",
      "Step: 30\tloss: 886.067182\n",
      "Step: 31\tloss: 886.657337\n",
      "Step: 32\tloss: 886.758413\n",
      "Step: 33\tloss: 886.110047\n",
      "Step: 34\tloss: 885.734391\n",
      "Step: 35\tloss: 886.766700\n",
      "Step: 36\tloss: 887.035788\n",
      "Step: 37\tloss: 886.252353\n",
      "Step: 38\tloss: 886.267089\n",
      "Step: 39\tloss: 886.749478\n",
      "Step: 40\tloss: 886.374979\n",
      "Step: 41\tloss: 885.743100\n",
      "Step: 42\tloss: 886.028185\n",
      "Step: 43\tloss: 886.923433\n",
      "Step: 44\tloss: 886.939933\n",
      "Step: 45\tloss: 885.805646\n",
      "Step: 46\tloss: 885.893434\n",
      "Step: 47\tloss: 886.791718\n",
      "Step: 48\tloss: 887.146856\n",
      "Step: 49\tloss: 885.778886\n",
      "Step: 50\tloss: 886.545810\n",
      "Step: 51\tloss: 886.789378\n",
      "Step: 52\tloss: 886.504028\n",
      "Step: 53\tloss: 886.383077\n",
      "Step: 54\tloss: 886.041743\n",
      "Step: 55\tloss: 886.762173\n",
      "Step: 56\tloss: 886.441474\n",
      "Step: 57\tloss: 885.870760\n",
      "Step: 58\tloss: 885.653639\n",
      "Step: 59\tloss: 886.921715\n",
      "Step: 60\tloss: 887.191993\n",
      "Step: 61\tloss: 885.678031\n",
      "Step: 62\tloss: 886.524560\n",
      "Step: 63\tloss: 886.585398\n",
      "Step: 64\tloss: 886.841455\n",
      "Step: 65\tloss: 885.718301\n",
      "Step: 66\tloss: 885.873789\n",
      "Step: 67\tloss: 886.990329\n",
      "Step: 68\tloss: 887.056299\n",
      "Step: 69\tloss: 885.845414\n",
      "Step: 70\tloss: 886.543180\n",
      "Step: 71\tloss: 886.796391\n",
      "Step: 72\tloss: 886.438483\n",
      "Step: 73\tloss: 885.598397\n",
      "Step: 74\tloss: 886.613239\n",
      "Step: 75\tloss: 886.638845\n",
      "Step: 76\tloss: 886.773906\n",
      "Step: 77\tloss: 885.749266\n",
      "Step: 78\tloss: 886.020315\n",
      "Step: 79\tloss: 886.885788\n",
      "Step: 80\tloss: 886.955529\n",
      "Step: 81\tloss: 885.655942\n",
      "Step: 82\tloss: 885.985160\n",
      "Step: 83\tloss: 886.700968\n",
      "Step: 84\tloss: 887.292718\n",
      "Step: 85\tloss: 885.756570\n",
      "Step: 86\tloss: 886.444417\n",
      "Step: 87\tloss: 886.670452\n",
      "Step: 88\tloss: 886.762961\n",
      "Step: 89\tloss: 885.829934\n",
      "Step: 90\tloss: 886.421538\n",
      "Step: 91\tloss: 886.812425\n",
      "Step: 92\tloss: 886.561556\n",
      "Step: 93\tloss: 885.361849\n",
      "Step: 94\tloss: 886.337985\n",
      "Step: 95\tloss: 886.885130\n",
      "Step: 96\tloss: 887.054204\n",
      "Step: 97\tloss: 885.675259\n",
      "Step: 98\tloss: 885.941965\n",
      "Step: 99\tloss: 886.938970\n",
      "Step: 100\tloss: 887.088077\n",
      "Step: 101\tloss: 885.981359\n",
      "Step: 102\tloss: 886.403871\n",
      "Step: 103\tloss: 886.591082\n",
      "Step: 104\tloss: 886.647042\n",
      "Step: 105\tloss: 885.651376\n",
      "Step: 106\tloss: 886.721204\n",
      "Step: 107\tloss: 886.388280\n",
      "Step: 108\tloss: 886.870309\n",
      "Step: 109\tloss: 885.703672\n",
      "Step: 110\tloss: 886.244832\n",
      "Step: 111\tloss: 886.811713\n",
      "Step: 112\tloss: 886.889665\n",
      "Step: 113\tloss: 885.861089\n",
      "Step: 114\tloss: 886.492883\n",
      "Step: 115\tloss: 886.667528\n",
      "Step: 116\tloss: 886.620310\n",
      "Step: 117\tloss: 885.791794\n",
      "Step: 118\tloss: 885.946822\n",
      "Step: 119\tloss: 886.979644\n",
      "Step: 120\tloss: 886.918548\n",
      "Step: 121\tloss: 885.783160\n",
      "Step: 122\tloss: 886.500851\n",
      "Step: 123\tloss: 886.684686\n",
      "Step: 124\tloss: 886.649727\n",
      "Step: 125\tloss: 886.070010\n",
      "Step: 126\tloss: 885.884185\n",
      "Step: 127\tloss: 886.682452\n",
      "Step: 128\tloss: 887.007892\n",
      "Step: 129\tloss: 885.994068\n",
      "Step: 130\tloss: 886.258523\n",
      "Step: 131\tloss: 886.377880\n",
      "Step: 132\tloss: 887.001005\n",
      "Step: 133\tloss: 885.770532\n",
      "Step: 134\tloss: 885.739194\n",
      "Step: 135\tloss: 886.988716\n",
      "Step: 136\tloss: 887.124285\n",
      "Step: 137\tloss: 886.004206\n",
      "Step: 138\tloss: 886.248090\n",
      "Step: 139\tloss: 886.710888\n",
      "Step: 140\tloss: 886.674185\n",
      "Step: 141\tloss: 885.859109\n",
      "Step: 142\tloss: 886.468538\n",
      "Step: 143\tloss: 886.712511\n",
      "Step: 144\tloss: 886.590355\n",
      "Step: 145\tloss: 885.838228\n",
      "Step: 146\tloss: 886.825593\n",
      "Step: 147\tloss: 886.407943\n",
      "Step: 148\tloss: 886.571167\n",
      "Step: 149\tloss: 885.647344\n",
      "Step: 150\tloss: 886.689773\n",
      "Step: 151\tloss: 886.210608\n",
      "Step: 152\tloss: 887.079356\n",
      "Step: 153\tloss: 886.345369\n",
      "Step: 154\tloss: 886.078198\n",
      "Step: 155\tloss: 886.989918\n",
      "Step: 156\tloss: 886.227128\n",
      "Step: 157\tloss: 885.828321\n",
      "Step: 158\tloss: 886.423434\n",
      "Step: 159\tloss: 886.594454\n",
      "Step: 160\tloss: 886.798980\n",
      "Step: 161\tloss: 885.503190\n",
      "Step: 162\tloss: 886.088393\n",
      "Step: 163\tloss: 886.949408\n",
      "Step: 164\tloss: 887.104103\n",
      "Step: 165\tloss: 885.951636\n",
      "Step: 166\tloss: 886.333226\n",
      "Step: 167\tloss: 886.650104\n",
      "Step: 168\tloss: 886.703570\n",
      "Step: 169\tloss: 885.484741\n",
      "Step: 170\tloss: 886.632668\n",
      "Step: 171\tloss: 886.534152\n",
      "Step: 172\tloss: 887.008605\n",
      "Step: 173\tloss: 886.272907\n",
      "Step: 174\tloss: 885.597024\n",
      "Step: 175\tloss: 886.824686\n",
      "Step: 176\tloss: 886.941476\n",
      "Step: 177\tloss: 886.104758\n",
      "Step: 178\tloss: 886.307209\n",
      "Step: 179\tloss: 886.428401\n",
      "Step: 180\tloss: 886.809494\n",
      "Step: 181\tloss: 885.908197\n",
      "Step: 182\tloss: 885.895988\n",
      "Step: 183\tloss: 886.562222\n",
      "Step: 184\tloss: 887.275741\n",
      "Step: 185\tloss: 885.737590\n",
      "Step: 186\tloss: 886.528869\n",
      "Step: 187\tloss: 886.694778\n",
      "Step: 188\tloss: 886.686668\n",
      "Step: 189\tloss: 885.766051\n",
      "Step: 190\tloss: 886.191052\n",
      "Step: 191\tloss: 886.747966\n",
      "Step: 192\tloss: 886.934351\n",
      "Step: 193\tloss: 886.002622\n",
      "Step: 194\tloss: 886.079260\n",
      "Step: 195\tloss: 886.865579\n",
      "Step: 196\tloss: 886.686652\n",
      "Step: 197\tloss: 885.910485\n",
      "Step: 198\tloss: 886.668974\n",
      "Step: 199\tloss: 886.319754\n",
      "Step: 200\tloss: 886.727587\n",
      "pval: 0.398391\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted parameters can be retrieved by calling the corresponding parameters of `estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_loc_params: 6, features: 100)>\n",
       "array([[ 8.625742,  9.126231,  7.285968, ...,  9.557749,  9.107832,  5.972532],\n",
       "       [-0.101383, -0.035887, -0.028139, ...,  0.040597,  0.326037, -0.104224],\n",
       "       [ 0.101383,  0.035887,  0.028139, ..., -0.040597, -0.326037,  0.104224],\n",
       "       [-0.214115, -0.063322, -0.168611, ..., -0.542266, -0.122744, -0.011004],\n",
       "       [ 0.214115,  0.063322,  0.168611, ...,  0.542266,  0.122744,  0.011004],\n",
       "       [-0.019127,  0.020523,  0.741212, ...,  0.054802, -0.300811, -0.184867]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "    feature_allzero    (features) bool False False False False False False ...\n",
       "  * features           (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_scale_params: 6, features: 100)>\n",
       "array([[ 1.312174,  0.621889,  0.420395, ...,  0.695584,  2.355036,  2.604524],\n",
       "       [ 0.014732, -0.019661, -0.13032 , ...,  0.142893, -0.062828, -0.380976],\n",
       "       [-0.014732,  0.019661,  0.13032 , ..., -0.142893,  0.062828,  0.380976],\n",
       "       [ 0.134801,  0.38704 ,  0.116158, ...,  0.09413 ,  0.428912,  0.076576],\n",
       "       [-0.134801, -0.38704 , -0.116158, ..., -0.09413 , -0.428912, -0.076576],\n",
       "       [ 0.580679,  0.895256,  0.135817, ...,  0.780899, -0.285947, -0.80249 ]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "    feature_allzero      (features) bool False False False False False False ...\n",
       "  * features             (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(5.551115e-17)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(5.551115e-17)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.02\n",
      "Root mean squared deviation of scale:    0.07\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have 4 biological replicates (animals, patients, cell culture replicates etc.) in a treatment experiment: 2 in each condition (treated, untreated). Accordingly, there is perfect confounding at this level already. We circumvent this by constraining the biological replicate coefficients to not model mean trends (constraints 0,1). Secondly, there a are technical replicates which contain cells from one biological replicate from each condition. Each biological replicate was assigned to one treated-untreated sample pair and each pair split into two technical replicates. Again, we correct perfect confouding by constrainig the techincal replicate coefficients not to model mean effects by constraints 2,3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 2000\n",
    "dmat = np.zeros([ncells, 10])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1\n",
    "dmat[500:1000,2] = 1 # bio rep 2\n",
    "dmat[1000:1500,3] = 1 # bio rep 3\n",
    "dmat[1500:2000,4] = 1 # bio rep 4\n",
    "dmat[0:250,5] = 1 # tech rep 1\n",
    "dmat[1000:1250,5] = 1 # tech rep 1\n",
    "dmat[250:500,6] = 1 # tech rep 2\n",
    "dmat[1250:1500,6] = 1 # tech rep 2\n",
    "dmat[500:750,7] = 1 # tech rep 3\n",
    "dmat[1500:1750,7] = 1 # tech rep 3\n",
    "dmat[750:1000,8] = 1 # tech rep 4\n",
    "dmat[1750:2000,8] = 1 # tech rep 4\n",
    "dmat[1000:2000,9] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[12680,  2506,  4908, ...,  5762,  4786,  5572],\n",
       "       [17086,  2900,  1987, ...,  4088,  9991,  6787],\n",
       "       [17571,  2841,  2802, ...,  3149, 11308, 10410],\n",
       "       ...,\n",
       "       [32627,  8677, 13994, ...,  6785,  7857, 12069],\n",
       "       [26009,  9312,  3751, ...,  5933, 13628, 14741],\n",
       "       [34630,  6377,  9495, ...,  6308,  7468,  6858]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build constraints based on sets of parameters that have to sum to zero. Each of these constraints is enforced by binding one of these parameters to the rest of the set. Such a constraint is encoded by assigning a 1 to each parameter in the set and a -1 to to the dependent parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dmat_est_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0., -1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([4, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset. \n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "# Constraint 2: Account for perfect confouding at biological replicate and technical replicate \n",
    "# by constraining technical replicate coefficients not to produce mean effects across biological replicates.\n",
    "constraints_loc[2,7] = -1\n",
    "constraints_loc[2,8:9] = 1\n",
    "# Constraint 3: Account for fact that first level of technical replicates was not absorbed into offset. \n",
    "constraints_loc[3,5] = -1\n",
    "constraints_loc[3,6:9] = 1\n",
    "\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 4\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale)\n",
    "input_data.constraints_loc = constraints_loc\n",
    "input_data.constraints_scale = constraints_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no closed form estimator for the mean model here due to the confounding. The model is initialised with least squares but the mean model is also trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: True\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 904.564282\n",
      "Step: 2\tloss: 928.493199\n",
      "Step: 3\tloss: 911.408461\n",
      "Step: 4\tloss: 909.972948\n",
      "Step: 5\tloss: 912.167042\n",
      "Step: 6\tloss: 918.066619\n",
      "Step: 7\tloss: 914.532071\n",
      "Step: 8\tloss: 912.801657\n",
      "Step: 9\tloss: 908.969919\n",
      "Step: 10\tloss: 911.728047\n",
      "Step: 11\tloss: 909.842970\n",
      "Step: 12\tloss: 910.580294\n",
      "Step: 13\tloss: 907.937643\n",
      "Step: 14\tloss: 911.068320\n",
      "Step: 15\tloss: 909.808798\n",
      "Step: 16\tloss: 909.980918\n",
      "Step: 17\tloss: 906.436535\n",
      "Step: 18\tloss: 908.543375\n",
      "Step: 19\tloss: 908.974829\n",
      "Step: 20\tloss: 908.862931\n",
      "Step: 21\tloss: 905.784165\n",
      "Step: 22\tloss: 908.232017\n",
      "Step: 23\tloss: 908.005119\n",
      "Step: 24\tloss: 907.335556\n",
      "Step: 25\tloss: 905.420198\n",
      "Step: 26\tloss: 907.444726\n",
      "Step: 27\tloss: 907.475302\n",
      "Step: 28\tloss: 906.886694\n",
      "Step: 29\tloss: 904.805719\n",
      "Step: 30\tloss: 907.623506\n",
      "Step: 31\tloss: 907.025185\n",
      "Step: 32\tloss: 906.508362\n",
      "Step: 33\tloss: 905.863367\n",
      "Step: 34\tloss: 907.011548\n",
      "Step: 35\tloss: 906.328903\n",
      "Step: 36\tloss: 905.843100\n",
      "Step: 37\tloss: 904.980870\n",
      "Step: 38\tloss: 907.080923\n",
      "Step: 39\tloss: 906.274185\n",
      "Step: 40\tloss: 906.203823\n",
      "Step: 41\tloss: 904.264367\n",
      "Step: 42\tloss: 906.898313\n",
      "Step: 43\tloss: 906.667967\n",
      "Step: 44\tloss: 906.404400\n",
      "Step: 45\tloss: 904.708897\n",
      "Step: 46\tloss: 906.658948\n",
      "Step: 47\tloss: 906.272314\n",
      "Step: 48\tloss: 906.438654\n",
      "Step: 49\tloss: 904.707451\n",
      "Step: 50\tloss: 907.006177\n",
      "Step: 51\tloss: 905.964368\n",
      "Step: 52\tloss: 906.475492\n",
      "Step: 53\tloss: 904.680461\n",
      "Step: 54\tloss: 906.190091\n",
      "Step: 55\tloss: 906.339805\n",
      "Step: 56\tloss: 906.718543\n",
      "Step: 57\tloss: 904.241641\n",
      "Step: 58\tloss: 906.316934\n",
      "Step: 59\tloss: 906.519771\n",
      "Step: 60\tloss: 906.688196\n",
      "Step: 61\tloss: 904.607045\n",
      "Step: 62\tloss: 906.537608\n",
      "Step: 63\tloss: 906.243234\n",
      "Step: 64\tloss: 906.320520\n",
      "Step: 65\tloss: 904.455579\n",
      "Step: 66\tloss: 906.717527\n",
      "Step: 67\tloss: 906.119048\n",
      "Step: 68\tloss: 906.554231\n",
      "Step: 69\tloss: 904.644577\n",
      "Step: 70\tloss: 906.084810\n",
      "Step: 71\tloss: 906.204519\n",
      "Step: 72\tloss: 907.194754\n",
      "Step: 73\tloss: 904.365944\n",
      "Step: 74\tloss: 906.151339\n",
      "Step: 75\tloss: 906.512728\n",
      "Step: 76\tloss: 906.949342\n",
      "Step: 77\tloss: 904.678852\n",
      "Step: 78\tloss: 906.569234\n",
      "Step: 79\tloss: 906.761726\n",
      "Step: 80\tloss: 906.060297\n",
      "Step: 81\tloss: 904.775037\n",
      "Step: 82\tloss: 906.525730\n",
      "Step: 83\tloss: 906.222495\n",
      "Step: 84\tloss: 906.716847\n",
      "Step: 85\tloss: 904.557917\n",
      "Step: 86\tloss: 906.836873\n",
      "Step: 87\tloss: 906.558192\n",
      "Step: 88\tloss: 906.550466\n",
      "Step: 89\tloss: 904.736122\n",
      "Step: 90\tloss: 907.595970\n",
      "Step: 91\tloss: 906.243108\n",
      "Step: 92\tloss: 906.222232\n",
      "Step: 93\tloss: 905.106148\n",
      "Step: 94\tloss: 906.435340\n",
      "Step: 95\tloss: 907.334976\n",
      "Step: 96\tloss: 906.280599\n",
      "Step: 97\tloss: 904.567119\n",
      "Step: 98\tloss: 907.007801\n",
      "Step: 99\tloss: 906.555637\n",
      "Step: 100\tloss: 906.923078\n",
      "Step: 101\tloss: 904.423817\n",
      "Step: 102\tloss: 906.820810\n",
      "Step: 103\tloss: 906.429696\n",
      "Step: 104\tloss: 907.262720\n",
      "Step: 105\tloss: 904.216106\n",
      "Step: 106\tloss: 906.730743\n",
      "Step: 107\tloss: 906.545773\n",
      "Step: 108\tloss: 907.146237\n",
      "Step: 109\tloss: 904.727371\n",
      "Step: 110\tloss: 906.694846\n",
      "Step: 111\tloss: 906.242527\n",
      "Step: 112\tloss: 906.897454\n",
      "Step: 113\tloss: 904.637864\n",
      "Step: 114\tloss: 906.826514\n",
      "Step: 115\tloss: 906.008841\n",
      "Step: 116\tloss: 907.152373\n",
      "Step: 117\tloss: 904.502825\n",
      "Step: 118\tloss: 905.820364\n",
      "Step: 119\tloss: 906.745417\n",
      "Step: 120\tloss: 907.271347\n",
      "Step: 121\tloss: 904.446201\n",
      "Step: 122\tloss: 906.470345\n",
      "Step: 123\tloss: 906.349071\n",
      "Step: 124\tloss: 906.972073\n",
      "Step: 125\tloss: 904.299678\n",
      "Step: 126\tloss: 907.198730\n",
      "Step: 127\tloss: 906.856006\n",
      "Step: 128\tloss: 906.132187\n",
      "Step: 129\tloss: 904.742639\n",
      "Step: 130\tloss: 906.763496\n",
      "Step: 131\tloss: 907.261517\n",
      "Step: 132\tloss: 905.899535\n",
      "Step: 133\tloss: 904.896714\n",
      "Step: 134\tloss: 906.596655\n",
      "Step: 135\tloss: 906.627781\n",
      "Step: 136\tloss: 906.988605\n",
      "Step: 137\tloss: 904.402207\n",
      "Step: 138\tloss: 906.959851\n",
      "Step: 139\tloss: 906.778308\n",
      "Step: 140\tloss: 907.111208\n",
      "Step: 141\tloss: 904.459248\n",
      "Step: 142\tloss: 906.789117\n",
      "Step: 143\tloss: 906.880507\n",
      "Step: 144\tloss: 906.865881\n",
      "Step: 145\tloss: 904.685831\n",
      "Step: 146\tloss: 906.467645\n",
      "Step: 147\tloss: 906.746610\n",
      "Step: 148\tloss: 906.865114\n",
      "Step: 149\tloss: 904.309599\n",
      "Step: 150\tloss: 906.729162\n",
      "Step: 151\tloss: 907.124583\n",
      "Step: 152\tloss: 906.769781\n",
      "Step: 153\tloss: 904.986058\n",
      "Step: 154\tloss: 907.397015\n",
      "Step: 155\tloss: 907.164169\n",
      "Step: 156\tloss: 905.804775\n",
      "Step: 157\tloss: 905.045124\n",
      "Step: 158\tloss: 907.025659\n",
      "Step: 159\tloss: 907.259803\n",
      "Step: 160\tloss: 906.308413\n",
      "Step: 161\tloss: 904.741787\n",
      "Step: 162\tloss: 906.927273\n",
      "Step: 163\tloss: 907.024666\n",
      "Step: 164\tloss: 906.816388\n",
      "Step: 165\tloss: 905.692328\n",
      "Step: 166\tloss: 905.910629\n",
      "Step: 167\tloss: 906.823584\n",
      "Step: 168\tloss: 906.661166\n",
      "Step: 169\tloss: 904.428071\n",
      "Step: 170\tloss: 907.403788\n",
      "Step: 171\tloss: 906.809740\n",
      "Step: 172\tloss: 906.257977\n",
      "Step: 173\tloss: 904.335038\n",
      "Step: 174\tloss: 907.268737\n",
      "Step: 175\tloss: 906.429367\n",
      "Step: 176\tloss: 907.182466\n",
      "Step: 177\tloss: 904.126147\n",
      "Step: 178\tloss: 906.977171\n",
      "Step: 179\tloss: 907.485925\n",
      "Step: 180\tloss: 907.264980\n",
      "Step: 181\tloss: 904.629509\n",
      "Step: 182\tloss: 907.032430\n",
      "Step: 183\tloss: 907.567716\n",
      "Step: 184\tloss: 906.734001\n",
      "Step: 185\tloss: 904.308602\n",
      "Step: 186\tloss: 907.520720\n",
      "Step: 187\tloss: 906.931407\n",
      "Step: 188\tloss: 907.028924\n",
      "Step: 189\tloss: 905.063341\n",
      "Step: 190\tloss: 906.919988\n",
      "Step: 191\tloss: 906.229268\n",
      "Step: 192\tloss: 907.231566\n",
      "Step: 193\tloss: 904.384761\n",
      "Step: 194\tloss: 907.540671\n",
      "Step: 195\tloss: 907.224294\n",
      "Step: 196\tloss: 906.492033\n",
      "Step: 197\tloss: 904.958727\n",
      "Step: 198\tloss: 906.612067\n",
      "Step: 199\tloss: 907.567946\n",
      "Step: 200\tloss: 906.476139\n",
      "pval: 0.002458\n",
      "Step: 201\tloss: 904.220624\n",
      "Step: 202\tloss: 907.582120\n",
      "Step: 203\tloss: 906.544103\n",
      "Step: 204\tloss: 907.224970\n",
      "Step: 205\tloss: 905.168714\n",
      "Step: 206\tloss: 906.764823\n",
      "Step: 207\tloss: 906.255131\n",
      "Step: 208\tloss: 907.196492\n",
      "Step: 209\tloss: 904.391974\n",
      "Step: 210\tloss: 906.750247\n",
      "Step: 211\tloss: 906.953595\n",
      "Step: 212\tloss: 907.104275\n",
      "Step: 213\tloss: 904.508096\n",
      "Step: 214\tloss: 907.493155\n",
      "Step: 215\tloss: 906.987324\n",
      "Step: 216\tloss: 906.374676\n",
      "Step: 217\tloss: 904.667824\n",
      "Step: 218\tloss: 907.481499\n",
      "Step: 219\tloss: 906.538365\n",
      "Step: 220\tloss: 906.697382\n",
      "Step: 221\tloss: 904.493855\n",
      "Step: 222\tloss: 907.256481\n",
      "Step: 223\tloss: 906.835099\n",
      "Step: 224\tloss: 906.801603\n",
      "Step: 225\tloss: 905.222468\n",
      "Step: 226\tloss: 906.351309\n",
      "Step: 227\tloss: 907.150718\n",
      "Step: 228\tloss: 906.662794\n",
      "Step: 229\tloss: 903.644511\n",
      "Step: 230\tloss: 907.072705\n",
      "Step: 231\tloss: 907.511432\n",
      "Step: 232\tloss: 907.333869\n",
      "Step: 233\tloss: 905.565578\n",
      "Step: 234\tloss: 906.263426\n",
      "Step: 235\tloss: 907.371505\n",
      "Step: 236\tloss: 906.754127\n",
      "Step: 237\tloss: 904.854262\n",
      "Step: 238\tloss: 906.869058\n",
      "Step: 239\tloss: 906.641527\n",
      "Step: 240\tloss: 907.346552\n",
      "Step: 241\tloss: 904.176141\n",
      "Step: 242\tloss: 906.887574\n",
      "Step: 243\tloss: 906.771030\n",
      "Step: 244\tloss: 907.433935\n",
      "Step: 245\tloss: 904.552324\n",
      "Step: 246\tloss: 906.772577\n",
      "Step: 247\tloss: 906.948705\n",
      "Step: 248\tloss: 906.821607\n",
      "Step: 249\tloss: 904.441404\n",
      "Step: 250\tloss: 906.693407\n",
      "Step: 251\tloss: 907.417509\n",
      "Step: 252\tloss: 906.791594\n",
      "Step: 253\tloss: 905.047276\n",
      "Step: 254\tloss: 907.324680\n",
      "Step: 255\tloss: 906.741128\n",
      "Step: 256\tloss: 906.450408\n",
      "Step: 257\tloss: 904.468356\n",
      "Step: 258\tloss: 906.951885\n",
      "Step: 259\tloss: 907.085979\n",
      "Step: 260\tloss: 907.140150\n",
      "Step: 261\tloss: 904.546604\n",
      "Step: 262\tloss: 906.855074\n",
      "Step: 263\tloss: 906.853568\n",
      "Step: 264\tloss: 907.028138\n",
      "Step: 265\tloss: 904.692351\n",
      "Step: 266\tloss: 906.094952\n",
      "Step: 267\tloss: 907.748165\n",
      "Step: 268\tloss: 906.680240\n",
      "Step: 269\tloss: 905.519559\n",
      "Step: 270\tloss: 906.523890\n",
      "Step: 271\tloss: 907.353267\n",
      "Step: 272\tloss: 905.881641\n",
      "Step: 273\tloss: 904.499489\n",
      "Step: 274\tloss: 906.692817\n",
      "Step: 275\tloss: 907.398447\n",
      "Step: 276\tloss: 906.648850\n",
      "Step: 277\tloss: 904.641527\n",
      "Step: 278\tloss: 907.094205\n",
      "Step: 279\tloss: 907.252378\n",
      "Step: 280\tloss: 906.059764\n",
      "Step: 281\tloss: 905.076875\n",
      "Step: 282\tloss: 906.636239\n",
      "Step: 283\tloss: 907.160037\n",
      "Step: 284\tloss: 906.360352\n",
      "Step: 285\tloss: 904.389911\n",
      "Step: 286\tloss: 906.863757\n",
      "Step: 287\tloss: 907.790084\n",
      "Step: 288\tloss: 906.400222\n",
      "Step: 289\tloss: 904.983958\n",
      "Step: 290\tloss: 906.666906\n",
      "Step: 291\tloss: 906.882297\n",
      "Step: 292\tloss: 906.936259\n",
      "Step: 293\tloss: 905.036615\n",
      "Step: 294\tloss: 907.250135\n",
      "Step: 295\tloss: 906.326300\n",
      "Step: 296\tloss: 906.786956\n",
      "Step: 297\tloss: 904.497478\n",
      "Step: 298\tloss: 907.489806\n",
      "Step: 299\tloss: 906.447368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300\tloss: 906.849538\n",
      "pval: 0.686730\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.04\n",
      "Root mean squared deviation of scale:    0.10\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have the same scenario as in example 2 but one technical replicate is missing. We have to drop the corresponding constraint and remove the two parameters belonging to this pair of technical replicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 2000\n",
    "dmat = np.zeros([ncells, 8])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1\n",
    "dmat[500:1000,2] = 1 # bio rep 2\n",
    "dmat[1000:1500,3] = 1 # bio rep 3\n",
    "dmat[1500:2000,4] = 1 # bio rep 4\n",
    "dmat[0:250,5] = 1 # tech rep 1\n",
    "dmat[1000:1250,5] = 1 # tech rep 1\n",
    "dmat[250:500,6] = 1 # tech rep 2\n",
    "dmat[1250:1500,6] = 1 # tech rep 2\n",
    "dmat[1000:2000,7] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[ 3832,  2956,   543, ...,  5513,  3259,  3833],\n",
       "       [ 1926,  2111,   627, ..., 19459,  2053,  2134],\n",
       "       [ 3835,  1066,   477, ...,  4612,  1840,  3153],\n",
       "       ...,\n",
       "       [ 4582,  3230,  1501, ...,  8243,  9656,  1498],\n",
       "       [ 5912,  1896,  2025, ...,  5869, 10596,  2846],\n",
       "       [ 8086,  4222,  1345, ...,  5232, 14108,  2616]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build constraints based on sets of parameters that have to sum to zero. Each of these constraints is enforced by binding one of these parameters to the rest of the set. Such a constraint is encoded by assigning a 1 to each parameter in the set and a -1 to to the dependent parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 1., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dmat_est_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0., -1.,  1.,  0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([3, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset. \n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "# Constraint 2: Account for fact that first level of technical replicates was not absorbed into offset. \n",
    "constraints_loc[2,5] = -1\n",
    "constraints_loc[2,6:7] = 1\n",
    "\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 3\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale)\n",
    "input_data.constraints_loc = constraints_loc\n",
    "input_data.constraints_scale = constraints_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no closed form estimator for the mean model here due to the confounding. The model is initialised with least squares but the mean model is also trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: True\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 904.564282\n",
      "Step: 2\tloss: 928.493199\n",
      "Step: 3\tloss: 911.408461\n",
      "Step: 4\tloss: 909.972948\n",
      "Step: 5\tloss: 912.167042\n",
      "Step: 6\tloss: 918.066619\n",
      "Step: 7\tloss: 914.532071\n",
      "Step: 8\tloss: 912.801657\n",
      "Step: 9\tloss: 908.969919\n",
      "Step: 10\tloss: 911.728047\n",
      "Step: 11\tloss: 909.842970\n",
      "Step: 12\tloss: 910.580294\n",
      "Step: 13\tloss: 907.937643\n",
      "Step: 14\tloss: 911.068320\n",
      "Step: 15\tloss: 909.808798\n",
      "Step: 16\tloss: 909.980918\n",
      "Step: 17\tloss: 906.436535\n",
      "Step: 18\tloss: 908.543375\n",
      "Step: 19\tloss: 908.974829\n",
      "Step: 20\tloss: 908.862931\n",
      "Step: 21\tloss: 905.784165\n",
      "Step: 22\tloss: 908.232017\n",
      "Step: 23\tloss: 908.005119\n",
      "Step: 24\tloss: 907.335556\n",
      "Step: 25\tloss: 905.420198\n",
      "Step: 26\tloss: 907.444726\n",
      "Step: 27\tloss: 907.475302\n",
      "Step: 28\tloss: 906.886694\n",
      "Step: 29\tloss: 904.805719\n",
      "Step: 30\tloss: 907.623506\n",
      "Step: 31\tloss: 907.025185\n",
      "Step: 32\tloss: 906.508362\n",
      "Step: 33\tloss: 905.863367\n",
      "Step: 34\tloss: 907.011548\n",
      "Step: 35\tloss: 906.328903\n",
      "Step: 36\tloss: 905.843100\n",
      "Step: 37\tloss: 904.980870\n",
      "Step: 38\tloss: 907.080923\n",
      "Step: 39\tloss: 906.274185\n",
      "Step: 40\tloss: 906.203823\n",
      "Step: 41\tloss: 904.264367\n",
      "Step: 42\tloss: 906.898313\n",
      "Step: 43\tloss: 906.667967\n",
      "Step: 44\tloss: 906.404400\n",
      "Step: 45\tloss: 904.708897\n",
      "Step: 46\tloss: 906.658948\n",
      "Step: 47\tloss: 906.272314\n",
      "Step: 48\tloss: 906.438654\n",
      "Step: 49\tloss: 904.707451\n",
      "Step: 50\tloss: 907.006177\n",
      "Step: 51\tloss: 905.964368\n",
      "Step: 52\tloss: 906.475492\n",
      "Step: 53\tloss: 904.680461\n",
      "Step: 54\tloss: 906.190091\n",
      "Step: 55\tloss: 906.339805\n",
      "Step: 56\tloss: 906.718543\n",
      "Step: 57\tloss: 904.241641\n",
      "Step: 58\tloss: 906.316934\n",
      "Step: 59\tloss: 906.519771\n",
      "Step: 60\tloss: 906.688196\n",
      "Step: 61\tloss: 904.607045\n",
      "Step: 62\tloss: 906.537608\n",
      "Step: 63\tloss: 906.243234\n",
      "Step: 64\tloss: 906.320520\n",
      "Step: 65\tloss: 904.455579\n",
      "Step: 66\tloss: 906.717527\n",
      "Step: 67\tloss: 906.119048\n",
      "Step: 68\tloss: 906.554231\n",
      "Step: 69\tloss: 904.644577\n",
      "Step: 70\tloss: 906.084810\n",
      "Step: 71\tloss: 906.204519\n",
      "Step: 72\tloss: 907.194754\n",
      "Step: 73\tloss: 904.365944\n",
      "Step: 74\tloss: 906.151339\n",
      "Step: 75\tloss: 906.512728\n",
      "Step: 76\tloss: 906.949342\n",
      "Step: 77\tloss: 904.678852\n",
      "Step: 78\tloss: 906.569234\n",
      "Step: 79\tloss: 906.761726\n",
      "Step: 80\tloss: 906.060297\n",
      "Step: 81\tloss: 904.775037\n",
      "Step: 82\tloss: 906.525730\n",
      "Step: 83\tloss: 906.222495\n",
      "Step: 84\tloss: 906.716847\n",
      "Step: 85\tloss: 904.557917\n",
      "Step: 86\tloss: 906.836873\n",
      "Step: 87\tloss: 906.558192\n",
      "Step: 88\tloss: 906.550466\n",
      "Step: 89\tloss: 904.736122\n",
      "Step: 90\tloss: 907.595970\n",
      "Step: 91\tloss: 906.243108\n",
      "Step: 92\tloss: 906.222232\n",
      "Step: 93\tloss: 905.106148\n",
      "Step: 94\tloss: 906.435340\n",
      "Step: 95\tloss: 907.334976\n",
      "Step: 96\tloss: 906.280599\n",
      "Step: 97\tloss: 904.567119\n",
      "Step: 98\tloss: 907.007801\n",
      "Step: 99\tloss: 906.555637\n",
      "Step: 100\tloss: 906.923078\n",
      "Step: 101\tloss: 904.423817\n",
      "Step: 102\tloss: 906.820810\n",
      "Step: 103\tloss: 906.429696\n",
      "Step: 104\tloss: 907.262720\n",
      "Step: 105\tloss: 904.216106\n",
      "Step: 106\tloss: 906.730743\n",
      "Step: 107\tloss: 906.545773\n",
      "Step: 108\tloss: 907.146237\n",
      "Step: 109\tloss: 904.727371\n",
      "Step: 110\tloss: 906.694846\n",
      "Step: 111\tloss: 906.242527\n",
      "Step: 112\tloss: 906.897454\n",
      "Step: 113\tloss: 904.637864\n",
      "Step: 114\tloss: 906.826514\n",
      "Step: 115\tloss: 906.008841\n",
      "Step: 116\tloss: 907.152373\n",
      "Step: 117\tloss: 904.502825\n",
      "Step: 118\tloss: 905.820364\n",
      "Step: 119\tloss: 906.745417\n",
      "Step: 120\tloss: 907.271347\n",
      "Step: 121\tloss: 904.446201\n",
      "Step: 122\tloss: 906.470345\n",
      "Step: 123\tloss: 906.349071\n",
      "Step: 124\tloss: 906.972073\n",
      "Step: 125\tloss: 904.299678\n",
      "Step: 126\tloss: 907.198730\n",
      "Step: 127\tloss: 906.856006\n",
      "Step: 128\tloss: 906.132187\n",
      "Step: 129\tloss: 904.742639\n",
      "Step: 130\tloss: 906.763496\n",
      "Step: 131\tloss: 907.261517\n",
      "Step: 132\tloss: 905.899535\n",
      "Step: 133\tloss: 904.896714\n",
      "Step: 134\tloss: 906.596655\n",
      "Step: 135\tloss: 906.627781\n",
      "Step: 136\tloss: 906.988605\n",
      "Step: 137\tloss: 904.402207\n",
      "Step: 138\tloss: 906.959851\n",
      "Step: 139\tloss: 906.778308\n",
      "Step: 140\tloss: 907.111208\n",
      "Step: 141\tloss: 904.459248\n",
      "Step: 142\tloss: 906.789117\n",
      "Step: 143\tloss: 906.880507\n",
      "Step: 144\tloss: 906.865881\n",
      "Step: 145\tloss: 904.685831\n",
      "Step: 146\tloss: 906.467645\n",
      "Step: 147\tloss: 906.746610\n",
      "Step: 148\tloss: 906.865114\n",
      "Step: 149\tloss: 904.309599\n",
      "Step: 150\tloss: 906.729162\n",
      "Step: 151\tloss: 907.124583\n",
      "Step: 152\tloss: 906.769781\n",
      "Step: 153\tloss: 904.986058\n",
      "Step: 154\tloss: 907.397015\n",
      "Step: 155\tloss: 907.164169\n",
      "Step: 156\tloss: 905.804775\n",
      "Step: 157\tloss: 905.045124\n",
      "Step: 158\tloss: 907.025659\n",
      "Step: 159\tloss: 907.259803\n",
      "Step: 160\tloss: 906.308413\n",
      "Step: 161\tloss: 904.741787\n",
      "Step: 162\tloss: 906.927273\n",
      "Step: 163\tloss: 907.024666\n",
      "Step: 164\tloss: 906.816388\n",
      "Step: 165\tloss: 905.692328\n",
      "Step: 166\tloss: 905.910629\n",
      "Step: 167\tloss: 906.823584\n",
      "Step: 168\tloss: 906.661166\n",
      "Step: 169\tloss: 904.428071\n",
      "Step: 170\tloss: 907.403788\n",
      "Step: 171\tloss: 906.809740\n",
      "Step: 172\tloss: 906.257977\n",
      "Step: 173\tloss: 904.335038\n",
      "Step: 174\tloss: 907.268737\n",
      "Step: 175\tloss: 906.429367\n",
      "Step: 176\tloss: 907.182466\n",
      "Step: 177\tloss: 904.126147\n",
      "Step: 178\tloss: 906.977171\n",
      "Step: 179\tloss: 907.485925\n",
      "Step: 180\tloss: 907.264980\n",
      "Step: 181\tloss: 904.629509\n",
      "Step: 182\tloss: 907.032430\n",
      "Step: 183\tloss: 907.567716\n",
      "Step: 184\tloss: 906.734001\n",
      "Step: 185\tloss: 904.308602\n",
      "Step: 186\tloss: 907.520720\n",
      "Step: 187\tloss: 906.931407\n",
      "Step: 188\tloss: 907.028924\n",
      "Step: 189\tloss: 905.063341\n",
      "Step: 190\tloss: 906.919988\n",
      "Step: 191\tloss: 906.229268\n",
      "Step: 192\tloss: 907.231566\n",
      "Step: 193\tloss: 904.384761\n",
      "Step: 194\tloss: 907.540671\n",
      "Step: 195\tloss: 907.224294\n",
      "Step: 196\tloss: 906.492033\n",
      "Step: 197\tloss: 904.958727\n",
      "Step: 198\tloss: 906.612067\n",
      "Step: 199\tloss: 907.567946\n",
      "Step: 200\tloss: 906.476139\n",
      "pval: 0.002458\n",
      "Step: 201\tloss: 904.220624\n",
      "Step: 202\tloss: 907.582120\n",
      "Step: 203\tloss: 906.544103\n",
      "Step: 204\tloss: 907.224970\n",
      "Step: 205\tloss: 905.168714\n",
      "Step: 206\tloss: 906.764823\n",
      "Step: 207\tloss: 906.255131\n",
      "Step: 208\tloss: 907.196492\n",
      "Step: 209\tloss: 904.391974\n",
      "Step: 210\tloss: 906.750247\n",
      "Step: 211\tloss: 906.953595\n",
      "Step: 212\tloss: 907.104275\n",
      "Step: 213\tloss: 904.508096\n",
      "Step: 214\tloss: 907.493155\n",
      "Step: 215\tloss: 906.987324\n",
      "Step: 216\tloss: 906.374676\n",
      "Step: 217\tloss: 904.667824\n",
      "Step: 218\tloss: 907.481499\n",
      "Step: 219\tloss: 906.538365\n",
      "Step: 220\tloss: 906.697382\n",
      "Step: 221\tloss: 904.493855\n",
      "Step: 222\tloss: 907.256481\n",
      "Step: 223\tloss: 906.835099\n",
      "Step: 224\tloss: 906.801603\n",
      "Step: 225\tloss: 905.222468\n",
      "Step: 226\tloss: 906.351309\n",
      "Step: 227\tloss: 907.150718\n",
      "Step: 228\tloss: 906.662794\n",
      "Step: 229\tloss: 903.644511\n",
      "Step: 230\tloss: 907.072705\n",
      "Step: 231\tloss: 907.511432\n",
      "Step: 232\tloss: 907.333869\n",
      "Step: 233\tloss: 905.565578\n",
      "Step: 234\tloss: 906.263426\n",
      "Step: 235\tloss: 907.371505\n",
      "Step: 236\tloss: 906.754127\n",
      "Step: 237\tloss: 904.854262\n",
      "Step: 238\tloss: 906.869058\n",
      "Step: 239\tloss: 906.641527\n",
      "Step: 240\tloss: 907.346552\n",
      "Step: 241\tloss: 904.176141\n",
      "Step: 242\tloss: 906.887574\n",
      "Step: 243\tloss: 906.771030\n",
      "Step: 244\tloss: 907.433935\n",
      "Step: 245\tloss: 904.552324\n",
      "Step: 246\tloss: 906.772577\n",
      "Step: 247\tloss: 906.948705\n",
      "Step: 248\tloss: 906.821607\n",
      "Step: 249\tloss: 904.441404\n",
      "Step: 250\tloss: 906.693407\n",
      "Step: 251\tloss: 907.417509\n",
      "Step: 252\tloss: 906.791594\n",
      "Step: 253\tloss: 905.047276\n",
      "Step: 254\tloss: 907.324680\n",
      "Step: 255\tloss: 906.741128\n",
      "Step: 256\tloss: 906.450408\n",
      "Step: 257\tloss: 904.468356\n",
      "Step: 258\tloss: 906.951885\n",
      "Step: 259\tloss: 907.085979\n",
      "Step: 260\tloss: 907.140150\n",
      "Step: 261\tloss: 904.546604\n",
      "Step: 262\tloss: 906.855074\n",
      "Step: 263\tloss: 906.853568\n",
      "Step: 264\tloss: 907.028138\n",
      "Step: 265\tloss: 904.692351\n",
      "Step: 266\tloss: 906.094952\n",
      "Step: 267\tloss: 907.748165\n",
      "Step: 268\tloss: 906.680240\n",
      "Step: 269\tloss: 905.519559\n",
      "Step: 270\tloss: 906.523890\n",
      "Step: 271\tloss: 907.353267\n",
      "Step: 272\tloss: 905.881641\n",
      "Step: 273\tloss: 904.499489\n",
      "Step: 274\tloss: 906.692817\n",
      "Step: 275\tloss: 907.398447\n",
      "Step: 276\tloss: 906.648850\n",
      "Step: 277\tloss: 904.641527\n",
      "Step: 278\tloss: 907.094205\n",
      "Step: 279\tloss: 907.252378\n",
      "Step: 280\tloss: 906.059764\n",
      "Step: 281\tloss: 905.076875\n",
      "Step: 282\tloss: 906.636239\n",
      "Step: 283\tloss: 907.160037\n",
      "Step: 284\tloss: 906.360352\n",
      "Step: 285\tloss: 904.389911\n",
      "Step: 286\tloss: 906.863757\n",
      "Step: 287\tloss: 907.790084\n",
      "Step: 288\tloss: 906.400222\n",
      "Step: 289\tloss: 904.983958\n",
      "Step: 290\tloss: 906.666906\n",
      "Step: 291\tloss: 906.882297\n",
      "Step: 292\tloss: 906.936259\n",
      "Step: 293\tloss: 905.036615\n",
      "Step: 294\tloss: 907.250135\n",
      "Step: 295\tloss: 906.326300\n",
      "Step: 296\tloss: 906.786956\n",
      "Step: 297\tloss: 904.497478\n",
      "Step: 298\tloss: 907.489806\n",
      "Step: 299\tloss: 906.447368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300\tloss: 906.849538\n",
      "pval: 0.686730\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.04\n",
      "Root mean squared deviation of scale:    0.10\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
