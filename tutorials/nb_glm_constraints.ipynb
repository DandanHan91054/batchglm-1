{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import xarray as xa\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect confounding occurs frequently in differential expression assays, often if biological replicates cannot be spread across conditions: This is often the case with animals or patients. Perfect confounding implies that the corresponding design matrix is not full rank and the model underdetermined. This can be circumvented by certain tricks (where replicates are modeled as the interaction of condition and and a replicate index per condition) which essentially regress repplicates to reference replicates. We believe that this is firstly undesirable as the condition coefficients depend on the identity of the reference replicates and accordingly on the ordering of the replicates, which has no experiental meaning and is purely a result of sample labels. Secondly, such tricks may be hard to come up with in hard cases such as presented in example 2 and 3 below. Here, we show how one can solve both problems by constraining parameterse in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have 4 biological replicates (animals, patients, cell culture replicates etc.) in a treatment experiment: 2 in each condition (treated, untreated). Accordingly, there is perfect confounding at this level. We circumvent this by constraining the biological replicate coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 2000\n",
    "dmat = np.zeros([ncells, 6])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1\n",
    "dmat[500:1000,2] = 1 # bio rep 2\n",
    "dmat[1000:1500,3] = 1 # bio rep 3\n",
    "dmat[1500:2000,4] = 1 # bio rep 4\n",
    "dmat[1000:2000,5] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[  694,  1738,  3272, ...,  2933,   922,  4066],\n",
       "       [  488,  1892,  3034, ...,  2933,   328,  2436],\n",
       "       [  655,  2255,  2069, ...,  2860,  1395,  2708],\n",
       "       ...,\n",
       "       [  404,  7535, 13393, ...,  3165,  1298,  4830],\n",
       "       [  410, 13540,  7661, ...,  9533,  3017,  4518],\n",
       "       [  283,  9925,  6575, ...,  3934,  1855,  2788]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters used to generate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'a' (design_loc_params: 6, features: 100)>\n",
       "array([[ 6.169171,  7.84706 ,  8.1824  , ...,  7.512092,  7.303938,  7.954016],\n",
       "       [ 0.616672,  0.049985,  0.156191, ...,  0.674974, -0.342042, -0.091577],\n",
       "       [-0.292955,  0.572764,  0.541457, ...,  0.519858,  0.231112, -0.456774],\n",
       "       [-0.552061,  0.559411,  0.667111, ...,  0.109105,  0.370208, -0.202652],\n",
       "       [-0.321391,  0.641852,  0.666109, ...,  0.34856 ,  0.521367,  0.376613],\n",
       "       [ 0.148241,  0.559405,  0.273667, ...,  0.579711, -0.485237, -0.270124]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'b' (design_scale_params: 6, features: 100)>\n",
       "array([[ 1.609438,  1.609438,  1.94591 , ...,  1.791759,  1.098612,  1.609438],\n",
       "       [ 0.233637,  0.625092,  0.468428, ...,  0.514288,  0.196374, -0.171865],\n",
       "       [-0.264192,  0.385856, -0.150861, ..., -0.177253,  0.180284,  0.321248],\n",
       "       [ 0.19759 ,  0.467183, -0.311065, ..., -0.153057,  0.245684,  0.05796 ],\n",
       "       [ 0.141119, -0.141433, -0.406333, ...,  0.600302,  0.394482,  0.623357],\n",
       "       [-0.129143,  0.227628,  0.252282, ..., -0.357212,  0.639319,  0.175286]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build constraints based on sets of parameters that have to sum to zero. Each of these constraints is enforced by binding one of these parameters to the rest of the set. Such a constraint is encoded by assigning a 1 to each parameter in the set and a -1 to to the dependent parameter. The constraints have to be ordered so that they can be iteratively applied from top to bottom and so that all independent parameters (1s) are defined at each stage: A dependent parameter may depend on another dependent parameter if the other dependent parameter was defined in a constrained that lies before the current constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([2, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset.\n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 2\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale,\n",
    "    constraints_loc=constraints_loc,\n",
    "    constraints_scale=constraints_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: False\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 884.155295\n",
      "Step: 2\tloss: 885.949234\n",
      "Step: 3\tloss: 886.515063\n",
      "Step: 4\tloss: 886.121214\n",
      "Step: 5\tloss: 884.298366\n",
      "Step: 6\tloss: 886.097554\n",
      "Step: 7\tloss: 885.781682\n",
      "Step: 8\tloss: 886.387190\n",
      "Step: 9\tloss: 884.287266\n",
      "Step: 10\tloss: 885.533888\n",
      "Step: 11\tloss: 886.013124\n",
      "Step: 12\tloss: 886.450422\n",
      "Step: 13\tloss: 884.066419\n",
      "Step: 14\tloss: 885.499019\n",
      "Step: 15\tloss: 886.634011\n",
      "Step: 16\tloss: 886.072353\n",
      "Step: 17\tloss: 884.011284\n",
      "Step: 18\tloss: 885.591750\n",
      "Step: 19\tloss: 885.806826\n",
      "Step: 20\tloss: 886.775648\n",
      "Step: 21\tloss: 884.142276\n",
      "Step: 22\tloss: 885.068036\n",
      "Step: 23\tloss: 886.356100\n",
      "Step: 24\tloss: 886.549458\n",
      "Step: 25\tloss: 883.926797\n",
      "Step: 26\tloss: 885.963304\n",
      "Step: 27\tloss: 886.032688\n",
      "Step: 28\tloss: 886.206786\n",
      "Step: 29\tloss: 884.735220\n",
      "Step: 30\tloss: 885.884261\n",
      "Step: 31\tloss: 885.712348\n",
      "Step: 32\tloss: 885.766772\n",
      "Step: 33\tloss: 884.133651\n",
      "Step: 34\tloss: 885.710198\n",
      "Step: 35\tloss: 886.317462\n",
      "Step: 36\tloss: 885.932682\n",
      "Step: 37\tloss: 884.224001\n",
      "Step: 38\tloss: 885.565194\n",
      "Step: 39\tloss: 886.587896\n",
      "Step: 40\tloss: 885.722530\n",
      "Step: 41\tloss: 884.340667\n",
      "Step: 42\tloss: 885.364912\n",
      "Step: 43\tloss: 885.471134\n",
      "Step: 44\tloss: 886.900006\n",
      "Step: 45\tloss: 883.792379\n",
      "Step: 46\tloss: 885.846442\n",
      "Step: 47\tloss: 886.584052\n",
      "Step: 48\tloss: 885.858021\n",
      "Step: 49\tloss: 884.224594\n",
      "Step: 50\tloss: 885.876069\n",
      "Step: 51\tloss: 885.826977\n",
      "Step: 52\tloss: 886.160562\n",
      "Step: 53\tloss: 884.657852\n",
      "Step: 54\tloss: 885.342229\n",
      "Step: 55\tloss: 886.064630\n",
      "Step: 56\tloss: 886.020860\n",
      "Step: 57\tloss: 884.137314\n",
      "Step: 58\tloss: 885.999870\n",
      "Step: 59\tloss: 885.622781\n",
      "Step: 60\tloss: 886.320590\n",
      "Step: 61\tloss: 884.317883\n",
      "Step: 62\tloss: 885.359559\n",
      "Step: 63\tloss: 886.040758\n",
      "Step: 64\tloss: 886.366337\n",
      "Step: 65\tloss: 884.531965\n",
      "Step: 66\tloss: 885.335511\n",
      "Step: 67\tloss: 885.696414\n",
      "Step: 68\tloss: 886.516993\n",
      "Step: 69\tloss: 884.632183\n",
      "Step: 70\tloss: 885.090128\n",
      "Step: 71\tloss: 886.560211\n",
      "Step: 72\tloss: 885.804169\n",
      "Step: 73\tloss: 883.780565\n",
      "Step: 74\tloss: 885.404722\n",
      "Step: 75\tloss: 886.270443\n",
      "Step: 76\tloss: 886.616829\n",
      "Step: 77\tloss: 884.135656\n",
      "Step: 78\tloss: 885.323764\n",
      "Step: 79\tloss: 886.172371\n",
      "Step: 80\tloss: 886.440966\n",
      "Step: 81\tloss: 884.608677\n",
      "Step: 82\tloss: 885.696085\n",
      "Step: 83\tloss: 885.898939\n",
      "Step: 84\tloss: 885.876806\n",
      "Step: 85\tloss: 884.357841\n",
      "Step: 86\tloss: 885.807913\n",
      "Step: 87\tloss: 886.009997\n",
      "Step: 88\tloss: 885.913269\n",
      "Step: 89\tloss: 884.241235\n",
      "Step: 90\tloss: 885.044609\n",
      "Step: 91\tloss: 886.606986\n",
      "Step: 92\tloss: 886.169448\n",
      "Step: 93\tloss: 884.017094\n",
      "Step: 94\tloss: 885.265347\n",
      "Step: 95\tloss: 886.426268\n",
      "Step: 96\tloss: 886.374816\n",
      "Step: 97\tloss: 884.513290\n",
      "Step: 98\tloss: 885.021783\n",
      "Step: 99\tloss: 885.696888\n",
      "Step: 100\tloss: 886.857720\n",
      "Step: 101\tloss: 884.614489\n",
      "Step: 102\tloss: 885.439132\n",
      "Step: 103\tloss: 886.525400\n",
      "Step: 104\tloss: 885.524428\n",
      "Step: 105\tloss: 883.887189\n",
      "Step: 106\tloss: 885.372630\n",
      "Step: 107\tloss: 886.128922\n",
      "Step: 108\tloss: 886.681996\n",
      "Step: 109\tloss: 884.410079\n",
      "Step: 110\tloss: 885.711807\n",
      "Step: 111\tloss: 885.621835\n",
      "Step: 112\tloss: 886.343056\n",
      "Step: 113\tloss: 884.268267\n",
      "Step: 114\tloss: 885.775851\n",
      "Step: 115\tloss: 885.673571\n",
      "Step: 116\tloss: 886.364654\n",
      "Step: 117\tloss: 884.198440\n",
      "Step: 118\tloss: 884.812962\n",
      "Step: 119\tloss: 886.560143\n",
      "Step: 120\tloss: 886.510064\n",
      "Step: 121\tloss: 884.809306\n",
      "Step: 122\tloss: 885.364047\n",
      "Step: 123\tloss: 886.148401\n",
      "Step: 124\tloss: 885.769926\n",
      "Step: 125\tloss: 884.196402\n",
      "Step: 126\tloss: 885.073848\n",
      "Step: 127\tloss: 886.529367\n",
      "Step: 128\tloss: 886.297055\n",
      "Step: 129\tloss: 884.172796\n",
      "Step: 130\tloss: 885.440222\n",
      "Step: 131\tloss: 886.113415\n",
      "Step: 132\tloss: 886.358516\n",
      "Step: 133\tloss: 884.735822\n",
      "Step: 134\tloss: 884.686678\n",
      "Step: 135\tloss: 886.324193\n",
      "Step: 136\tloss: 886.342070\n",
      "Step: 137\tloss: 884.265266\n",
      "Step: 138\tloss: 885.315472\n",
      "Step: 139\tloss: 886.279918\n",
      "Step: 140\tloss: 886.221305\n",
      "Step: 141\tloss: 884.321956\n",
      "Step: 142\tloss: 885.656819\n",
      "Step: 143\tloss: 885.746512\n",
      "Step: 144\tloss: 886.359128\n",
      "Step: 145\tloss: 883.999052\n",
      "Step: 146\tloss: 885.692688\n",
      "Step: 147\tloss: 886.271720\n",
      "Step: 148\tloss: 886.121239\n",
      "Step: 149\tloss: 884.575052\n",
      "Step: 150\tloss: 885.500059\n",
      "Step: 151\tloss: 885.839792\n",
      "Step: 152\tloss: 886.167966\n",
      "Step: 153\tloss: 884.472851\n",
      "Step: 154\tloss: 885.004543\n",
      "Step: 155\tloss: 886.377883\n",
      "Step: 156\tloss: 886.225060\n",
      "Step: 157\tloss: 883.902493\n",
      "Step: 158\tloss: 885.652581\n",
      "Step: 159\tloss: 886.339379\n",
      "Step: 160\tloss: 886.185064\n",
      "Step: 161\tloss: 884.740250\n",
      "Step: 162\tloss: 885.245989\n",
      "Step: 163\tloss: 886.483392\n",
      "Step: 164\tloss: 885.638019\n",
      "Step: 165\tloss: 884.334984\n",
      "Step: 166\tloss: 885.396297\n",
      "Step: 167\tloss: 886.232705\n",
      "Step: 168\tloss: 886.113187\n",
      "Step: 169\tloss: 884.306257\n",
      "Step: 170\tloss: 885.370254\n",
      "Step: 171\tloss: 886.724079\n",
      "Step: 172\tloss: 885.686472\n",
      "Step: 173\tloss: 884.478884\n",
      "Step: 174\tloss: 884.796268\n",
      "Step: 175\tloss: 886.811474\n",
      "Step: 176\tloss: 885.999788\n",
      "Step: 177\tloss: 884.401667\n",
      "Step: 178\tloss: 885.439817\n",
      "Step: 179\tloss: 885.913195\n",
      "Step: 180\tloss: 886.325138\n",
      "Step: 181\tloss: 884.009348\n",
      "Step: 182\tloss: 885.864172\n",
      "Step: 183\tloss: 885.675983\n",
      "Step: 184\tloss: 886.534623\n",
      "Step: 185\tloss: 884.366137\n",
      "Step: 186\tloss: 885.772806\n",
      "Step: 187\tloss: 886.016124\n",
      "Step: 188\tloss: 885.927820\n",
      "Step: 189\tloss: 884.622036\n",
      "Step: 190\tloss: 885.431462\n",
      "Step: 191\tloss: 885.945180\n",
      "Step: 192\tloss: 886.084253\n",
      "Step: 193\tloss: 884.475605\n",
      "Step: 194\tloss: 885.381023\n",
      "Step: 195\tloss: 886.491711\n",
      "Step: 196\tloss: 885.734246\n",
      "Step: 197\tloss: 884.397997\n",
      "Step: 198\tloss: 885.060351\n",
      "Step: 199\tloss: 886.292736\n",
      "Step: 200\tloss: 886.347099\n",
      "pval: 0.444001\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted parameters can be retrieved by calling the corresponding parameters of `estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_loc_params: 6, features: 100)>\n",
       "array([[ 6.348189,  8.164046,  8.538716, ...,  8.103653,  7.265792,  7.67668 ],\n",
       "       [ 0.459377, -0.266948, -0.208716, ...,  0.07624 , -0.239818,  0.184094],\n",
       "       [-0.459377,  0.266948,  0.208716, ..., -0.07624 ,  0.239818, -0.184094],\n",
       "       [-0.108924, -0.032833, -0.013623, ..., -0.13636 , -0.076002, -0.288177],\n",
       "       [ 0.108924,  0.032833,  0.013623, ...,  0.13636 ,  0.076002,  0.288177],\n",
       "       [-0.479316,  0.834696,  0.575472, ...,  0.190039,  0.016902,  0.082734]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "    feature_allzero    (features) bool False False False False False False ...\n",
       "  * features           (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_scale_params: 6, features: 100)>\n",
       "array([[ 1.615262,  2.038328,  2.0574  , ...,  2.063593,  1.261082,  1.732057],\n",
       "       [ 0.24935 ,  0.119381,  0.294329, ...,  0.33788 , -0.069746, -0.198959],\n",
       "       [-0.24935 , -0.119381, -0.294329, ..., -0.33788 ,  0.069746,  0.198959],\n",
       "       [ 0.009784,  0.353081,  0.02172 , ..., -0.423438, -0.0333  , -0.310581],\n",
       "       [-0.009784, -0.353081, -0.02172 , ...,  0.423438,  0.0333  ,  0.310581],\n",
       "       [-0.007874,  0.004785, -0.255241, ..., -0.32047 ,  0.780985,  0.420808]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "    feature_allzero      (features) bool False False False False False False ...\n",
       "  * features             (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(5.551115e-17)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.02\n",
      "Root mean squared deviation of scale:    0.07\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have 4 biological replicates (animals, patients, cell culture replicates etc.) in a treatment experiment: 2 in each condition (treated, untreated). Accordingly, there is perfect confounding at this level already. We circumvent this by constraining the biological replicate coefficients to not model mean trends (constraints 0,1). Secondly, there are technical replicates which contain cells from one biological replicate from each condition. Each biological replicate was assigned to one treated-untreated sample pair and each pair split into two technical replicates. Again, we correct perfect confouding by constrainig the techincal replicate coefficients not to model mean effects by constraints 2,3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 2000\n",
    "dmat = np.zeros([ncells, 10])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1\n",
    "dmat[500:1000,2] = 1 # bio rep 2\n",
    "dmat[1000:1500,3] = 1 # bio rep 3\n",
    "dmat[1500:2000,4] = 1 # bio rep 4\n",
    "dmat[0:250,5] = 1 # tech rep 1\n",
    "dmat[1000:1250,5] = 1 # tech rep 1\n",
    "dmat[250:500,6] = 1 # tech rep 2\n",
    "dmat[1250:1500,6] = 1 # tech rep 2\n",
    "dmat[500:750,7] = 1 # tech rep 3\n",
    "dmat[1500:1750,7] = 1 # tech rep 3\n",
    "dmat[750:1000,8] = 1 # tech rep 4\n",
    "dmat[1750:2000,8] = 1 # tech rep 4\n",
    "dmat[1000:2000,9] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[ 6791,   476,   645, ...,  6876,  7486, 14190],\n",
       "       [12923,  2740,  1093, ...,  4252,  6449, 12855],\n",
       "       [ 9694,  3576,  2441, ...,  4385,  7285, 15416],\n",
       "       ...,\n",
       "       [ 9782,  9281,   469, ..., 21728,  9560, 12528],\n",
       "       [ 7251,  9682,   676, ..., 21535, 11451,  9472],\n",
       "       [ 8910,  6861,  1934, ..., 30346,  5248, 20390]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dmat_est_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0., -1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([4, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset. \n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "# Constraint 2: Account for perfect confouding at biological replicate and technical replicate \n",
    "# by constraining technical replicate coefficients not to produce mean effects across biological replicates.\n",
    "constraints_loc[2,7] = -1\n",
    "constraints_loc[2,8:9] = 1\n",
    "# Constraint 3: Account for fact that first level of technical replicates was not absorbed into offset. \n",
    "constraints_loc[3,5] = -1\n",
    "constraints_loc[3,6:9] = 1\n",
    "\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 4\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale,\n",
    "    constraints_loc=constraints_loc,\n",
    "    constraints_scale=constraints_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no closed form estimator for the mean model here due to the confounding. The model is initialised with least squares but the mean model is also trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: True\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 897.701602\n",
      "Step: 2\tloss: 923.520278\n",
      "Step: 3\tloss: 908.112978\n",
      "Step: 4\tloss: 906.723808\n",
      "Step: 5\tloss: 905.482292\n",
      "Step: 6\tloss: 912.208433\n",
      "Step: 7\tloss: 912.437294\n",
      "Step: 8\tloss: 910.213020\n",
      "Step: 9\tloss: 903.013620\n",
      "Step: 10\tloss: 905.645439\n",
      "Step: 11\tloss: 908.425297\n",
      "Step: 12\tloss: 908.020575\n",
      "Step: 13\tloss: 901.502998\n",
      "Step: 14\tloss: 904.960904\n",
      "Step: 15\tloss: 907.300517\n",
      "Step: 16\tloss: 907.049878\n",
      "Step: 17\tloss: 900.179336\n",
      "Step: 18\tloss: 904.038785\n",
      "Step: 19\tloss: 905.077502\n",
      "Step: 20\tloss: 904.887815\n",
      "Step: 21\tloss: 899.667640\n",
      "Step: 22\tloss: 902.370722\n",
      "Step: 23\tloss: 904.283863\n",
      "Step: 24\tloss: 904.310988\n",
      "Step: 25\tloss: 898.702699\n",
      "Step: 26\tloss: 902.409750\n",
      "Step: 27\tloss: 904.419129\n",
      "Step: 28\tloss: 903.415020\n",
      "Step: 29\tloss: 898.090837\n",
      "Step: 30\tloss: 901.887437\n",
      "Step: 31\tloss: 903.316804\n",
      "Step: 32\tloss: 904.425322\n",
      "Step: 33\tloss: 898.402796\n",
      "Step: 34\tloss: 901.534879\n",
      "Step: 35\tloss: 903.101230\n",
      "Step: 36\tloss: 903.711218\n",
      "Step: 37\tloss: 897.721010\n",
      "Step: 38\tloss: 901.569581\n",
      "Step: 39\tloss: 903.099600\n",
      "Step: 40\tloss: 903.668827\n",
      "Step: 41\tloss: 897.950172\n",
      "Step: 42\tloss: 901.507157\n",
      "Step: 43\tloss: 903.458151\n",
      "Step: 44\tloss: 902.747584\n",
      "Step: 45\tloss: 897.724029\n",
      "Step: 46\tloss: 901.287298\n",
      "Step: 47\tloss: 903.206326\n",
      "Step: 48\tloss: 903.142030\n",
      "Step: 49\tloss: 897.939530\n",
      "Step: 50\tloss: 901.507566\n",
      "Step: 51\tloss: 902.845384\n",
      "Step: 52\tloss: 903.179161\n",
      "Step: 53\tloss: 897.718425\n",
      "Step: 54\tloss: 900.931264\n",
      "Step: 55\tloss: 903.147718\n",
      "Step: 56\tloss: 903.510608\n",
      "Step: 57\tloss: 897.868478\n",
      "Step: 58\tloss: 900.890416\n",
      "Step: 59\tloss: 903.355198\n",
      "Step: 60\tloss: 903.431985\n",
      "Step: 61\tloss: 897.469479\n",
      "Step: 62\tloss: 901.130281\n",
      "Step: 63\tloss: 903.700216\n",
      "Step: 64\tloss: 903.095570\n",
      "Step: 65\tloss: 898.005333\n",
      "Step: 66\tloss: 900.843121\n",
      "Step: 67\tloss: 902.889688\n",
      "Step: 68\tloss: 903.422279\n",
      "Step: 69\tloss: 898.011150\n",
      "Step: 70\tloss: 901.053908\n",
      "Step: 71\tloss: 902.796911\n",
      "Step: 72\tloss: 903.369653\n",
      "Step: 73\tloss: 897.733695\n",
      "Step: 74\tloss: 901.178007\n",
      "Step: 75\tloss: 902.906707\n",
      "Step: 76\tloss: 903.325296\n",
      "Step: 77\tloss: 897.802055\n",
      "Step: 78\tloss: 901.488332\n",
      "Step: 79\tloss: 902.837954\n",
      "Step: 80\tloss: 903.331671\n",
      "Step: 81\tloss: 897.933012\n",
      "Step: 82\tloss: 901.140435\n",
      "Step: 83\tloss: 902.704368\n",
      "Step: 84\tloss: 903.843791\n",
      "Step: 85\tloss: 897.792000\n",
      "Step: 86\tloss: 901.387786\n",
      "Step: 87\tloss: 903.546292\n",
      "Step: 88\tloss: 903.310465\n",
      "Step: 89\tloss: 897.965549\n",
      "Step: 90\tloss: 901.429979\n",
      "Step: 91\tloss: 903.525858\n",
      "Step: 92\tloss: 903.450776\n",
      "Step: 93\tloss: 897.676864\n",
      "Step: 94\tloss: 900.982497\n",
      "Step: 95\tloss: 903.800108\n",
      "Step: 96\tloss: 903.730797\n",
      "Step: 97\tloss: 898.122377\n",
      "Step: 98\tloss: 902.067263\n",
      "Step: 99\tloss: 903.461286\n",
      "Step: 100\tloss: 902.519410\n",
      "Step: 101\tloss: 897.742439\n",
      "Step: 102\tloss: 901.819770\n",
      "Step: 103\tloss: 903.088555\n",
      "Step: 104\tloss: 903.245606\n",
      "Step: 105\tloss: 897.385092\n",
      "Step: 106\tloss: 901.445256\n",
      "Step: 107\tloss: 903.427508\n",
      "Step: 108\tloss: 903.841865\n",
      "Step: 109\tloss: 897.656813\n",
      "Step: 110\tloss: 901.636137\n",
      "Step: 111\tloss: 903.714067\n",
      "Step: 112\tloss: 903.421644\n",
      "Step: 113\tloss: 897.766784\n",
      "Step: 114\tloss: 901.258322\n",
      "Step: 115\tloss: 903.574118\n",
      "Step: 116\tloss: 904.033996\n",
      "Step: 117\tloss: 897.653830\n",
      "Step: 118\tloss: 901.936146\n",
      "Step: 119\tloss: 903.800008\n",
      "Step: 120\tloss: 903.354363\n",
      "Step: 121\tloss: 897.995769\n",
      "Step: 122\tloss: 900.749250\n",
      "Step: 123\tloss: 903.934952\n",
      "Step: 124\tloss: 903.932810\n",
      "Step: 125\tloss: 897.659463\n",
      "Step: 126\tloss: 901.060125\n",
      "Step: 127\tloss: 904.237653\n",
      "Step: 128\tloss: 903.771829\n",
      "Step: 129\tloss: 898.149250\n",
      "Step: 130\tloss: 901.865311\n",
      "Step: 131\tloss: 903.549831\n",
      "Step: 132\tloss: 903.358333\n",
      "Step: 133\tloss: 898.626801\n",
      "Step: 134\tloss: 900.811181\n",
      "Step: 135\tloss: 903.968449\n",
      "Step: 136\tloss: 903.417587\n",
      "Step: 137\tloss: 897.613935\n",
      "Step: 138\tloss: 902.086749\n",
      "Step: 139\tloss: 903.573421\n",
      "Step: 140\tloss: 903.325908\n",
      "Step: 141\tloss: 897.833399\n",
      "Step: 142\tloss: 901.190264\n",
      "Step: 143\tloss: 903.799042\n",
      "Step: 144\tloss: 903.445083\n",
      "Step: 145\tloss: 897.965234\n",
      "Step: 146\tloss: 900.965625\n",
      "Step: 147\tloss: 903.245904\n",
      "Step: 148\tloss: 904.103878\n",
      "Step: 149\tloss: 898.168597\n",
      "Step: 150\tloss: 901.102295\n",
      "Step: 151\tloss: 903.368450\n",
      "Step: 152\tloss: 903.261997\n",
      "Step: 153\tloss: 897.816865\n",
      "Step: 154\tloss: 901.590172\n",
      "Step: 155\tloss: 902.956189\n",
      "Step: 156\tloss: 903.616280\n",
      "Step: 157\tloss: 898.387272\n",
      "Step: 158\tloss: 901.270564\n",
      "Step: 159\tloss: 903.598243\n",
      "Step: 160\tloss: 903.126741\n",
      "Step: 161\tloss: 898.233289\n",
      "Step: 162\tloss: 901.631289\n",
      "Step: 163\tloss: 903.935432\n",
      "Step: 164\tloss: 903.002196\n",
      "Step: 165\tloss: 897.879058\n",
      "Step: 166\tloss: 902.060866\n",
      "Step: 167\tloss: 903.273835\n",
      "Step: 168\tloss: 903.542722\n",
      "Step: 169\tloss: 897.338144\n",
      "Step: 170\tloss: 900.699219\n",
      "Step: 171\tloss: 903.997192\n",
      "Step: 172\tloss: 904.728861\n",
      "Step: 173\tloss: 897.958906\n",
      "Step: 174\tloss: 901.513208\n",
      "Step: 175\tloss: 903.479668\n",
      "Step: 176\tloss: 903.341019\n",
      "Step: 177\tloss: 897.987576\n",
      "Step: 178\tloss: 900.819141\n",
      "Step: 179\tloss: 903.972749\n",
      "Step: 180\tloss: 903.333991\n",
      "Step: 181\tloss: 898.450535\n",
      "Step: 182\tloss: 901.330967\n",
      "Step: 183\tloss: 903.019051\n",
      "Step: 184\tloss: 903.311966\n",
      "Step: 185\tloss: 897.611448\n",
      "Step: 186\tloss: 901.860146\n",
      "Step: 187\tloss: 903.460089\n",
      "Step: 188\tloss: 903.310984\n",
      "Step: 189\tloss: 897.913020\n",
      "Step: 190\tloss: 901.908982\n",
      "Step: 191\tloss: 902.944524\n",
      "Step: 192\tloss: 903.672507\n",
      "Step: 193\tloss: 898.579706\n",
      "Step: 194\tloss: 901.400194\n",
      "Step: 195\tloss: 902.986494\n",
      "Step: 196\tloss: 903.554463\n",
      "Step: 197\tloss: 898.481293\n",
      "Step: 198\tloss: 900.760722\n",
      "Step: 199\tloss: 903.882767\n",
      "Step: 200\tloss: 903.361713\n",
      "pval: 0.012133\n",
      "Step: 201\tloss: 898.528238\n",
      "Step: 202\tloss: 901.075467\n",
      "Step: 203\tloss: 903.312219\n",
      "Step: 204\tloss: 903.536112\n",
      "Step: 205\tloss: 897.493944\n",
      "Step: 206\tloss: 901.513671\n",
      "Step: 207\tloss: 903.330036\n",
      "Step: 208\tloss: 904.042069\n",
      "Step: 209\tloss: 898.163520\n",
      "Step: 210\tloss: 900.882022\n",
      "Step: 211\tloss: 903.760349\n",
      "Step: 212\tloss: 903.896708\n",
      "Step: 213\tloss: 897.533945\n",
      "Step: 214\tloss: 901.380184\n",
      "Step: 215\tloss: 904.551904\n",
      "Step: 216\tloss: 903.414016\n",
      "Step: 217\tloss: 898.235959\n",
      "Step: 218\tloss: 901.066753\n",
      "Step: 219\tloss: 903.840852\n",
      "Step: 220\tloss: 903.747599\n",
      "Step: 221\tloss: 898.364990\n",
      "Step: 222\tloss: 901.077695\n",
      "Step: 223\tloss: 903.561361\n",
      "Step: 224\tloss: 903.778968\n",
      "Step: 225\tloss: 898.351241\n",
      "Step: 226\tloss: 900.900172\n",
      "Step: 227\tloss: 904.205738\n",
      "Step: 228\tloss: 903.206278\n",
      "Step: 229\tloss: 897.969869\n",
      "Step: 230\tloss: 901.702285\n",
      "Step: 231\tloss: 903.397019\n",
      "Step: 232\tloss: 903.591645\n",
      "Step: 233\tloss: 898.404919\n",
      "Step: 234\tloss: 902.131440\n",
      "Step: 235\tloss: 902.995125\n",
      "Step: 236\tloss: 903.117315\n",
      "Step: 237\tloss: 898.629522\n",
      "Step: 238\tloss: 900.885008\n",
      "Step: 239\tloss: 903.945790\n",
      "Step: 240\tloss: 903.092292\n",
      "Step: 241\tloss: 898.364327\n",
      "Step: 242\tloss: 901.745042\n",
      "Step: 243\tloss: 903.520789\n",
      "Step: 244\tloss: 903.124646\n",
      "Step: 245\tloss: 898.545911\n",
      "Step: 246\tloss: 901.492031\n",
      "Step: 247\tloss: 903.633613\n",
      "Step: 248\tloss: 903.293886\n",
      "Step: 249\tloss: 898.516565\n",
      "Step: 250\tloss: 900.687327\n",
      "Step: 251\tloss: 904.104503\n",
      "Step: 252\tloss: 903.741743\n",
      "Step: 253\tloss: 897.818983\n",
      "Step: 254\tloss: 901.270548\n",
      "Step: 255\tloss: 903.680820\n",
      "Step: 256\tloss: 903.951816\n",
      "Step: 257\tloss: 898.677902\n",
      "Step: 258\tloss: 901.334668\n",
      "Step: 259\tloss: 903.440158\n",
      "Step: 260\tloss: 903.655545\n",
      "Step: 261\tloss: 898.509773\n",
      "Step: 262\tloss: 900.926678\n",
      "Step: 263\tloss: 903.221670\n",
      "Step: 264\tloss: 904.218205\n",
      "Step: 265\tloss: 898.122290\n",
      "Step: 266\tloss: 901.919983\n",
      "Step: 267\tloss: 903.163089\n",
      "Step: 268\tloss: 903.585291\n",
      "Step: 269\tloss: 897.646559\n",
      "Step: 270\tloss: 901.336054\n",
      "Step: 271\tloss: 904.470831\n",
      "Step: 272\tloss: 903.239262\n",
      "Step: 273\tloss: 898.195178\n",
      "Step: 274\tloss: 901.334402\n",
      "Step: 275\tloss: 903.791422\n",
      "Step: 276\tloss: 903.425472\n",
      "Step: 277\tloss: 897.468955\n",
      "Step: 278\tloss: 901.669515\n",
      "Step: 279\tloss: 903.280913\n",
      "Step: 280\tloss: 904.639286\n",
      "Step: 281\tloss: 897.808663\n",
      "Step: 282\tloss: 901.388827\n",
      "Step: 283\tloss: 903.798146\n",
      "Step: 284\tloss: 903.965221\n",
      "Step: 285\tloss: 898.300580\n",
      "Step: 286\tloss: 901.450627\n",
      "Step: 287\tloss: 903.906504\n",
      "Step: 288\tloss: 903.299416\n",
      "Step: 289\tloss: 897.584016\n",
      "Step: 290\tloss: 901.914328\n",
      "Step: 291\tloss: 903.305017\n",
      "Step: 292\tloss: 903.826382\n",
      "Step: 293\tloss: 898.271453\n",
      "Step: 294\tloss: 901.383955\n",
      "Step: 295\tloss: 903.791821\n",
      "Step: 296\tloss: 903.177857\n",
      "Step: 297\tloss: 898.405440\n",
      "Step: 298\tloss: 901.073533\n",
      "Step: 299\tloss: 903.624198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300\tloss: 903.479272\n",
      "pval: 0.600743\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.04\n",
      "Root mean squared deviation of scale:    0.10\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have the same scenario as in example 2 but one technical replicate is missing. We have to drop the corresponding constraint and remove the two parameters belonging to this pair of technical replicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 1500\n",
    "dmat = np.zeros([ncells, 9])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1 \n",
    "dmat[500:750,2] = 1 # bio rep 2 # 50%=1 tech_rep missing\n",
    "dmat[750:1250,3] = 1 # bio rep 3\n",
    "dmat[1250:1500,4] = 1 # bio rep 4 # 50%=1 tech_rep missing\n",
    "\n",
    "dmat[0:250,5] = 1 # tech rep 1 in bio rep 1\n",
    "dmat[750:1000,5] = 1 # tech rep 1 in bio rep 3\n",
    "dmat[250:500,6] = 1 # tech rep 2 in bio rep 1\n",
    "dmat[1000:1250,6] = 1 # tech rep 2 in bio rep 3\n",
    "# tech rep 3 is missing in bio rep 2,4\n",
    "dmat[500:750,7] = 1 # tech rep 4 in bio rep 2\n",
    "dmat[1250:1500,7] = 1 # tech rep 4 in bio rep 4\n",
    "\n",
    "dmat[1000:2000,8] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove coefficient for single technical replicate 4 from models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc[:,np.array([0,1,2,3,4,5,6,8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale[:,np.array([0,1,2,3,4,5,6,8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 1., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dmat_est_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0., -1.,  1.,  0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([3, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset. \n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "# Constraint 2: Account for fact that first level of technical replicates was not absorbed into offset. \n",
    "constraints_loc[2,5] = -1\n",
    "constraints_loc[2,6:7] = 1\n",
    "\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 2\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale,\n",
    "    constraints_loc=constraints_loc,\n",
    "    constraints_scale=constraints_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no closed form estimator for the mean model here due to the confounding. The model is initialised with least squares but the mean model is also trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: True\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 907.310813\n",
      "Step: 2\tloss: 917.416733\n",
      "Step: 3\tloss: 909.281433\n",
      "Step: 4\tloss: 908.050036\n",
      "Step: 5\tloss: 910.225890\n",
      "Step: 6\tloss: 909.918557\n",
      "Step: 7\tloss: 907.845170\n",
      "Step: 8\tloss: 907.651723\n",
      "Step: 9\tloss: 907.515244\n",
      "Step: 10\tloss: 905.854563\n",
      "Step: 11\tloss: 907.409702\n",
      "Step: 12\tloss: 906.256109\n",
      "Step: 13\tloss: 905.232032\n",
      "Step: 14\tloss: 905.509946\n",
      "Step: 15\tloss: 906.438927\n",
      "Step: 16\tloss: 904.972781\n",
      "Step: 17\tloss: 904.929343\n",
      "Step: 18\tloss: 905.555354\n",
      "Step: 19\tloss: 904.464431\n",
      "Step: 20\tloss: 904.745232\n",
      "Step: 21\tloss: 904.869783\n",
      "Step: 22\tloss: 903.304794\n",
      "Step: 23\tloss: 904.576924\n",
      "Step: 24\tloss: 905.193506\n",
      "Step: 25\tloss: 903.376691\n",
      "Step: 26\tloss: 904.364825\n",
      "Step: 27\tloss: 904.811288\n",
      "Step: 28\tloss: 903.099013\n",
      "Step: 29\tloss: 903.477355\n",
      "Step: 30\tloss: 905.131594\n",
      "Step: 31\tloss: 902.845137\n",
      "Step: 32\tloss: 905.107170\n",
      "Step: 33\tloss: 903.658515\n",
      "Step: 34\tloss: 903.589207\n",
      "Step: 35\tloss: 903.775358\n",
      "Step: 36\tloss: 904.043942\n",
      "Step: 37\tloss: 902.738422\n",
      "Step: 38\tloss: 904.033514\n",
      "Step: 39\tloss: 904.363085\n",
      "Step: 40\tloss: 902.700721\n",
      "Step: 41\tloss: 904.149492\n",
      "Step: 42\tloss: 904.283182\n",
      "Step: 43\tloss: 902.475746\n",
      "Step: 44\tloss: 903.870992\n",
      "Step: 45\tloss: 904.691448\n",
      "Step: 46\tloss: 902.260805\n",
      "Step: 47\tloss: 904.195855\n",
      "Step: 48\tloss: 904.549032\n",
      "Step: 49\tloss: 903.198033\n",
      "Step: 50\tloss: 903.618917\n",
      "Step: 51\tloss: 904.230982\n",
      "Step: 52\tloss: 902.986445\n",
      "Step: 53\tloss: 904.253313\n",
      "Step: 54\tloss: 903.624407\n",
      "Step: 55\tloss: 903.058541\n",
      "Step: 56\tloss: 903.614221\n",
      "Step: 57\tloss: 904.286192\n",
      "Step: 58\tloss: 903.066417\n",
      "Step: 59\tloss: 903.970457\n",
      "Step: 60\tloss: 903.782974\n",
      "Step: 61\tloss: 903.264468\n",
      "Step: 62\tloss: 903.643118\n",
      "Step: 63\tloss: 904.002001\n",
      "Step: 64\tloss: 902.371162\n",
      "Step: 65\tloss: 904.128310\n",
      "Step: 66\tloss: 904.351469\n",
      "Step: 67\tloss: 902.345435\n",
      "Step: 68\tloss: 904.256352\n",
      "Step: 69\tloss: 904.213486\n",
      "Step: 70\tloss: 903.657372\n",
      "Step: 71\tloss: 903.116994\n",
      "Step: 72\tloss: 904.146895\n",
      "Step: 73\tloss: 902.881237\n",
      "Step: 74\tloss: 903.570790\n",
      "Step: 75\tloss: 904.377882\n",
      "Step: 76\tloss: 903.014782\n",
      "Step: 77\tloss: 904.430290\n",
      "Step: 78\tloss: 903.406514\n",
      "Step: 79\tloss: 904.017793\n",
      "Step: 80\tloss: 903.342454\n",
      "Step: 81\tloss: 903.571146\n",
      "Step: 82\tloss: 902.484942\n",
      "Step: 83\tloss: 903.817752\n",
      "Step: 84\tloss: 904.674082\n",
      "Step: 85\tloss: 903.362389\n",
      "Step: 86\tloss: 902.958042\n",
      "Step: 87\tloss: 904.646604\n",
      "Step: 88\tloss: 902.837449\n",
      "Step: 89\tloss: 903.965431\n",
      "Step: 90\tloss: 904.234578\n",
      "Step: 91\tloss: 902.758714\n",
      "Step: 92\tloss: 904.616421\n",
      "Step: 93\tloss: 903.579943\n",
      "Step: 94\tloss: 902.585635\n",
      "Step: 95\tloss: 904.184386\n",
      "Step: 96\tloss: 904.227389\n",
      "Step: 97\tloss: 903.028543\n",
      "Step: 98\tloss: 903.627468\n",
      "Step: 99\tloss: 904.432146\n",
      "Step: 100\tloss: 903.514671\n",
      "Step: 101\tloss: 903.960804\n",
      "Step: 102\tloss: 903.679837\n",
      "Step: 103\tloss: 903.551033\n",
      "Step: 104\tloss: 903.681462\n",
      "Step: 105\tloss: 903.769915\n",
      "Step: 106\tloss: 903.114340\n",
      "Step: 107\tloss: 903.250069\n",
      "Step: 108\tloss: 904.714717\n",
      "Step: 109\tloss: 901.972424\n",
      "Step: 110\tloss: 904.853299\n",
      "Step: 111\tloss: 904.256054\n",
      "Step: 112\tloss: 903.395499\n",
      "Step: 113\tloss: 902.917914\n",
      "Step: 114\tloss: 904.678735\n",
      "Step: 115\tloss: 902.207758\n",
      "Step: 116\tloss: 904.368244\n",
      "Step: 117\tloss: 904.557871\n",
      "Step: 118\tloss: 902.297261\n",
      "Step: 119\tloss: 904.786091\n",
      "Step: 120\tloss: 903.988734\n",
      "Step: 121\tloss: 903.024189\n",
      "Step: 122\tloss: 904.050984\n",
      "Step: 123\tloss: 904.084782\n",
      "Step: 124\tloss: 902.436978\n",
      "Step: 125\tloss: 904.555976\n",
      "Step: 126\tloss: 904.144857\n",
      "Step: 127\tloss: 903.571433\n",
      "Step: 128\tloss: 903.618034\n",
      "Step: 129\tloss: 903.988293\n",
      "Step: 130\tloss: 903.081454\n",
      "Step: 131\tloss: 903.865214\n",
      "Step: 132\tloss: 904.301522\n",
      "Step: 133\tloss: 903.134652\n",
      "Step: 134\tloss: 904.658567\n",
      "Step: 135\tloss: 903.467252\n",
      "Step: 136\tloss: 901.949333\n",
      "Step: 137\tloss: 905.029638\n",
      "Step: 138\tloss: 904.108395\n",
      "Step: 139\tloss: 903.360236\n",
      "Step: 140\tloss: 903.800375\n",
      "Step: 141\tloss: 904.047437\n",
      "Step: 142\tloss: 902.981386\n",
      "Step: 143\tloss: 903.706842\n",
      "Step: 144\tloss: 904.572297\n",
      "Step: 145\tloss: 902.882207\n",
      "Step: 146\tloss: 904.011101\n",
      "Step: 147\tloss: 904.114460\n",
      "Step: 148\tloss: 903.335683\n",
      "Step: 149\tloss: 904.572120\n",
      "Step: 150\tloss: 903.184740\n",
      "Step: 151\tloss: 903.384676\n",
      "Step: 152\tloss: 904.232472\n",
      "Step: 153\tloss: 903.474471\n",
      "Step: 154\tloss: 902.980483\n",
      "Step: 155\tloss: 903.548447\n",
      "Step: 156\tloss: 904.725336\n",
      "Step: 157\tloss: 903.092109\n",
      "Step: 158\tloss: 904.474909\n",
      "Step: 159\tloss: 903.651283\n",
      "Step: 160\tloss: 902.474669\n",
      "Step: 161\tloss: 904.190027\n",
      "Step: 162\tloss: 904.684456\n",
      "Step: 163\tloss: 903.673164\n",
      "Step: 164\tloss: 903.570632\n",
      "Step: 165\tloss: 904.089789\n",
      "Step: 166\tloss: 903.221551\n",
      "Step: 167\tloss: 903.188076\n",
      "Step: 168\tloss: 904.855308\n",
      "Step: 169\tloss: 902.861789\n",
      "Step: 170\tloss: 903.892372\n",
      "Step: 171\tloss: 904.608068\n",
      "Step: 172\tloss: 904.321044\n",
      "Step: 173\tloss: 902.748431\n",
      "Step: 174\tloss: 904.116190\n",
      "Step: 175\tloss: 903.082337\n",
      "Step: 176\tloss: 904.640305\n",
      "Step: 177\tloss: 903.630315\n",
      "Step: 178\tloss: 903.577786\n",
      "Step: 179\tloss: 903.555380\n",
      "Step: 180\tloss: 904.170542\n",
      "Step: 181\tloss: 903.272254\n",
      "Step: 182\tloss: 904.018804\n",
      "Step: 183\tloss: 903.961439\n",
      "Step: 184\tloss: 902.500078\n",
      "Step: 185\tloss: 904.424671\n",
      "Step: 186\tloss: 904.205732\n",
      "Step: 187\tloss: 902.599453\n",
      "Step: 188\tloss: 904.047464\n",
      "Step: 189\tloss: 904.703899\n",
      "Step: 190\tloss: 902.169163\n",
      "Step: 191\tloss: 904.414579\n",
      "Step: 192\tloss: 904.695831\n",
      "Step: 193\tloss: 902.610208\n",
      "Step: 194\tloss: 904.130232\n",
      "Step: 195\tloss: 904.621558\n",
      "Step: 196\tloss: 902.404688\n",
      "Step: 197\tloss: 904.525121\n",
      "Step: 198\tloss: 904.366433\n",
      "Step: 199\tloss: 903.162283\n",
      "Step: 200\tloss: 903.497043\n",
      "pval: 0.000491\n",
      "Step: 201\tloss: 904.568932\n",
      "Step: 202\tloss: 903.440909\n",
      "Step: 203\tloss: 903.118272\n",
      "Step: 204\tloss: 904.658384\n",
      "Step: 205\tloss: 902.933459\n",
      "Step: 206\tloss: 903.684823\n",
      "Step: 207\tloss: 904.649296\n",
      "Step: 208\tloss: 902.967348\n",
      "Step: 209\tloss: 903.619914\n",
      "Step: 210\tloss: 904.611487\n",
      "Step: 211\tloss: 903.659341\n",
      "Step: 212\tloss: 904.114793\n",
      "Step: 213\tloss: 903.587092\n",
      "Step: 214\tloss: 903.304427\n",
      "Step: 215\tloss: 903.013907\n",
      "Step: 216\tloss: 905.130828\n",
      "Step: 217\tloss: 903.485393\n",
      "Step: 218\tloss: 903.558447\n",
      "Step: 219\tloss: 904.445612\n",
      "Step: 220\tloss: 902.566047\n",
      "Step: 221\tloss: 904.358622\n",
      "Step: 222\tloss: 904.386948\n",
      "Step: 223\tloss: 902.938210\n",
      "Step: 224\tloss: 904.172779\n",
      "Step: 225\tloss: 904.291440\n",
      "Step: 226\tloss: 902.919294\n",
      "Step: 227\tloss: 903.580399\n",
      "Step: 228\tloss: 904.689612\n",
      "Step: 229\tloss: 902.964214\n",
      "Step: 230\tloss: 904.169399\n",
      "Step: 231\tloss: 904.282880\n",
      "Step: 232\tloss: 902.984306\n",
      "Step: 233\tloss: 904.211977\n",
      "Step: 234\tloss: 904.044634\n",
      "Step: 235\tloss: 902.972416\n",
      "Step: 236\tloss: 904.542741\n",
      "Step: 237\tloss: 903.746210\n",
      "Step: 238\tloss: 903.106718\n",
      "Step: 239\tloss: 904.001400\n",
      "Step: 240\tloss: 904.150336\n",
      "Step: 241\tloss: 902.827390\n",
      "Step: 242\tloss: 904.407187\n",
      "Step: 243\tloss: 904.059822\n",
      "Step: 244\tloss: 903.073784\n",
      "Step: 245\tloss: 903.939933\n",
      "Step: 246\tloss: 904.231265\n",
      "Step: 247\tloss: 902.811266\n",
      "Step: 248\tloss: 904.712143\n",
      "Step: 249\tloss: 903.702976\n",
      "Step: 250\tloss: 903.291559\n",
      "Step: 251\tloss: 903.949463\n",
      "Step: 252\tloss: 903.858226\n",
      "Step: 253\tloss: 902.333307\n",
      "Step: 254\tloss: 903.715490\n",
      "Step: 255\tloss: 905.089365\n",
      "Step: 256\tloss: 902.839227\n",
      "Step: 257\tloss: 904.146451\n",
      "Step: 258\tloss: 904.232407\n",
      "Step: 259\tloss: 902.876796\n",
      "Step: 260\tloss: 903.763742\n",
      "Step: 261\tloss: 904.663800\n",
      "Step: 262\tloss: 903.044259\n",
      "Step: 263\tloss: 903.853325\n",
      "Step: 264\tloss: 904.452291\n",
      "Step: 265\tloss: 903.651150\n",
      "Step: 266\tloss: 903.265348\n",
      "Step: 267\tloss: 904.472479\n",
      "Step: 268\tloss: 903.075863\n",
      "Step: 269\tloss: 903.471910\n",
      "Step: 270\tloss: 904.759930\n",
      "Step: 271\tloss: 902.795889\n",
      "Step: 272\tloss: 904.349554\n",
      "Step: 273\tloss: 904.147825\n",
      "Step: 274\tloss: 902.946299\n",
      "Step: 275\tloss: 904.267931\n",
      "Step: 276\tloss: 904.089021\n",
      "Step: 277\tloss: 902.490592\n",
      "Step: 278\tloss: 904.306023\n",
      "Step: 279\tloss: 904.534184\n",
      "Step: 280\tloss: 903.045038\n",
      "Step: 281\tloss: 903.850539\n",
      "Step: 282\tloss: 904.491001\n",
      "Step: 283\tloss: 903.705557\n",
      "Step: 284\tloss: 903.242711\n",
      "Step: 285\tloss: 904.420300\n",
      "Step: 286\tloss: 902.593365\n",
      "Step: 287\tloss: 904.078383\n",
      "Step: 288\tloss: 904.559888\n",
      "Step: 289\tloss: 903.441476\n",
      "Step: 290\tloss: 904.596647\n",
      "Step: 291\tloss: 903.215867\n",
      "Step: 292\tloss: 904.107504\n",
      "Step: 293\tloss: 903.689819\n",
      "Step: 294\tloss: 903.583795\n",
      "Step: 295\tloss: 903.196749\n",
      "Step: 296\tloss: 904.808985\n",
      "Step: 297\tloss: 903.272153\n",
      "Step: 298\tloss: 902.721986\n",
      "Step: 299\tloss: 904.376466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300\tloss: 904.126106\n",
      "pval: 0.676358\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(2.220446e-16)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.11\n",
      "Root mean squared deviation of scale:    0.19\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
