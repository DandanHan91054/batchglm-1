{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import xarray as xa\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have 4 biological replicates (animals, patients, cell culture replicates etc.) in a treatment experiment: 2 in each condition (treated, untreated). Accordingly, there is perfect confounding at this level. We circumvent this by constraining the biological replicate coefficients to not model mean trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 2000\n",
    "dmat = np.zeros([ncells, 6])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1\n",
    "dmat[500:1000,2] = 1 # bio rep 2\n",
    "dmat[1000:1500,3] = 1 # bio rep 3\n",
    "dmat[1500:2000,4] = 1 # bio rep 4\n",
    "dmat[1000:2000,5] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[12903,  5849,  2604, ...,  7251, 18090,  5080],\n",
       "       [11761, 27807,  3453, ...,  4643,    17,  6729],\n",
       "       [11733, 14188,  4125, ...,  7424, 34598,  8651],\n",
       "       ...,\n",
       "       [40679, 10016, 18212, ...,  8702,  2372,  5319],\n",
       "       [22378, 12701, 11424, ..., 15495,  6123,  6795],\n",
       "       [12572, 11356, 14371, ..., 17543,  2710, 10276]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters used to generate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'a' (design_loc_params: 6, features: 100)>\n",
       "array([[ 9.115424,  8.908089,  8.774674, ...,  8.347528,  9.112151,  8.613704],\n",
       "       [ 0.225722,  0.574323, -0.406029, ...,  0.549426,  0.531398,  0.139959],\n",
       "       [ 0.109343, -0.060766,  0.674088, ..., -0.21946 ,  0.310577,  0.634328],\n",
       "       [ 0.09099 ,  0.252674,  0.591868, ...,  0.515935, -0.072529, -0.446625],\n",
       "       [ 0.385514,  0.531545,  0.475988, ...,  0.689591, -0.200923, -0.188396],\n",
       "       [ 0.635883, -0.027357,  0.455295, ...,  0.570734, -0.238338,  0.67039 ]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'b' (design_scale_params: 6, features: 100)>\n",
       "array([[ 2.079442,  1.94591 ,  1.94591 , ...,  2.197225,  0.693147,  2.197225],\n",
       "       [ 0.509976,  0.043864, -0.239517, ...,  0.331443, -0.535629,  0.68757 ],\n",
       "       [-0.173803,  0.553164, -0.214653, ..., -0.133374,  0.591388,  0.538071],\n",
       "       [ 0.366421,  0.431785,  0.573761, ...,  0.296076, -0.037804,  0.578232],\n",
       "       [ 0.571573,  0.610246,  0.020735, ..., -0.418196,  0.554592,  0.54537 ],\n",
       "       [-0.567676,  0.643797,  0.428963, ...,  0.588652,  0.241874,  0.157571]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "Dimensions without coordinates: features"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build constraints based on sets of parameters that have to sum to zero. Each of these constraints is enforced by binding one of these parameters to the rest of the set. Such a constraint is encoded by assigning a 1 to each parameter in the set and a -1 to to the dependent parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([2, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset.\n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 2\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale)\n",
    "input_data.constraints_loc = constraints_loc\n",
    "input_data.constraints_scale = constraints_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: False\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.01,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.25,\n",
      "  'use_batching': False}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 909.982321\n",
      "Step: 2\tloss: 909.973624\n",
      "Step: 3\tloss: 909.968350\n",
      "Step: 4\tloss: 909.965493\n",
      "Step: 5\tloss: 909.963875\n",
      "Step: 6\tloss: 909.962728\n",
      "Step: 7\tloss: 909.961821\n",
      "Step: 8\tloss: 909.961120\n",
      "Step: 9\tloss: 909.960557\n",
      "Step: 10\tloss: 909.960035\n",
      "Step: 11\tloss: 909.959539\n",
      "Step: 12\tloss: 909.959123\n",
      "Step: 13\tloss: 909.958834\n",
      "Step: 14\tloss: 909.958649\n",
      "Step: 15\tloss: 909.958509\n",
      "Step: 16\tloss: 909.958381\n",
      "Step: 17\tloss: 909.958267\n",
      "Step: 18\tloss: 909.958179\n",
      "Step: 19\tloss: 909.958110\n",
      "Step: 20\tloss: 909.958046\n",
      "pval: 0.003388\n",
      "Step: 21\tloss: 909.957982\n",
      "Step: 22\tloss: 909.957925\n",
      "Step: 23\tloss: 909.957883\n",
      "Step: 24\tloss: 909.957854\n",
      "Step: 25\tloss: 909.957831\n",
      "Step: 26\tloss: 909.957807\n",
      "Step: 27\tloss: 909.957778\n",
      "Step: 28\tloss: 909.957743\n",
      "Step: 29\tloss: 909.957701\n",
      "Step: 30\tloss: 909.957657\n",
      "pval: 0.000262\n",
      "Step: 31\tloss: 909.957616\n",
      "Step: 32\tloss: 909.957579\n",
      "Step: 33\tloss: 909.957540\n",
      "Step: 34\tloss: 909.957496\n",
      "Step: 35\tloss: 909.957449\n",
      "Step: 36\tloss: 909.957404\n",
      "Step: 37\tloss: 909.957365\n",
      "Step: 38\tloss: 909.957331\n",
      "Step: 39\tloss: 909.957304\n",
      "Step: 40\tloss: 909.957284\n",
      "pval: 0.000000\n",
      "Step: 41\tloss: 909.957270\n",
      "Step: 42\tloss: 909.957261\n",
      "Step: 43\tloss: 909.957256\n",
      "Step: 44\tloss: 909.957253\n",
      "Step: 45\tloss: 909.957251\n",
      "Step: 46\tloss: 909.957249\n",
      "Step: 47\tloss: 909.957246\n",
      "Step: 48\tloss: 909.957243\n",
      "Step: 49\tloss: 909.957241\n",
      "Step: 50\tloss: 909.957238\n",
      "pval: 0.000256\n",
      "Step: 51\tloss: 909.957236\n",
      "Step: 52\tloss: 909.957232\n",
      "Step: 53\tloss: 909.957229\n",
      "Step: 54\tloss: 909.957226\n",
      "Step: 55\tloss: 909.957221\n",
      "Step: 56\tloss: 909.957217\n",
      "Step: 57\tloss: 909.957212\n",
      "Step: 58\tloss: 909.957208\n",
      "Step: 59\tloss: 909.957204\n",
      "Step: 60\tloss: 909.957201\n",
      "pval: 0.000001\n",
      "Step: 61\tloss: 909.957198\n",
      "Step: 62\tloss: 909.957196\n",
      "Step: 63\tloss: 909.957193\n",
      "Step: 64\tloss: 909.957192\n",
      "Step: 65\tloss: 909.957190\n",
      "Step: 66\tloss: 909.957189\n",
      "Step: 67\tloss: 909.957189\n",
      "Step: 68\tloss: 909.957189\n",
      "Step: 69\tloss: 909.957189\n",
      "Step: 70\tloss: 909.957189\n",
      "pval: 0.000015\n",
      "Step: 71\tloss: 909.957189\n",
      "Step: 72\tloss: 909.957189\n",
      "Step: 73\tloss: 909.957188\n",
      "Step: 74\tloss: 909.957188\n",
      "Step: 75\tloss: 909.957188\n",
      "Step: 76\tloss: 909.957187\n",
      "Step: 77\tloss: 909.957186\n",
      "Step: 78\tloss: 909.957186\n",
      "Step: 79\tloss: 909.957185\n",
      "Step: 80\tloss: 909.957185\n",
      "pval: 0.000867\n",
      "Step: 81\tloss: 909.957184\n",
      "Step: 82\tloss: 909.957184\n",
      "Step: 83\tloss: 909.957184\n",
      "Step: 84\tloss: 909.957183\n",
      "Step: 85\tloss: 909.957183\n",
      "Step: 86\tloss: 909.957183\n",
      "Step: 87\tloss: 909.957183\n",
      "Step: 88\tloss: 909.957183\n",
      "Step: 89\tloss: 909.957183\n",
      "Step: 90\tloss: 909.957183\n",
      "pval: 0.000003\n",
      "Step: 91\tloss: 909.957183\n",
      "Step: 92\tloss: 909.957183\n",
      "Step: 93\tloss: 909.957183\n",
      "Step: 94\tloss: 909.957183\n",
      "Step: 95\tloss: 909.957183\n",
      "Step: 96\tloss: 909.957183\n",
      "Step: 97\tloss: 909.957183\n",
      "Step: 98\tloss: 909.957183\n",
      "Step: 99\tloss: 909.957183\n",
      "Step: 100\tloss: 909.957183\n",
      "pval: 0.000469\n",
      "Step: 101\tloss: 909.957183\n",
      "Step: 102\tloss: 909.957183\n",
      "Step: 103\tloss: 909.957183\n",
      "Step: 104\tloss: 909.957182\n",
      "Step: 105\tloss: 909.957182\n",
      "Step: 106\tloss: 909.957182\n",
      "Step: 107\tloss: 909.957182\n",
      "Step: 108\tloss: 909.957182\n",
      "Step: 109\tloss: 909.957182\n",
      "Step: 110\tloss: 909.957182\n",
      "pval: 0.000163\n",
      "Step: 111\tloss: 909.957182\n",
      "Step: 112\tloss: 909.957182\n",
      "Step: 113\tloss: 909.957182\n",
      "Step: 114\tloss: 909.957182\n",
      "Step: 115\tloss: 909.957182\n",
      "Step: 116\tloss: 909.957182\n",
      "Step: 117\tloss: 909.957182\n",
      "Step: 118\tloss: 909.957182\n",
      "Step: 119\tloss: 909.957182\n",
      "Step: 120\tloss: 909.957182\n",
      "pval: 0.000015\n",
      "Step: 121\tloss: 909.957182\n",
      "Step: 122\tloss: 909.957182\n",
      "Step: 123\tloss: 909.957182\n",
      "Step: 124\tloss: 909.957182\n",
      "Step: 125\tloss: 909.957182\n",
      "Step: 126\tloss: 909.957182\n",
      "Step: 127\tloss: 909.957182\n",
      "Step: 128\tloss: 909.957182\n",
      "Step: 129\tloss: 909.957182\n",
      "Step: 130\tloss: 909.957182\n",
      "pval: 0.000133\n",
      "Step: 131\tloss: 909.957182\n",
      "Step: 132\tloss: 909.957182\n",
      "Step: 133\tloss: 909.957182\n",
      "Step: 134\tloss: 909.957182\n",
      "Step: 135\tloss: 909.957182\n",
      "Step: 136\tloss: 909.957182\n",
      "Step: 137\tloss: 909.957182\n",
      "Step: 138\tloss: 909.957182\n",
      "Step: 139\tloss: 909.957182\n",
      "Step: 140\tloss: 909.957182\n",
      "pval: 0.000025\n",
      "Step: 141\tloss: 909.957182\n",
      "Step: 142\tloss: 909.957182\n",
      "Step: 143\tloss: 909.957182\n",
      "Step: 144\tloss: 909.957182\n",
      "Step: 145\tloss: 909.957182\n",
      "Step: 146\tloss: 909.957182\n",
      "Step: 147\tloss: 909.957182\n",
      "Step: 148\tloss: 909.957182\n",
      "Step: 149\tloss: 909.957182\n",
      "Step: 150\tloss: 909.957182\n",
      "pval: 0.000500\n",
      "Step: 151\tloss: 909.957182\n",
      "Step: 152\tloss: 909.957182\n",
      "Step: 153\tloss: 909.957182\n",
      "Step: 154\tloss: 909.957182\n",
      "Step: 155\tloss: 909.957182\n",
      "Step: 156\tloss: 909.957182\n",
      "Step: 157\tloss: 909.957182\n",
      "Step: 158\tloss: 909.957182\n",
      "Step: 159\tloss: 909.957182\n",
      "Step: 160\tloss: 909.957182\n",
      "pval: 0.000008\n",
      "Step: 161\tloss: 909.957182\n",
      "Step: 162\tloss: 909.957182\n",
      "Step: 163\tloss: 909.957182\n",
      "Step: 164\tloss: 909.957182\n",
      "Step: 165\tloss: 909.957182\n",
      "Step: 166\tloss: 909.957182\n",
      "Step: 167\tloss: 909.957182\n",
      "Step: 168\tloss: 909.957182\n",
      "Step: 169\tloss: 909.957182\n",
      "Step: 170\tloss: 909.957182\n",
      "pval: 0.000012\n",
      "Step: 171\tloss: 909.957182\n",
      "Step: 172\tloss: 909.957182\n",
      "Step: 173\tloss: 909.957182\n",
      "Step: 174\tloss: 909.957182\n",
      "Step: 175\tloss: 909.957182\n",
      "Step: 176\tloss: 909.957182\n",
      "Step: 177\tloss: 909.957182\n",
      "Step: 178\tloss: 909.957182\n",
      "Step: 179\tloss: 909.957182\n",
      "Step: 180\tloss: 909.957182\n",
      "pval: 0.000028\n",
      "Step: 181\tloss: 909.957182\n",
      "Step: 182\tloss: 909.957182\n",
      "Step: 183\tloss: 909.957182\n",
      "Step: 184\tloss: 909.957182\n",
      "Step: 185\tloss: 909.957182\n",
      "Step: 186\tloss: 909.957182\n",
      "Step: 187\tloss: 909.957182\n",
      "Step: 188\tloss: 909.957182\n",
      "Step: 189\tloss: 909.957182\n",
      "Step: 190\tloss: 909.957182\n",
      "pval: 0.000043\n",
      "Step: 191\tloss: 909.957182\n",
      "Step: 192\tloss: 909.957182\n",
      "Step: 193\tloss: 909.957182\n",
      "Step: 194\tloss: 909.957182\n",
      "Step: 195\tloss: 909.957182\n",
      "Step: 196\tloss: 909.957182\n",
      "Step: 197\tloss: 909.957182\n",
      "Step: 198\tloss: 909.957182\n",
      "Step: 199\tloss: 909.957182\n",
      "Step: 200\tloss: 909.957182\n",
      "pval: 0.000001\n",
      "Step: 201\tloss: 909.957182\n",
      "Step: 202\tloss: 909.957182\n",
      "Step: 203\tloss: 909.957182\n",
      "Step: 204\tloss: 909.957182\n",
      "Step: 205\tloss: 909.957182\n",
      "Step: 206\tloss: 909.957182\n",
      "Step: 207\tloss: 909.957182\n",
      "Step: 208\tloss: 909.957182\n",
      "Step: 209\tloss: 909.957182\n",
      "Step: 210\tloss: 909.957182\n",
      "pval: 0.232709\n",
      "Step: 211\tloss: 909.957182\n",
      "Step: 212\tloss: 909.957182\n",
      "Step: 213\tloss: 909.957182\n",
      "Step: 214\tloss: 909.957182\n",
      "Step: 215\tloss: 909.957182\n",
      "Step: 216\tloss: 909.957182\n",
      "Step: 217\tloss: 909.957182\n",
      "Step: 218\tloss: 909.957182\n",
      "Step: 219\tloss: 909.957182\n",
      "Step: 220\tloss: 909.957182\n",
      "pval: 0.017934\n",
      "Step: 221\tloss: 909.957182\n",
      "Step: 222\tloss: 909.957182\n",
      "Step: 223\tloss: 909.957182\n",
      "Step: 224\tloss: 909.957182\n",
      "Step: 225\tloss: 909.957182\n",
      "Step: 226\tloss: 909.957182\n",
      "Step: 227\tloss: 909.957182\n",
      "Step: 228\tloss: 909.957182\n",
      "Step: 229\tloss: 909.957182\n",
      "Step: 230\tloss: 909.957182\n",
      "pval: 0.897292\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted parameters can be retrieved by calling the corresponding parameters of `estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_loc_params: 6, features: 100)>\n",
       "array([[ 9.278052,  9.165176,  8.917734, ...,  8.526134,  9.486837,  9.003132],\n",
       "       [ 0.061182,  0.322709, -0.542116, ...,  0.393476,  0.106263, -0.243964],\n",
       "       [-0.061182, -0.322709,  0.542116, ..., -0.393476, -0.106263,  0.243964],\n",
       "       [-0.166838, -0.14927 ,  0.051688, ..., -0.081269,  0.066566, -0.127305],\n",
       "       [ 0.166838,  0.14927 , -0.051688, ...,  0.081269, -0.066566,  0.127305],\n",
       "       [ 0.698859,  0.102646,  0.83754 , ...,  0.983851, -0.75716 , -0.03016 ]])\n",
       "Coordinates:\n",
       "  * design_loc_params  (design_loc_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "    feature_allzero    (features) bool False False False False False False ...\n",
       "  * features           (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (design_scale_params: 6, features: 100)>\n",
       "array([[ 2.199805,  2.191249,  1.744115, ...,  2.309823,  0.703358,  2.806439],\n",
       "       [ 0.396867, -0.195051,  0.07298 , ...,  0.258389, -0.475605,  0.123256],\n",
       "       [-0.396867,  0.195051, -0.07298 , ..., -0.258389,  0.475605, -0.123256],\n",
       "       [-0.063983, -0.151285,  0.275507, ...,  0.382301, -0.267185, -0.075441],\n",
       "       [ 0.063983,  0.151285, -0.275507, ..., -0.382301,  0.267185,  0.075441],\n",
       "       [-0.27461 ,  0.906192,  0.847079, ...,  0.429621,  0.480646,  0.145633]])\n",
       "Coordinates:\n",
       "  * design_scale_params  (design_scale_params) <U2 'p0' 'p1' 'p2' 'p3' 'p4' 'p5'\n",
       "    feature_allzero      (features) bool False False False False False False ...\n",
       "  * features             (features) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.par_link_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(5.551115e-17)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(5.551115e-17)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.02\n",
      "Root mean squared deviation of scale:    0.06\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have 4 biological replicates (animals, patients, cell culture replicates etc.) in a treatment experiment: 2 in each condition (treated, untreated). Accordingly, there is perfect confounding at this level already. We circumvent this by constraining the biological replicate coefficients to not model mean trends (constraints 0,1). Secondly, there a are technical replicates which contain cells from one biological replicate from each condition. Each biological replicate was assigned to one treated-untreated sample pair and each pair split into two technical replicates. Again, we correct perfect confouding by constrainig the techincal replicate coefficients not to model mean effects by constraints 2,3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ncells = 2000\n",
    "dmat = np.zeros([ncells, 10])\n",
    "dmat[:,0] = 1\n",
    "dmat[:500,1] = 1 # bio rep 1\n",
    "dmat[500:1000,2] = 1 # bio rep 2\n",
    "dmat[1000:1500,3] = 1 # bio rep 3\n",
    "dmat[1500:2000,4] = 1 # bio rep 4\n",
    "dmat[0:250,5] = 1 # tech rep 1\n",
    "dmat[1000:1250,5] = 1 # tech rep 1\n",
    "dmat[250:500,6] = 1 # tech rep 2\n",
    "dmat[1250:1500,6] = 1 # tech rep 2\n",
    "dmat[500:750,7] = 1 # tech rep 3\n",
    "dmat[1500:1750,7] = 1 # tech rep 3\n",
    "dmat[750:1000,8] = 1 # tech rep 4\n",
    "dmat[1750:2000,8] = 1 # tech rep 4\n",
    "dmat[1000:2000,9] = 1 # condition effect\n",
    "print(np.unique(dmat, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.parse_dmat_loc(dmat = dmat)\n",
    "sim.parse_dmat_scale(dmat = dmat)\n",
    "sim.generate_params()\n",
    "sim.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'X' (observations: 2000, features: 100)>\n",
       "array([[ 5784,   164,  4191, ...,  6312,  1274, 10346],\n",
       "       [ 4503,   221,  3810, ...,  2482,  4323, 14221],\n",
       "       [ 3289,   705,  6224, ...,  7159,  2031,  6204],\n",
       "       ...,\n",
       "       [ 3396,  3531,  1152, ..., 15260,  4269,  2373],\n",
       "       [ 2603,  2649,   741, ...,  4346,   900,  2629],\n",
       "       [ 1385,  3452,   762, ...,  4697,  5049,  3041]])\n",
       "Dimensions without coordinates: observations, features"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_loc = sim.design_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_est_scale = sim.design_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build constraints based on sets of parameters that have to sum to zero. Each of these constraints is enforced by binding one of these parameters to the rest of the set. Such a constraint is encoded by assigning a 1 to each parameter in the set and a -1 to to the dependent parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dmat_est_loc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0., -1.,  1.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_loc = np.zeros([4, dmat_est_loc.shape[1]])\n",
    "# Constraint 0: Account for perfect confouding at biological replicate and treatment level \n",
    "# by constraining biological replicate coefficients not to produce mean effects across conditions.\n",
    "constraints_loc[0,3] = -1\n",
    "constraints_loc[0,4:5] = 1\n",
    "# Constraint 1: Account for fact that first level of biological replicates was not absorbed into offset. \n",
    "constraints_loc[1,1] = -1\n",
    "constraints_loc[1,2:5] = 1\n",
    "# Constraint 2: Account for fact that first level of technical replicates was not absorbed into offset. \n",
    "constraints_loc[2,5] = -1\n",
    "constraints_loc[2,6:9] = 1\n",
    "# Constraint 3: Account for perfect confouding at biological replicate and technical replicate \n",
    "# by constraining technical replicate coefficients not to produce mean effects across biological replicates.\n",
    "constraints_loc[3,7] = -1\n",
    "constraints_loc[3,8:9] = 1\n",
    "\n",
    "constraints_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0., -1.,  1.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_scale = constraints_loc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "rank deficiency without constraints: 4\n",
      "rank deficiency with constraints: 0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "constraints_loc_mod = constraints_loc.copy()\n",
    "constraints_loc_mod[constraints_loc_mod==-1] = 1\n",
    "print(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))\n",
    "print(\"rank deficiency without constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0)]))))\n",
    "print(\"rank deficiency with constraints: \"+ str(dmat_est_loc.shape[1] - matrix_rank(np.vstack([np.unique(dmat_est_loc, axis=0), constraints_loc_mod]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.X\n",
    "design_loc = dmat_est_loc\n",
    "design_scale = dmat_est_scale\n",
    "\n",
    "# input data\n",
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc,\n",
    "    design_scale=design_scale)\n",
    "input_data.constraints_loc = constraints_loc\n",
    "input_data.constraints_scale = constraints_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using closed-form MLE initialization for mean\n",
      "Should train mu: True\n",
      "Using closed-form MME initialization for dispersion\n",
      "Should train r: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/david.fischer/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(input_data, quick_scale=False)\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training sequence and let the estimator choose automatically the best training strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True},\n",
      " {'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.05,\n",
      "  'loss_window_size': 10,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': False}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 901.681688\n",
      "Step: 2\tloss: 913.664792\n",
      "Step: 3\tloss: 905.085027\n",
      "Step: 4\tloss: 903.485653\n",
      "Step: 5\tloss: 900.701931\n",
      "Step: 6\tloss: 905.045980\n",
      "Step: 7\tloss: 904.836149\n",
      "Step: 8\tloss: 902.916081\n",
      "Step: 9\tloss: 896.496285\n",
      "Step: 10\tloss: 900.040830\n",
      "Step: 11\tloss: 902.491017\n",
      "Step: 12\tloss: 901.849604\n",
      "Step: 13\tloss: 894.050507\n",
      "Step: 14\tloss: 899.251237\n",
      "Step: 15\tloss: 900.766051\n",
      "Step: 16\tloss: 899.668476\n",
      "Step: 17\tloss: 893.412154\n",
      "Step: 18\tloss: 896.617505\n",
      "Step: 19\tloss: 898.626475\n",
      "Step: 20\tloss: 899.040557\n",
      "Step: 21\tloss: 892.611549\n",
      "Step: 22\tloss: 896.447203\n",
      "Step: 23\tloss: 897.852252\n",
      "Step: 24\tloss: 897.913341\n",
      "Step: 25\tloss: 892.051056\n",
      "Step: 26\tloss: 895.596896\n",
      "Step: 27\tloss: 897.658355\n",
      "Step: 28\tloss: 897.310093\n",
      "Step: 29\tloss: 891.079524\n",
      "Step: 30\tloss: 895.236628\n",
      "Step: 31\tloss: 897.921111\n",
      "Step: 32\tloss: 897.321101\n",
      "Step: 33\tloss: 890.694723\n",
      "Step: 34\tloss: 895.757210\n",
      "Step: 35\tloss: 897.003707\n",
      "Step: 36\tloss: 897.040261\n",
      "Step: 37\tloss: 891.224615\n",
      "Step: 38\tloss: 895.059711\n",
      "Step: 39\tloss: 897.112263\n",
      "Step: 40\tloss: 896.226182\n",
      "Step: 41\tloss: 890.220423\n",
      "Step: 42\tloss: 895.404209\n",
      "Step: 43\tloss: 897.050249\n",
      "Step: 44\tloss: 896.686781\n",
      "Step: 45\tloss: 891.384560\n",
      "Step: 46\tloss: 894.475062\n",
      "Step: 47\tloss: 896.058558\n",
      "Step: 48\tloss: 897.444169\n",
      "Step: 49\tloss: 890.421108\n",
      "Step: 50\tloss: 895.297082\n",
      "Step: 51\tloss: 896.663044\n",
      "Step: 52\tloss: 896.798564\n",
      "Step: 53\tloss: 890.494366\n",
      "Step: 54\tloss: 894.972304\n",
      "Step: 55\tloss: 896.893577\n",
      "Step: 56\tloss: 896.985634\n",
      "Step: 57\tloss: 890.827966\n",
      "Step: 58\tloss: 895.684726\n",
      "Step: 59\tloss: 896.813945\n",
      "Step: 60\tloss: 896.198719\n",
      "Step: 61\tloss: 890.844430\n",
      "Step: 62\tloss: 894.869778\n",
      "Step: 63\tloss: 896.115514\n",
      "Step: 64\tloss: 897.147532\n",
      "Step: 65\tloss: 890.373730\n",
      "Step: 66\tloss: 894.932223\n",
      "Step: 67\tloss: 897.171496\n",
      "Step: 68\tloss: 896.589537\n",
      "Step: 69\tloss: 890.531769\n",
      "Step: 70\tloss: 895.056309\n",
      "Step: 71\tloss: 896.141788\n",
      "Step: 72\tloss: 897.252851\n",
      "Step: 73\tloss: 890.503208\n",
      "Step: 74\tloss: 894.705043\n",
      "Step: 75\tloss: 897.202232\n",
      "Step: 76\tloss: 896.805187\n",
      "Step: 77\tloss: 890.185350\n",
      "Step: 78\tloss: 895.564913\n",
      "Step: 79\tloss: 897.299600\n",
      "Step: 80\tloss: 896.411859\n",
      "Step: 81\tloss: 890.889900\n",
      "Step: 82\tloss: 895.042048\n",
      "Step: 83\tloss: 896.723560\n",
      "Step: 84\tloss: 896.799261\n",
      "Step: 85\tloss: 891.381923\n",
      "Step: 86\tloss: 894.897842\n",
      "Step: 87\tloss: 896.855306\n",
      "Step: 88\tloss: 896.209859\n",
      "Step: 89\tloss: 890.832834\n",
      "Step: 90\tloss: 894.742685\n",
      "Step: 91\tloss: 896.976990\n",
      "Step: 92\tloss: 896.684881\n",
      "Step: 93\tloss: 890.444264\n",
      "Step: 94\tloss: 895.545471\n",
      "Step: 95\tloss: 896.202339\n",
      "Step: 96\tloss: 897.284364\n",
      "Step: 97\tloss: 890.890088\n",
      "Step: 98\tloss: 895.157450\n",
      "Step: 99\tloss: 896.721796\n",
      "Step: 100\tloss: 896.846598\n",
      "Step: 101\tloss: 890.526288\n",
      "Step: 102\tloss: 895.322558\n",
      "Step: 103\tloss: 896.586345\n",
      "Step: 104\tloss: 897.149081\n",
      "Step: 105\tloss: 890.518772\n",
      "Step: 106\tloss: 895.186427\n",
      "Step: 107\tloss: 896.486877\n",
      "Step: 108\tloss: 897.460811\n",
      "Step: 109\tloss: 891.047989\n",
      "Step: 110\tloss: 895.124141\n",
      "Step: 111\tloss: 896.303912\n",
      "Step: 112\tloss: 897.290581\n",
      "Step: 113\tloss: 890.535177\n",
      "Step: 114\tloss: 895.268046\n",
      "Step: 115\tloss: 897.686254\n",
      "Step: 116\tloss: 896.320595\n",
      "Step: 117\tloss: 891.435754\n",
      "Step: 118\tloss: 894.864826\n",
      "Step: 119\tloss: 897.269381\n",
      "Step: 120\tloss: 896.495253\n",
      "Step: 121\tloss: 891.110661\n",
      "Step: 122\tloss: 894.714679\n",
      "Step: 123\tloss: 896.943206\n",
      "Step: 124\tloss: 897.271868\n",
      "Step: 125\tloss: 890.432680\n",
      "Step: 126\tloss: 894.523637\n",
      "Step: 127\tloss: 897.717010\n",
      "Step: 128\tloss: 897.475140\n",
      "Step: 129\tloss: 890.899827\n",
      "Step: 130\tloss: 894.984343\n",
      "Step: 131\tloss: 897.626455\n",
      "Step: 132\tloss: 896.346925\n",
      "Step: 133\tloss: 890.386823\n",
      "Step: 134\tloss: 894.888491\n",
      "Step: 135\tloss: 898.335553\n",
      "Step: 136\tloss: 896.458815\n",
      "Step: 137\tloss: 891.088965\n",
      "Step: 138\tloss: 894.854373\n",
      "Step: 139\tloss: 897.591418\n",
      "Step: 140\tloss: 896.733113\n",
      "Step: 141\tloss: 891.316260\n",
      "Step: 142\tloss: 894.382064\n",
      "Step: 143\tloss: 897.065009\n",
      "Step: 144\tloss: 897.298017\n",
      "Step: 145\tloss: 890.839021\n",
      "Step: 146\tloss: 895.238199\n",
      "Step: 147\tloss: 897.737421\n",
      "Step: 148\tloss: 896.411073\n",
      "Step: 149\tloss: 890.851913\n",
      "Step: 150\tloss: 894.470102\n",
      "Step: 151\tloss: 897.587451\n",
      "Step: 152\tloss: 897.381730\n",
      "Step: 153\tloss: 890.691286\n",
      "Step: 154\tloss: 895.456912\n",
      "Step: 155\tloss: 897.167838\n",
      "Step: 156\tloss: 897.113322\n",
      "Step: 157\tloss: 890.962106\n",
      "Step: 158\tloss: 895.341634\n",
      "Step: 159\tloss: 897.643510\n",
      "Step: 160\tloss: 896.900637\n",
      "Step: 161\tloss: 890.711297\n",
      "Step: 162\tloss: 895.499169\n",
      "Step: 163\tloss: 897.332015\n",
      "Step: 164\tloss: 897.148016\n",
      "Step: 165\tloss: 890.943622\n",
      "Step: 166\tloss: 894.799350\n",
      "Step: 167\tloss: 897.273983\n",
      "Step: 168\tloss: 897.652504\n",
      "Step: 169\tloss: 891.171823\n",
      "Step: 170\tloss: 895.268515\n",
      "Step: 171\tloss: 896.822081\n",
      "Step: 172\tloss: 897.481905\n",
      "Step: 173\tloss: 890.604864\n",
      "Step: 174\tloss: 894.615247\n",
      "Step: 175\tloss: 898.019083\n",
      "Step: 176\tloss: 897.542105\n",
      "Step: 177\tloss: 890.282248\n",
      "Step: 178\tloss: 896.509720\n",
      "Step: 179\tloss: 896.575051\n",
      "Step: 180\tloss: 897.187206\n",
      "Step: 181\tloss: 890.630918\n",
      "Step: 182\tloss: 895.340756\n",
      "Step: 183\tloss: 897.594345\n",
      "Step: 184\tloss: 897.065007\n",
      "Step: 185\tloss: 890.897071\n",
      "Step: 186\tloss: 896.103340\n",
      "Step: 187\tloss: 896.658777\n",
      "Step: 188\tloss: 896.619364\n",
      "Step: 189\tloss: 891.092310\n",
      "Step: 190\tloss: 895.451362\n",
      "Step: 191\tloss: 896.646471\n",
      "Step: 192\tloss: 897.262624\n",
      "Step: 193\tloss: 891.015724\n",
      "Step: 194\tloss: 895.001197\n",
      "Step: 195\tloss: 897.463243\n",
      "Step: 196\tloss: 897.216505\n",
      "Step: 197\tloss: 890.698725\n",
      "Step: 198\tloss: 895.337458\n",
      "Step: 199\tloss: 896.676478\n",
      "Step: 200\tloss: 898.002368\n",
      "pval: 0.008819\n",
      "Step: 201\tloss: 891.140804\n",
      "Step: 202\tloss: 894.718034\n",
      "Step: 203\tloss: 897.312588\n",
      "Step: 204\tloss: 897.888999\n",
      "Step: 205\tloss: 890.499849\n",
      "Step: 206\tloss: 895.107305\n",
      "Step: 207\tloss: 898.085721\n",
      "Step: 208\tloss: 897.513059\n",
      "Step: 209\tloss: 890.281929\n",
      "Step: 210\tloss: 896.103687\n",
      "Step: 211\tloss: 897.741225\n",
      "Step: 212\tloss: 896.776161\n",
      "Step: 213\tloss: 889.833601\n",
      "Step: 214\tloss: 895.794412\n",
      "Step: 215\tloss: 898.294033\n",
      "Step: 216\tloss: 897.138748\n",
      "Step: 217\tloss: 890.672009\n",
      "Step: 218\tloss: 896.324572\n",
      "Step: 219\tloss: 897.526969\n",
      "Step: 220\tloss: 896.416292\n",
      "Step: 221\tloss: 890.906192\n",
      "Step: 222\tloss: 895.678000\n",
      "Step: 223\tloss: 897.515236\n",
      "Step: 224\tloss: 896.716368\n",
      "Step: 225\tloss: 890.672167\n",
      "Step: 226\tloss: 895.463646\n",
      "Step: 227\tloss: 897.723193\n",
      "Step: 228\tloss: 896.928994\n",
      "Step: 229\tloss: 890.185699\n",
      "Step: 230\tloss: 895.465986\n",
      "Step: 231\tloss: 896.978397\n",
      "Step: 232\tloss: 898.208929\n",
      "Step: 233\tloss: 890.831651\n",
      "Step: 234\tloss: 894.536394\n",
      "Step: 235\tloss: 897.215507\n",
      "Step: 236\tloss: 898.254118\n",
      "Step: 237\tloss: 891.034026\n",
      "Step: 238\tloss: 894.867534\n",
      "Step: 239\tloss: 897.463076\n",
      "Step: 240\tloss: 897.678032\n",
      "Step: 241\tloss: 891.227231\n",
      "Step: 242\tloss: 894.733051\n",
      "Step: 243\tloss: 897.516180\n",
      "Step: 244\tloss: 897.663425\n",
      "Step: 245\tloss: 891.132010\n",
      "Step: 246\tloss: 895.458536\n",
      "Step: 247\tloss: 897.108237\n",
      "Step: 248\tloss: 897.586248\n",
      "Step: 249\tloss: 891.129965\n",
      "Step: 250\tloss: 895.381525\n",
      "Step: 251\tloss: 897.522491\n",
      "Step: 252\tloss: 897.268175\n",
      "Step: 253\tloss: 890.624411\n",
      "Step: 254\tloss: 894.962830\n",
      "Step: 255\tloss: 898.116640\n",
      "Step: 256\tloss: 897.581784\n",
      "Step: 257\tloss: 891.143604\n",
      "Step: 258\tloss: 895.260143\n",
      "Step: 259\tloss: 897.755401\n",
      "Step: 260\tloss: 897.222886\n",
      "Step: 261\tloss: 892.189905\n",
      "Step: 262\tloss: 895.633278\n",
      "Step: 263\tloss: 896.637766\n",
      "Step: 264\tloss: 896.524049\n",
      "Step: 265\tloss: 890.120549\n",
      "Step: 266\tloss: 895.587888\n",
      "Step: 267\tloss: 897.693325\n",
      "Step: 268\tloss: 897.334063\n",
      "Step: 269\tloss: 890.413020\n",
      "Step: 270\tloss: 895.697484\n",
      "Step: 271\tloss: 897.537428\n",
      "Step: 272\tloss: 897.495251\n",
      "Step: 273\tloss: 890.494442\n",
      "Step: 274\tloss: 895.732898\n",
      "Step: 275\tloss: 897.966162\n",
      "Step: 276\tloss: 896.697584\n",
      "Step: 277\tloss: 890.595216\n",
      "Step: 278\tloss: 895.419587\n",
      "Step: 279\tloss: 897.901065\n",
      "Step: 280\tloss: 896.853281\n",
      "Step: 281\tloss: 890.356059\n",
      "Step: 282\tloss: 896.203867\n",
      "Step: 283\tloss: 897.274411\n",
      "Step: 284\tloss: 897.158058\n",
      "Step: 285\tloss: 891.090780\n",
      "Step: 286\tloss: 895.277925\n",
      "Step: 287\tloss: 897.366001\n",
      "Step: 288\tloss: 897.103817\n",
      "Step: 289\tloss: 891.576019\n",
      "Step: 290\tloss: 893.952495\n",
      "Step: 291\tloss: 897.428834\n",
      "Step: 292\tloss: 897.875594\n",
      "Step: 293\tloss: 890.662284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 294\tloss: 895.587162\n",
      "Step: 295\tloss: 897.837425\n",
      "Step: 296\tloss: 896.752155\n",
      "Step: 297\tloss: 890.293085\n",
      "Step: 298\tloss: 895.965920\n",
      "Step: 299\tloss: 897.445376\n",
      "Step: 300\tloss: 897.462147\n",
      "pval: 0.679089\n",
      "Training sequence #1 complete\n",
      "Beginning with training sequence #2\n",
      "Step: 301\tloss: 894.823153\n",
      "Step: 302\tloss: 899.268276\n",
      "Step: 303\tloss: 895.076122\n",
      "Step: 304\tloss: 895.257103\n",
      "Step: 305\tloss: 896.624632\n",
      "Step: 306\tloss: 896.308158\n",
      "Step: 307\tloss: 895.225080\n",
      "Step: 308\tloss: 894.690255\n",
      "Step: 309\tloss: 894.962803\n",
      "Step: 310\tloss: 895.325616\n",
      "Step: 311\tloss: 895.216138\n",
      "Step: 312\tloss: 894.820048\n",
      "Step: 313\tloss: 894.561062\n",
      "Step: 314\tloss: 894.568433\n",
      "Step: 315\tloss: 894.659313\n",
      "Step: 316\tloss: 894.648754\n",
      "Step: 317\tloss: 894.539861\n",
      "Step: 318\tloss: 894.442858\n",
      "Step: 319\tloss: 894.414304\n",
      "Step: 320\tloss: 894.422377\n",
      "pval: 0.012077\n",
      "Step: 321\tloss: 894.418280\n",
      "Step: 322\tloss: 894.385701\n",
      "Step: 323\tloss: 894.338637\n",
      "Step: 324\tloss: 894.303665\n",
      "Step: 325\tloss: 894.291808\n",
      "Step: 326\tloss: 894.287617\n",
      "Step: 327\tloss: 894.275970\n",
      "Step: 328\tloss: 894.259057\n",
      "Step: 329\tloss: 894.244425\n",
      "Step: 330\tloss: 894.234446\n",
      "pval: 0.000682\n",
      "Step: 331\tloss: 894.225676\n",
      "Step: 332\tloss: 894.214419\n",
      "Step: 333\tloss: 894.202842\n",
      "Step: 334\tloss: 894.195692\n",
      "Step: 335\tloss: 894.192354\n",
      "Step: 336\tloss: 894.186747\n",
      "Step: 337\tloss: 894.176968\n",
      "Step: 338\tloss: 894.169414\n",
      "Step: 339\tloss: 894.168630\n",
      "Step: 340\tloss: 894.169184\n",
      "pval: 0.000046\n",
      "Step: 341\tloss: 894.164234\n",
      "Step: 342\tloss: 894.156008\n",
      "Step: 343\tloss: 894.151404\n",
      "Step: 344\tloss: 894.151610\n",
      "Step: 345\tloss: 894.151574\n",
      "Step: 346\tloss: 894.147970\n",
      "Step: 347\tloss: 894.143330\n",
      "Step: 348\tloss: 894.141330\n",
      "Step: 349\tloss: 894.141603\n",
      "Step: 350\tloss: 894.141236\n",
      "pval: 0.000019\n",
      "Step: 351\tloss: 894.139032\n",
      "Step: 352\tloss: 894.136448\n",
      "Step: 353\tloss: 894.135175\n",
      "Step: 354\tloss: 894.135080\n",
      "Step: 355\tloss: 894.134580\n",
      "Step: 356\tloss: 894.133028\n",
      "Step: 357\tloss: 894.131722\n",
      "Step: 358\tloss: 894.131604\n",
      "Step: 359\tloss: 894.131606\n",
      "Step: 360\tloss: 894.130616\n",
      "pval: 0.000025\n",
      "Step: 361\tloss: 894.129427\n",
      "Step: 362\tloss: 894.129187\n",
      "Step: 363\tloss: 894.129359\n",
      "Step: 364\tloss: 894.128872\n",
      "Step: 365\tloss: 894.128088\n",
      "Step: 366\tloss: 894.127875\n",
      "Step: 367\tloss: 894.127934\n",
      "Step: 368\tloss: 894.127569\n",
      "Step: 369\tloss: 894.127016\n",
      "Step: 370\tloss: 894.126865\n",
      "pval: 0.000015\n",
      "Step: 371\tloss: 894.126965\n",
      "Step: 372\tloss: 894.126802\n",
      "Step: 373\tloss: 894.126413\n",
      "Step: 374\tloss: 894.126239\n",
      "Step: 375\tloss: 894.126305\n",
      "Step: 376\tloss: 894.126234\n",
      "Step: 377\tloss: 894.125970\n",
      "Step: 378\tloss: 894.125832\n",
      "Step: 379\tloss: 894.125873\n",
      "Step: 380\tloss: 894.125829\n",
      "pval: 0.000016\n",
      "Step: 381\tloss: 894.125667\n",
      "Step: 382\tloss: 894.125592\n",
      "Step: 383\tloss: 894.125608\n",
      "Step: 384\tloss: 894.125553\n",
      "Step: 385\tloss: 894.125455\n",
      "Step: 386\tloss: 894.125429\n",
      "Step: 387\tloss: 894.125430\n",
      "Step: 388\tloss: 894.125372\n",
      "Step: 389\tloss: 894.125306\n",
      "Step: 390\tloss: 894.125305\n",
      "pval: 0.000036\n",
      "Step: 391\tloss: 894.125315\n",
      "Step: 392\tloss: 894.125268\n",
      "Step: 393\tloss: 894.125218\n",
      "Step: 394\tloss: 894.125226\n",
      "Step: 395\tloss: 894.125232\n",
      "Step: 396\tloss: 894.125186\n",
      "Step: 397\tloss: 894.125156\n",
      "Step: 398\tloss: 894.125174\n",
      "Step: 399\tloss: 894.125174\n",
      "Step: 400\tloss: 894.125139\n",
      "pval: 0.000018\n",
      "Step: 401\tloss: 894.125125\n",
      "Step: 402\tloss: 894.125135\n",
      "Step: 403\tloss: 894.125130\n",
      "Step: 404\tloss: 894.125112\n",
      "Step: 405\tloss: 894.125103\n",
      "Step: 406\tloss: 894.125103\n",
      "Step: 407\tloss: 894.125102\n",
      "Step: 408\tloss: 894.125095\n",
      "Step: 409\tloss: 894.125088\n",
      "Step: 410\tloss: 894.125086\n",
      "pval: 0.000053\n",
      "Step: 411\tloss: 894.125086\n",
      "Step: 412\tloss: 894.125080\n",
      "Step: 413\tloss: 894.125076\n",
      "Step: 414\tloss: 894.125075\n",
      "Step: 415\tloss: 894.125074\n",
      "Step: 416\tloss: 894.125071\n",
      "Step: 417\tloss: 894.125069\n",
      "Step: 418\tloss: 894.125068\n",
      "Step: 419\tloss: 894.125067\n",
      "Step: 420\tloss: 894.125065\n",
      "pval: 0.000023\n",
      "Step: 421\tloss: 894.125064\n",
      "Step: 422\tloss: 894.125063\n",
      "Step: 423\tloss: 894.125062\n",
      "Step: 424\tloss: 894.125061\n",
      "Step: 425\tloss: 894.125060\n",
      "Step: 426\tloss: 894.125060\n",
      "Step: 427\tloss: 894.125059\n",
      "Step: 428\tloss: 894.125058\n",
      "Step: 429\tloss: 894.125058\n",
      "Step: 430\tloss: 894.125058\n",
      "pval: 0.000032\n",
      "Step: 431\tloss: 894.125057\n",
      "Step: 432\tloss: 894.125057\n",
      "Step: 433\tloss: 894.125056\n",
      "Step: 434\tloss: 894.125056\n",
      "Step: 435\tloss: 894.125056\n",
      "Step: 436\tloss: 894.125056\n",
      "Step: 437\tloss: 894.125055\n",
      "Step: 438\tloss: 894.125055\n",
      "Step: 439\tloss: 894.125055\n",
      "Step: 440\tloss: 894.125055\n",
      "pval: 0.000019\n",
      "Step: 441\tloss: 894.125055\n",
      "Step: 442\tloss: 894.125055\n",
      "Step: 443\tloss: 894.125054\n",
      "Step: 444\tloss: 894.125054\n",
      "Step: 445\tloss: 894.125054\n",
      "Step: 446\tloss: 894.125054\n",
      "Step: 447\tloss: 894.125054\n",
      "Step: 448\tloss: 894.125054\n",
      "Step: 449\tloss: 894.125054\n",
      "Step: 450\tloss: 894.125054\n",
      "pval: 0.000017\n",
      "Step: 451\tloss: 894.125054\n",
      "Step: 452\tloss: 894.125054\n",
      "Step: 453\tloss: 894.125054\n",
      "Step: 454\tloss: 894.125054\n",
      "Step: 455\tloss: 894.125054\n",
      "Step: 456\tloss: 894.125054\n",
      "Step: 457\tloss: 894.125054\n",
      "Step: 458\tloss: 894.125053\n",
      "Step: 459\tloss: 894.125053\n",
      "Step: 460\tloss: 894.125053\n",
      "pval: 0.000021\n",
      "Step: 461\tloss: 894.125053\n",
      "Step: 462\tloss: 894.125053\n",
      "Step: 463\tloss: 894.125053\n",
      "Step: 464\tloss: 894.125053\n",
      "Step: 465\tloss: 894.125053\n",
      "Step: 466\tloss: 894.125053\n",
      "Step: 467\tloss: 894.125053\n",
      "Step: 468\tloss: 894.125053\n",
      "Step: 469\tloss: 894.125053\n",
      "Step: 470\tloss: 894.125054\n",
      "pval: 0.000151\n",
      "Step: 471\tloss: 894.125054\n",
      "Step: 472\tloss: 894.125055\n",
      "Step: 473\tloss: 894.125058\n",
      "Step: 474\tloss: 894.125065\n",
      "Step: 475\tloss: 894.125083\n",
      "Step: 476\tloss: 894.125120\n",
      "Step: 477\tloss: 894.125166\n",
      "Step: 478\tloss: 894.125156\n",
      "Step: 479\tloss: 894.125076\n",
      "Step: 480\tloss: 894.125062\n",
      "pval: 0.990061\n",
      "Training sequence #2 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that constraints were met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter sets should sum to zero for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)\n",
       "Coordinates:\n",
       "    design_loc_params  <U2 'p1'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(estimator.par_link_loc[1,:]+np.sum(estimator.par_link_loc[2:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array(1.110223e-16)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(estimator.par_link_loc[1:3,:], axis=0)+np.sum(estimator.par_link_loc[3:5,:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.07\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
