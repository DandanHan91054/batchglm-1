{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=1000)\n",
    "sim.generate()\n",
    "X = sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superimpose library size effects on counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_factors = numpy.random.normal(loc=1, scale=0.5, size=X.shape[0]) # draw random factors\n",
    "size_factors[size_factors < 0.2] = 0.2 # threshold\n",
    "X = np.round(X*np.repeat(np.expand_dims(size_factors, axis=1), axis=1, repeats=X.shape[1])) # scale counts and round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size factor scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.015730194737318067"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(X, axis=1).values/np.mean(np.sum(X, axis=1)).values - size_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add continuous covariate to desing_loc so that location model cannot be perfectly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_loc = sim.design_loc\n",
    "design_loc = np.hstack([design_loc.values, np.expand_dims(numpy.random.normal(loc=1, scale=1, size=design_loc.shape[0]), axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=size_factors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(\n",
    "    input_data, \n",
    "    init_a='standard', \n",
    "    init_b='standard',\n",
    "    batch_size=500\n",
    ")\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 9254.189149\n",
      "Step: 2\tloss: 9292.989550\n",
      "Step: 3\tloss: 9167.053270\n",
      "Step: 4\tloss: 9052.905853\n",
      "Step: 5\tloss: 8967.553832\n",
      "Step: 6\tloss: 8959.901128\n",
      "Step: 7\tloss: 8874.794233\n",
      "Step: 8\tloss: 8854.083242\n",
      "Step: 9\tloss: 8852.239760\n",
      "Step: 10\tloss: 8777.278185\n",
      "Step: 11\tloss: 8845.530370\n",
      "Step: 12\tloss: 8820.528019\n",
      "Step: 13\tloss: 8818.555588\n",
      "Step: 14\tloss: 8812.567318\n",
      "Step: 15\tloss: 8762.291693\n",
      "Step: 16\tloss: 8797.929816\n",
      "Step: 17\tloss: 8840.081402\n",
      "Step: 18\tloss: 8745.417729\n",
      "Step: 19\tloss: 8792.725467\n",
      "Step: 20\tloss: 8736.065210\n",
      "Step: 21\tloss: 8732.579664\n",
      "Step: 22\tloss: 8785.134898\n",
      "Step: 23\tloss: 8720.633028\n",
      "Step: 24\tloss: 8782.008826\n",
      "Step: 25\tloss: 8752.383262\n",
      "Step: 26\tloss: 8733.759706\n",
      "Step: 27\tloss: 8751.284042\n",
      "Step: 28\tloss: 8746.472003\n",
      "Step: 29\tloss: 8768.293886\n",
      "Step: 30\tloss: 8732.775239\n",
      "Step: 31\tloss: 8738.495989\n",
      "Step: 32\tloss: 8711.262123\n",
      "Step: 33\tloss: 8718.596630\n",
      "Step: 34\tloss: 8750.232652\n",
      "Step: 35\tloss: 8720.690091\n",
      "Step: 36\tloss: 8725.805557\n",
      "Step: 37\tloss: 8697.602008\n",
      "Step: 38\tloss: 8756.369849\n",
      "Step: 39\tloss: 8724.338734\n",
      "Step: 40\tloss: 8711.509786\n",
      "Step: 41\tloss: 8696.868554\n",
      "Step: 42\tloss: 8726.091721\n",
      "Step: 43\tloss: 8707.077530\n",
      "Step: 44\tloss: 8746.975852\n",
      "Step: 45\tloss: 8691.774777\n",
      "Step: 46\tloss: 8702.250122\n",
      "Step: 47\tloss: 8704.278937\n",
      "Step: 48\tloss: 8768.973257\n",
      "Step: 49\tloss: 8705.703990\n",
      "Step: 50\tloss: 8749.307343\n",
      "Step: 51\tloss: 8728.526871\n",
      "Step: 52\tloss: 8673.591843\n",
      "Step: 53\tloss: 8714.567012\n",
      "Step: 54\tloss: 8698.056579\n",
      "Step: 55\tloss: 8691.914615\n",
      "Step: 56\tloss: 8744.965709\n",
      "Step: 57\tloss: 8677.514891\n",
      "Step: 58\tloss: 8696.833084\n",
      "Step: 59\tloss: 8717.773210\n",
      "Step: 60\tloss: 8751.660094\n",
      "Step: 61\tloss: 8694.411296\n",
      "Step: 62\tloss: 8702.855706\n",
      "Step: 63\tloss: 8718.987185\n",
      "Step: 64\tloss: 8723.703392\n",
      "Step: 65\tloss: 8721.458360\n",
      "Step: 66\tloss: 8718.794734\n",
      "Step: 67\tloss: 8707.982234\n",
      "Step: 68\tloss: 8688.733475\n",
      "Step: 69\tloss: 8687.133019\n",
      "Step: 70\tloss: 8708.568051\n",
      "Step: 71\tloss: 8729.111199\n",
      "Step: 72\tloss: 8711.125034\n",
      "Step: 73\tloss: 8721.669259\n",
      "Step: 74\tloss: 8701.567774\n",
      "Step: 75\tloss: 8741.328068\n",
      "Step: 76\tloss: 8671.131235\n",
      "Step: 77\tloss: 8687.251747\n",
      "Step: 78\tloss: 8703.769863\n",
      "Step: 79\tloss: 8687.602253\n",
      "Step: 80\tloss: 8755.856556\n",
      "Step: 81\tloss: 8692.067604\n",
      "Step: 82\tloss: 8726.036195\n",
      "Step: 83\tloss: 8700.350908\n",
      "Step: 84\tloss: 8716.205624\n",
      "Step: 85\tloss: 8720.951390\n",
      "Step: 86\tloss: 8691.135904\n",
      "Step: 87\tloss: 8718.452548\n",
      "Step: 88\tloss: 8704.160581\n",
      "Step: 89\tloss: 8700.648437\n",
      "Step: 90\tloss: 8704.742954\n",
      "Step: 91\tloss: 8709.849688\n",
      "Step: 92\tloss: 8719.971579\n",
      "Step: 93\tloss: 8735.953916\n",
      "Step: 94\tloss: 8661.252025\n",
      "Step: 95\tloss: 8739.875952\n",
      "Step: 96\tloss: 8697.984315\n",
      "Step: 97\tloss: 8719.201365\n",
      "Step: 98\tloss: 8709.380685\n",
      "Step: 99\tloss: 8683.280826\n",
      "Step: 100\tloss: 8723.017198\n",
      "Step: 101\tloss: 8705.406950\n",
      "Step: 102\tloss: 8708.978014\n",
      "Step: 103\tloss: 8728.206108\n",
      "Step: 104\tloss: 8692.376771\n",
      "Step: 105\tloss: 8701.022993\n",
      "Step: 106\tloss: 8708.284090\n",
      "Step: 107\tloss: 8735.563580\n",
      "Step: 108\tloss: 8689.583339\n",
      "Step: 109\tloss: 8673.334454\n",
      "Step: 110\tloss: 8733.352845\n",
      "Step: 111\tloss: 8730.591527\n",
      "Step: 112\tloss: 8697.091024\n",
      "Step: 113\tloss: 8712.429564\n",
      "Step: 114\tloss: 8714.569668\n",
      "Step: 115\tloss: 8695.609474\n",
      "Step: 116\tloss: 8712.788833\n",
      "Step: 117\tloss: 8671.374446\n",
      "Step: 118\tloss: 8727.056995\n",
      "Step: 119\tloss: 8711.105353\n",
      "Step: 120\tloss: 8725.749394\n",
      "Step: 121\tloss: 8673.217268\n",
      "Step: 122\tloss: 8720.680536\n",
      "Step: 123\tloss: 8739.505368\n",
      "Step: 124\tloss: 8703.546142\n",
      "Step: 125\tloss: 8724.532253\n",
      "Step: 126\tloss: 8721.425258\n",
      "Step: 127\tloss: 8705.744094\n",
      "Step: 128\tloss: 8685.305656\n",
      "Step: 129\tloss: 8714.309687\n",
      "Step: 130\tloss: 8681.213330\n",
      "Step: 131\tloss: 8751.012737\n",
      "Step: 132\tloss: 8691.188515\n",
      "Step: 133\tloss: 8711.915851\n",
      "Step: 134\tloss: 8703.695404\n",
      "Step: 135\tloss: 8708.356603\n",
      "Step: 136\tloss: 8713.536034\n",
      "Step: 137\tloss: 8685.083519\n",
      "Step: 138\tloss: 8750.533292\n",
      "Step: 139\tloss: 8683.365248\n",
      "Step: 140\tloss: 8720.913187\n",
      "Step: 141\tloss: 8700.044337\n",
      "Step: 142\tloss: 8700.313525\n",
      "Step: 143\tloss: 8739.467785\n",
      "Step: 144\tloss: 8699.843293\n",
      "Step: 145\tloss: 8696.226426\n",
      "Step: 146\tloss: 8718.272822\n",
      "Step: 147\tloss: 8726.053751\n",
      "Step: 148\tloss: 8699.548153\n",
      "Step: 149\tloss: 8696.187996\n",
      "Step: 150\tloss: 8686.717598\n",
      "Step: 151\tloss: 8721.465723\n",
      "Step: 152\tloss: 8735.173078\n",
      "Step: 153\tloss: 8674.054611\n",
      "Step: 154\tloss: 8686.321949\n",
      "Step: 155\tloss: 8755.703843\n",
      "Step: 156\tloss: 8723.937795\n",
      "Step: 157\tloss: 8727.913080\n",
      "Step: 158\tloss: 8670.493131\n",
      "Step: 159\tloss: 8692.857722\n",
      "Step: 160\tloss: 8748.256494\n",
      "Step: 161\tloss: 8729.490542\n",
      "Step: 162\tloss: 8703.285175\n",
      "Step: 163\tloss: 8705.483628\n",
      "Step: 164\tloss: 8701.490747\n",
      "Step: 165\tloss: 8701.034683\n",
      "Step: 166\tloss: 8711.469646\n",
      "Step: 167\tloss: 8711.543862\n",
      "Step: 168\tloss: 8716.210028\n",
      "Step: 169\tloss: 8727.232491\n",
      "Step: 170\tloss: 8715.929582\n",
      "Step: 171\tloss: 8690.930875\n",
      "Step: 172\tloss: 8706.176274\n",
      "Step: 173\tloss: 8718.578066\n",
      "Step: 174\tloss: 8685.527212\n",
      "Step: 175\tloss: 8710.335207\n",
      "Step: 176\tloss: 8724.763299\n",
      "Step: 177\tloss: 8736.050609\n",
      "Step: 178\tloss: 8717.206535\n",
      "Step: 179\tloss: 8692.046372\n",
      "Step: 180\tloss: 8694.133108\n",
      "Step: 181\tloss: 8708.821952\n",
      "Step: 182\tloss: 8694.667809\n",
      "Step: 183\tloss: 8728.983431\n",
      "Step: 184\tloss: 8707.301618\n",
      "Step: 185\tloss: 8714.721886\n",
      "Step: 186\tloss: 8709.911586\n",
      "Step: 187\tloss: 8687.364088\n",
      "Step: 188\tloss: 8727.870449\n",
      "Step: 189\tloss: 8686.778613\n",
      "Step: 190\tloss: 8704.707104\n",
      "Step: 191\tloss: 8734.603928\n",
      "Step: 192\tloss: 8713.774840\n",
      "Step: 193\tloss: 8700.875916\n",
      "Step: 194\tloss: 8726.656728\n",
      "Step: 195\tloss: 8696.141088\n",
      "Step: 196\tloss: 8717.558464\n",
      "Step: 197\tloss: 8679.738512\n",
      "Step: 198\tloss: 8735.985949\n",
      "Step: 199\tloss: 8685.484825\n",
      "Step: 200\tloss: 8740.252733\n",
      "pval: 0.000031\n",
      "Step: 201\tloss: 8685.933583\n",
      "Step: 202\tloss: 8729.535126\n",
      "Step: 203\tloss: 8716.660333\n",
      "Step: 204\tloss: 8709.160144\n",
      "Step: 205\tloss: 8721.967901\n",
      "Step: 206\tloss: 8707.034561\n",
      "Step: 207\tloss: 8688.717652\n",
      "Step: 208\tloss: 8725.210725\n",
      "Step: 209\tloss: 8649.036828\n",
      "Step: 210\tloss: 8725.554716\n",
      "Step: 211\tloss: 8740.981480\n",
      "Step: 212\tloss: 8727.970259\n",
      "Step: 213\tloss: 8716.467428\n",
      "Step: 214\tloss: 8668.447726\n",
      "Step: 215\tloss: 8708.943614\n",
      "Step: 216\tloss: 8749.165414\n",
      "Step: 217\tloss: 8714.879846\n",
      "Step: 218\tloss: 8688.998716\n",
      "Step: 219\tloss: 8743.809467\n",
      "Step: 220\tloss: 8697.318499\n",
      "Step: 221\tloss: 8689.751437\n",
      "Step: 222\tloss: 8707.829022\n",
      "Step: 223\tloss: 8720.223119\n",
      "Step: 224\tloss: 8726.162551\n",
      "Step: 225\tloss: 8707.945048\n",
      "Step: 226\tloss: 8689.661112\n",
      "Step: 227\tloss: 8733.562289\n",
      "Step: 228\tloss: 8712.222466\n",
      "Step: 229\tloss: 8718.822459\n",
      "Step: 230\tloss: 8738.846886\n",
      "Step: 231\tloss: 8708.654425\n",
      "Step: 232\tloss: 8677.277692\n",
      "Step: 233\tloss: 8681.372569\n",
      "Step: 234\tloss: 8718.712305\n",
      "Step: 235\tloss: 8720.429562\n",
      "Step: 236\tloss: 8722.866405\n",
      "Step: 237\tloss: 8714.007758\n",
      "Step: 238\tloss: 8772.570201\n",
      "Step: 239\tloss: 8685.844356\n",
      "Step: 240\tloss: 8671.801557\n",
      "Step: 241\tloss: 8694.357841\n",
      "Step: 242\tloss: 8713.257852\n",
      "Step: 243\tloss: 8744.839883\n",
      "Step: 244\tloss: 8692.150932\n",
      "Step: 245\tloss: 8688.224909\n",
      "Step: 246\tloss: 8719.156757\n",
      "Step: 247\tloss: 8708.698208\n",
      "Step: 248\tloss: 8729.164849\n",
      "Step: 249\tloss: 8705.081043\n",
      "Step: 250\tloss: 8746.446832\n",
      "Step: 251\tloss: 8664.563914\n",
      "Step: 252\tloss: 8727.831423\n",
      "Step: 253\tloss: 8708.360948\n",
      "Step: 254\tloss: 8733.558449\n",
      "Step: 255\tloss: 8705.277011\n",
      "Step: 256\tloss: 8696.861381\n",
      "Step: 257\tloss: 8713.702984\n",
      "Step: 258\tloss: 8691.846730\n",
      "Step: 259\tloss: 8691.367778\n",
      "Step: 260\tloss: 8748.214927\n",
      "Step: 261\tloss: 8704.343788\n",
      "Step: 262\tloss: 8685.577736\n",
      "Step: 263\tloss: 8734.208661\n",
      "Step: 264\tloss: 8719.761126\n",
      "Step: 265\tloss: 8689.119063\n",
      "Step: 266\tloss: 8721.567861\n",
      "Step: 267\tloss: 8711.068214\n",
      "Step: 268\tloss: 8722.853600\n",
      "Step: 269\tloss: 8695.473272\n",
      "Step: 270\tloss: 8699.854429\n",
      "Step: 271\tloss: 8722.321709\n",
      "Step: 272\tloss: 8725.730694\n",
      "Step: 273\tloss: 8693.225512\n",
      "Step: 274\tloss: 8715.977314\n",
      "Step: 275\tloss: 8718.706763\n",
      "Step: 276\tloss: 8715.287645\n",
      "Step: 277\tloss: 8708.799073\n",
      "Step: 278\tloss: 8701.820775\n",
      "Step: 279\tloss: 8693.039456\n",
      "Step: 280\tloss: 8739.055475\n",
      "Step: 281\tloss: 8714.024147\n",
      "Step: 282\tloss: 8700.885207\n",
      "Step: 283\tloss: 8694.395941\n",
      "Step: 284\tloss: 8733.433495\n",
      "Step: 285\tloss: 8677.796281\n",
      "Step: 286\tloss: 8715.830957\n",
      "Step: 287\tloss: 8722.393929\n",
      "Step: 288\tloss: 8727.750449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 289\tloss: 8712.210471\n",
      "Step: 290\tloss: 8724.768196\n",
      "Step: 291\tloss: 8699.936695\n",
      "Step: 292\tloss: 8708.175842\n",
      "Step: 293\tloss: 8709.959639\n",
      "Step: 294\tloss: 8709.060281\n",
      "Step: 295\tloss: 8677.341841\n",
      "Step: 296\tloss: 8748.233477\n",
      "Step: 297\tloss: 8702.330357\n",
      "Step: 298\tloss: 8711.757124\n",
      "Step: 299\tloss: 8697.965955\n",
      "Step: 300\tloss: 8732.572436\n",
      "pval: 0.680317\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc[:-1,:].values, sim.par_link_loc.values)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.05\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=sim.design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(\n",
    "    input_data, \n",
    "    init_a='standard', \n",
    "    init_b='standard',\n",
    "    batch_size=500\n",
    ")\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.train_sequence(\"QUICK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc, sim.par_link_loc)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the dispersion (the variance mdoel) is badly estimated if the size-factor are not accounted for as they represent unaccounted confoudning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
