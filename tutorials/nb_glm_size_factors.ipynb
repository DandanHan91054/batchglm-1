{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pprint\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"batchglm\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import batchglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batchglm.api as glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to ignore some tensorflow warnings; just ignore this line\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = glm.models.nb_glm.Simulator(num_features=1000)\n",
    "sim.generate()\n",
    "X = sim.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superimpose library size effects on counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_factors = numpy.random.normal(loc=1, scale=0.5, size=X.shape[0]) # draw random factors\n",
    "size_factors[size_factors < 0.2] = 0.2 # threshold\n",
    "X = np.round(X*np.repeat(np.expand_dims(size_factors, axis=1), axis=1, repeats=X.shape[1])) # scale counts and round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size factor scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.017922069110753173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(X, axis=1).values/np.mean(np.sum(X, axis=1)).values - size_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sim.design_loc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add continuous covariate to desing_loc so that location model cannot be perfectly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_loc = sim.design_loc\n",
    "design_loc = np.hstack([design_loc.values, np.expand_dims(numpy.random.normal(loc=1, scale=1, size=design_loc.shape[0]), axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=size_factors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(\n",
    "    input_data, \n",
    "    init_a='standard', \n",
    "    init_b='standard',\n",
    "    batch_size=500\n",
    ")\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 9310.697198\n",
      "Step: 2\tloss: 9286.242484\n",
      "Step: 3\tloss: 9210.827453\n",
      "Step: 4\tloss: 9079.162826\n",
      "Step: 5\tloss: 9024.435197\n",
      "Step: 6\tloss: 8978.609955\n",
      "Step: 7\tloss: 8883.524098\n",
      "Step: 8\tloss: 8919.163054\n",
      "Step: 9\tloss: 8888.721216\n",
      "Step: 10\tloss: 8873.238280\n",
      "Step: 11\tloss: 8857.346668\n",
      "Step: 12\tloss: 8840.651387\n",
      "Step: 13\tloss: 8819.011068\n",
      "Step: 14\tloss: 8837.200998\n",
      "Step: 15\tloss: 8849.620234\n",
      "Step: 16\tloss: 8835.735092\n",
      "Step: 17\tloss: 8794.641498\n",
      "Step: 18\tloss: 8842.283250\n",
      "Step: 19\tloss: 8788.239622\n",
      "Step: 20\tloss: 8830.757182\n",
      "Step: 21\tloss: 8792.120407\n",
      "Step: 22\tloss: 8798.450208\n",
      "Step: 23\tloss: 8775.659502\n",
      "Step: 24\tloss: 8801.616521\n",
      "Step: 25\tloss: 8805.316500\n",
      "Step: 26\tloss: 8752.636973\n",
      "Step: 27\tloss: 8776.755684\n",
      "Step: 28\tloss: 8797.421302\n",
      "Step: 29\tloss: 8750.314173\n",
      "Step: 30\tloss: 8806.412523\n",
      "Step: 31\tloss: 8755.963809\n",
      "Step: 32\tloss: 8787.842065\n",
      "Step: 33\tloss: 8728.135710\n",
      "Step: 34\tloss: 8764.576546\n",
      "Step: 35\tloss: 8789.710733\n",
      "Step: 36\tloss: 8785.415688\n",
      "Step: 37\tloss: 8736.404123\n",
      "Step: 38\tloss: 8771.805684\n",
      "Step: 39\tloss: 8776.095886\n",
      "Step: 40\tloss: 8758.865658\n",
      "Step: 41\tloss: 8735.615738\n",
      "Step: 42\tloss: 8788.363105\n",
      "Step: 43\tloss: 8797.850206\n",
      "Step: 44\tloss: 8707.666541\n",
      "Step: 45\tloss: 8742.842006\n",
      "Step: 46\tloss: 8759.556983\n",
      "Step: 47\tloss: 8736.700348\n",
      "Step: 48\tloss: 8779.881778\n",
      "Step: 49\tloss: 8757.551252\n",
      "Step: 50\tloss: 8747.790623\n",
      "Step: 51\tloss: 8787.959177\n",
      "Step: 52\tloss: 8716.541149\n",
      "Step: 53\tloss: 8724.662708\n",
      "Step: 54\tloss: 8774.905561\n",
      "Step: 55\tloss: 8759.074115\n",
      "Step: 56\tloss: 8744.783539\n",
      "Step: 57\tloss: 8678.156782\n",
      "Step: 58\tloss: 8784.214467\n",
      "Step: 59\tloss: 8734.495174\n",
      "Step: 60\tloss: 8801.407764\n",
      "Step: 61\tloss: 8748.126322\n",
      "Step: 62\tloss: 8747.984500\n",
      "Step: 63\tloss: 8734.921560\n",
      "Step: 64\tloss: 8762.395151\n",
      "Step: 65\tloss: 8737.890031\n",
      "Step: 66\tloss: 8725.975009\n",
      "Step: 67\tloss: 8751.140019\n",
      "Step: 68\tloss: 8776.341092\n",
      "Step: 69\tloss: 8719.396686\n",
      "Step: 70\tloss: 8772.437219\n",
      "Step: 71\tloss: 8750.055602\n",
      "Step: 72\tloss: 8748.246974\n",
      "Step: 73\tloss: 8750.227693\n",
      "Step: 74\tloss: 8741.230668\n",
      "Step: 75\tloss: 8740.097239\n",
      "Step: 76\tloss: 8757.866387\n",
      "Step: 77\tloss: 8740.277310\n",
      "Step: 78\tloss: 8744.273947\n",
      "Step: 79\tloss: 8742.947845\n",
      "Step: 80\tloss: 8761.235499\n",
      "Step: 81\tloss: 8765.329857\n",
      "Step: 82\tloss: 8726.276681\n",
      "Step: 83\tloss: 8765.768015\n",
      "Step: 84\tloss: 8730.930643\n",
      "Step: 85\tloss: 8692.127908\n",
      "Step: 86\tloss: 8769.357933\n",
      "Step: 87\tloss: 8746.619537\n",
      "Step: 88\tloss: 8779.967868\n",
      "Step: 89\tloss: 8743.165836\n",
      "Step: 90\tloss: 8738.086214\n",
      "Step: 91\tloss: 8778.558450\n",
      "Step: 92\tloss: 8728.045562\n",
      "Step: 93\tloss: 8710.768606\n",
      "Step: 94\tloss: 8759.549090\n",
      "Step: 95\tloss: 8733.277691\n",
      "Step: 96\tloss: 8784.564544\n",
      "Step: 97\tloss: 8745.991330\n",
      "Step: 98\tloss: 8746.017483\n",
      "Step: 99\tloss: 8721.379961\n",
      "Step: 100\tloss: 8774.829185\n",
      "Step: 101\tloss: 8710.623282\n",
      "Step: 102\tloss: 8773.838480\n",
      "Step: 103\tloss: 8774.491109\n",
      "Step: 104\tloss: 8729.699837\n",
      "Step: 105\tloss: 8728.659765\n",
      "Step: 106\tloss: 8758.983753\n",
      "Step: 107\tloss: 8748.552349\n",
      "Step: 108\tloss: 8752.465954\n",
      "Step: 109\tloss: 8717.150628\n",
      "Step: 110\tloss: 8785.220783\n",
      "Step: 111\tloss: 8762.905162\n",
      "Step: 112\tloss: 8723.636133\n",
      "Step: 113\tloss: 8743.781358\n",
      "Step: 114\tloss: 8757.979017\n",
      "Step: 115\tloss: 8754.862569\n",
      "Step: 116\tloss: 8732.572894\n",
      "Step: 117\tloss: 8723.123909\n",
      "Step: 118\tloss: 8732.566093\n",
      "Step: 119\tloss: 8761.679229\n",
      "Step: 120\tloss: 8772.496612\n",
      "Step: 121\tloss: 8728.496268\n",
      "Step: 122\tloss: 8793.984591\n",
      "Step: 123\tloss: 8729.930325\n",
      "Step: 124\tloss: 8738.103478\n",
      "Step: 125\tloss: 8733.001716\n",
      "Step: 126\tloss: 8771.478780\n",
      "Step: 127\tloss: 8754.766737\n",
      "Step: 128\tloss: 8731.193797\n",
      "Step: 129\tloss: 8723.893216\n",
      "Step: 130\tloss: 8744.898897\n",
      "Step: 131\tloss: 8743.027354\n",
      "Step: 132\tloss: 8779.043077\n",
      "Step: 133\tloss: 8736.757593\n",
      "Step: 134\tloss: 8744.783796\n",
      "Step: 135\tloss: 8790.743709\n",
      "Step: 136\tloss: 8718.917982\n",
      "Step: 137\tloss: 8711.481735\n",
      "Step: 138\tloss: 8765.262152\n",
      "Step: 139\tloss: 8756.016865\n",
      "Step: 140\tloss: 8759.104898\n",
      "Step: 141\tloss: 8745.814277\n",
      "Step: 142\tloss: 8783.951494\n",
      "Step: 143\tloss: 8735.201989\n",
      "Step: 144\tloss: 8726.545446\n",
      "Step: 145\tloss: 8688.314869\n",
      "Step: 146\tloss: 8769.745524\n",
      "Step: 147\tloss: 8773.317916\n",
      "Step: 148\tloss: 8760.097764\n",
      "Step: 149\tloss: 8740.015357\n",
      "Step: 150\tloss: 8734.525534\n",
      "Step: 151\tloss: 8784.692947\n",
      "Step: 152\tloss: 8731.945791\n",
      "Step: 153\tloss: 8692.235054\n",
      "Step: 154\tloss: 8753.014048\n",
      "Step: 155\tloss: 8776.787159\n",
      "Step: 156\tloss: 8770.035036\n",
      "Step: 157\tloss: 8760.318880\n",
      "Step: 158\tloss: 8737.326506\n",
      "Step: 159\tloss: 8759.972439\n",
      "Step: 160\tloss: 8735.699132\n",
      "Step: 161\tloss: 8754.334163\n",
      "Step: 162\tloss: 8742.474883\n",
      "Step: 163\tloss: 8729.573434\n",
      "Step: 164\tloss: 8766.693335\n",
      "Step: 165\tloss: 8722.773899\n",
      "Step: 166\tloss: 8737.886065\n",
      "Step: 167\tloss: 8745.826416\n",
      "Step: 168\tloss: 8787.510804\n",
      "Step: 169\tloss: 8704.188740\n",
      "Step: 170\tloss: 8791.432965\n",
      "Step: 171\tloss: 8751.190716\n",
      "Step: 172\tloss: 8747.656095\n",
      "Step: 173\tloss: 8708.593501\n",
      "Step: 174\tloss: 8785.053305\n",
      "Step: 175\tloss: 8714.887754\n",
      "Step: 176\tloss: 8787.179814\n",
      "Step: 177\tloss: 8706.041142\n",
      "Step: 178\tloss: 8768.615273\n",
      "Step: 179\tloss: 8813.007048\n",
      "Step: 180\tloss: 8707.497865\n",
      "Step: 181\tloss: 8711.936844\n",
      "Step: 182\tloss: 8776.608735\n",
      "Step: 183\tloss: 8759.149614\n",
      "Step: 184\tloss: 8748.439451\n",
      "Step: 185\tloss: 8702.823566\n",
      "Step: 186\tloss: 8735.131853\n",
      "Step: 187\tloss: 8780.064519\n",
      "Step: 188\tloss: 8777.324274\n",
      "Step: 189\tloss: 8727.254589\n",
      "Step: 190\tloss: 8718.910620\n",
      "Step: 191\tloss: 8790.120828\n",
      "Step: 192\tloss: 8759.218650\n",
      "Step: 193\tloss: 8712.462214\n",
      "Step: 194\tloss: 8733.389784\n",
      "Step: 195\tloss: 8774.175938\n",
      "Step: 196\tloss: 8776.266451\n",
      "Step: 197\tloss: 8712.808682\n",
      "Step: 198\tloss: 8732.219927\n",
      "Step: 199\tloss: 8773.174244\n",
      "Step: 200\tloss: 8777.268012\n",
      "pval: 0.000038\n",
      "Step: 201\tloss: 8699.547916\n",
      "Step: 202\tloss: 8785.091199\n",
      "Step: 203\tloss: 8779.110781\n",
      "Step: 204\tloss: 8732.657272\n",
      "Step: 205\tloss: 8741.880534\n",
      "Step: 206\tloss: 8739.486836\n",
      "Step: 207\tloss: 8759.027146\n",
      "Step: 208\tloss: 8755.889084\n",
      "Step: 209\tloss: 8715.612525\n",
      "Step: 210\tloss: 8742.839147\n",
      "Step: 211\tloss: 8775.037719\n",
      "Step: 212\tloss: 8761.854996\n",
      "Step: 213\tloss: 8676.134536\n",
      "Step: 214\tloss: 8774.955480\n",
      "Step: 215\tloss: 8762.734772\n",
      "Step: 216\tloss: 8782.367470\n",
      "Step: 217\tloss: 8748.498309\n",
      "Step: 218\tloss: 8755.880677\n",
      "Step: 219\tloss: 8756.253262\n",
      "Step: 220\tloss: 8738.560850\n",
      "Step: 221\tloss: 8762.345353\n",
      "Step: 222\tloss: 8768.519023\n",
      "Step: 223\tloss: 8729.348756\n",
      "Step: 224\tloss: 8737.343572\n",
      "Step: 225\tloss: 8725.740174\n",
      "Step: 226\tloss: 8735.823367\n",
      "Step: 227\tloss: 8797.401882\n",
      "Step: 228\tloss: 8737.339035\n",
      "Step: 229\tloss: 8694.013614\n",
      "Step: 230\tloss: 8746.752523\n",
      "Step: 231\tloss: 8793.641787\n",
      "Step: 232\tloss: 8762.350214\n",
      "Step: 233\tloss: 8717.735220\n",
      "Step: 234\tloss: 8798.328753\n",
      "Step: 235\tloss: 8728.027414\n",
      "Step: 236\tloss: 8753.495631\n",
      "Step: 237\tloss: 8688.064422\n",
      "Step: 238\tloss: 8769.769710\n",
      "Step: 239\tloss: 8781.322024\n",
      "Step: 240\tloss: 8757.718963\n",
      "Step: 241\tloss: 8722.978260\n",
      "Step: 242\tloss: 8803.517802\n",
      "Step: 243\tloss: 8720.802986\n",
      "Step: 244\tloss: 8747.880205\n",
      "Step: 245\tloss: 8748.404025\n",
      "Step: 246\tloss: 8769.931641\n",
      "Step: 247\tloss: 8746.023691\n",
      "Step: 248\tloss: 8731.980435\n",
      "Step: 249\tloss: 8718.990617\n",
      "Step: 250\tloss: 8761.163494\n",
      "Step: 251\tloss: 8736.509094\n",
      "Step: 252\tloss: 8778.259223\n",
      "Step: 253\tloss: 8711.609244\n",
      "Step: 254\tloss: 8747.941577\n",
      "Step: 255\tloss: 8768.156033\n",
      "Step: 256\tloss: 8767.274388\n",
      "Step: 257\tloss: 8717.234572\n",
      "Step: 258\tloss: 8795.737556\n",
      "Step: 259\tloss: 8713.217026\n",
      "Step: 260\tloss: 8771.479342\n",
      "Step: 261\tloss: 8745.039554\n",
      "Step: 262\tloss: 8762.508971\n",
      "Step: 263\tloss: 8742.954759\n",
      "Step: 264\tloss: 8746.594702\n",
      "Step: 265\tloss: 8708.891806\n",
      "Step: 266\tloss: 8772.898496\n",
      "Step: 267\tloss: 8777.085952\n",
      "Step: 268\tloss: 8738.671303\n",
      "Step: 269\tloss: 8714.372865\n",
      "Step: 270\tloss: 8768.442336\n",
      "Step: 271\tloss: 8738.230753\n",
      "Step: 272\tloss: 8776.875420\n",
      "Step: 273\tloss: 8759.510658\n",
      "Step: 274\tloss: 8732.058409\n",
      "Step: 275\tloss: 8761.702559\n",
      "Step: 276\tloss: 8746.208121\n",
      "Step: 277\tloss: 8744.253028\n",
      "Step: 278\tloss: 8771.129895\n",
      "Step: 279\tloss: 8748.205073\n",
      "Step: 280\tloss: 8734.714890\n",
      "Step: 281\tloss: 8710.384763\n",
      "Step: 282\tloss: 8768.129835\n",
      "Step: 283\tloss: 8758.410735\n",
      "Step: 284\tloss: 8759.690867\n",
      "Step: 285\tloss: 8717.621531\n",
      "Step: 286\tloss: 8738.074300\n",
      "Step: 287\tloss: 8814.492704\n",
      "Step: 288\tloss: 8727.661662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 289\tloss: 8669.915697\n",
      "Step: 290\tloss: 8780.410298\n",
      "Step: 291\tloss: 8767.295765\n",
      "Step: 292\tloss: 8780.389245\n",
      "Step: 293\tloss: 8729.448167\n",
      "Step: 294\tloss: 8741.509567\n",
      "Step: 295\tloss: 8774.033526\n",
      "Step: 296\tloss: 8750.479377\n",
      "Step: 297\tloss: 8722.656853\n",
      "Step: 298\tloss: 8776.073892\n",
      "Step: 299\tloss: 8777.552791\n",
      "Step: 300\tloss: 8720.446490\n",
      "pval: 0.615669\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence('QUICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.03\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc[:-1,:].values, sim.par_link_loc.values)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.05\n",
      "Root mean squared deviation of scale:    0.08\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without size factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glm.models.nb_glm.InputData.new(\n",
    "    data=X, \n",
    "    design_loc=sim.design_loc, \n",
    "    design_scale=sim.design_scale,\n",
    "    size_factors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard initialization for mean\n",
      "Should train mu: True\n",
      "Using standard initialization for dispersion\n",
      "Should train r: True\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = glm.models.nb_glm.Estimator(\n",
    "    input_data, \n",
    "    init_a='standard', \n",
    "    init_b='standard',\n",
    "    batch_size=500\n",
    ")\n",
    "estimator.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training strategy:\n",
      "[{'convergence_criteria': 't_test',\n",
      "  'learning_rate': 0.1,\n",
      "  'loss_window_size': 100,\n",
      "  'optim_algo': 'ADAM',\n",
      "  'stop_at_loss_change': 0.05,\n",
      "  'use_batching': True}]\n",
      "Beginning with training sequence #1\n",
      "Step: 1\tloss: 9470.138304\n",
      "Step: 2\tloss: 9458.059912\n",
      "Step: 3\tloss: 9373.541370\n",
      "Step: 4\tloss: 9334.954461\n",
      "Step: 5\tloss: 9306.730194\n",
      "Step: 6\tloss: 9347.867113\n",
      "Step: 7\tloss: 9289.773319\n",
      "Step: 8\tloss: 9306.822366\n",
      "Step: 9\tloss: 9282.921417\n",
      "Step: 10\tloss: 9336.342939\n",
      "Step: 11\tloss: 9308.485450\n",
      "Step: 12\tloss: 9272.548602\n",
      "Step: 13\tloss: 9276.389099\n",
      "Step: 14\tloss: 9292.443456\n",
      "Step: 15\tloss: 9290.597210\n",
      "Step: 16\tloss: 9282.899090\n",
      "Step: 17\tloss: 9301.267784\n",
      "Step: 18\tloss: 9259.684296\n",
      "Step: 19\tloss: 9292.685667\n",
      "Step: 20\tloss: 9274.766977\n",
      "Step: 21\tloss: 9279.150510\n",
      "Step: 22\tloss: 9289.743058\n",
      "Step: 23\tloss: 9267.081905\n",
      "Step: 24\tloss: 9262.336138\n",
      "Step: 25\tloss: 9247.386821\n",
      "Step: 26\tloss: 9306.014561\n",
      "Step: 27\tloss: 9265.024327\n",
      "Step: 28\tloss: 9271.668478\n",
      "Step: 29\tloss: 9230.510962\n",
      "Step: 30\tloss: 9284.256183\n",
      "Step: 31\tloss: 9301.006908\n",
      "Step: 32\tloss: 9270.074273\n",
      "Step: 33\tloss: 9246.281415\n",
      "Step: 34\tloss: 9294.424928\n",
      "Step: 35\tloss: 9245.158129\n",
      "Step: 36\tloss: 9293.974531\n",
      "Step: 37\tloss: 9268.905142\n",
      "Step: 38\tloss: 9282.983660\n",
      "Step: 39\tloss: 9247.879730\n",
      "Step: 40\tloss: 9277.247946\n",
      "Step: 41\tloss: 9231.637669\n",
      "Step: 42\tloss: 9296.298861\n",
      "Step: 43\tloss: 9303.975750\n",
      "Step: 44\tloss: 9244.151034\n",
      "Step: 45\tloss: 9292.514610\n",
      "Step: 46\tloss: 9264.486521\n",
      "Step: 47\tloss: 9259.314433\n",
      "Step: 48\tloss: 9255.933655\n",
      "Step: 49\tloss: 9276.550815\n",
      "Step: 50\tloss: 9253.843806\n",
      "Step: 51\tloss: 9275.795351\n",
      "Step: 52\tloss: 9266.007500\n",
      "Step: 53\tloss: 9237.024121\n",
      "Step: 54\tloss: 9280.209582\n",
      "Step: 55\tloss: 9281.019882\n",
      "Step: 56\tloss: 9273.316530\n",
      "Step: 57\tloss: 9247.839411\n",
      "Step: 58\tloss: 9276.464294\n",
      "Step: 59\tloss: 9292.375470\n",
      "Step: 60\tloss: 9254.169131\n",
      "Step: 61\tloss: 9246.639582\n",
      "Step: 62\tloss: 9300.871932\n",
      "Step: 63\tloss: 9273.184334\n",
      "Step: 64\tloss: 9250.077341\n",
      "Step: 65\tloss: 9265.472181\n",
      "Step: 66\tloss: 9266.814556\n",
      "Step: 67\tloss: 9282.298170\n",
      "Step: 68\tloss: 9257.273766\n",
      "Step: 69\tloss: 9224.717639\n",
      "Step: 70\tloss: 9306.272863\n",
      "Step: 71\tloss: 9293.095459\n",
      "Step: 72\tloss: 9247.969969\n",
      "Step: 73\tloss: 9268.721635\n",
      "Step: 74\tloss: 9263.029760\n",
      "Step: 75\tloss: 9272.424708\n",
      "Step: 76\tloss: 9267.783483\n",
      "Step: 77\tloss: 9249.691370\n",
      "Step: 78\tloss: 9274.253661\n",
      "Step: 79\tloss: 9311.050188\n",
      "Step: 80\tloss: 9236.935420\n",
      "Step: 81\tloss: 9253.484175\n",
      "Step: 82\tloss: 9230.688979\n",
      "Step: 83\tloss: 9267.741108\n",
      "Step: 84\tloss: 9323.270080\n",
      "Step: 85\tloss: 9236.452902\n",
      "Step: 86\tloss: 9288.468768\n",
      "Step: 87\tloss: 9284.364901\n",
      "Step: 88\tloss: 9263.186285\n",
      "Step: 89\tloss: 9243.498446\n",
      "Step: 90\tloss: 9263.534401\n",
      "Step: 91\tloss: 9291.199083\n",
      "Step: 92\tloss: 9272.712825\n",
      "Step: 93\tloss: 9244.158847\n",
      "Step: 94\tloss: 9293.552447\n",
      "Step: 95\tloss: 9274.271969\n",
      "Step: 96\tloss: 9257.371939\n",
      "Step: 97\tloss: 9258.144216\n",
      "Step: 98\tloss: 9262.698527\n",
      "Step: 99\tloss: 9277.130180\n",
      "Step: 100\tloss: 9272.671694\n",
      "Step: 101\tloss: 9255.370471\n",
      "Step: 102\tloss: 9278.590895\n",
      "Step: 103\tloss: 9278.583830\n",
      "Step: 104\tloss: 9261.897618\n",
      "Step: 105\tloss: 9249.197228\n",
      "Step: 106\tloss: 9279.877604\n",
      "Step: 107\tloss: 9283.191900\n",
      "Step: 108\tloss: 9262.964786\n",
      "Step: 109\tloss: 9233.938617\n",
      "Step: 110\tloss: 9296.723166\n",
      "Step: 111\tloss: 9289.540269\n",
      "Step: 112\tloss: 9252.610969\n",
      "Step: 113\tloss: 9254.956916\n",
      "Step: 114\tloss: 9261.490344\n",
      "Step: 115\tloss: 9253.051815\n",
      "Step: 116\tloss: 9303.310576\n",
      "Step: 117\tloss: 9260.792724\n",
      "Step: 118\tloss: 9281.792460\n",
      "Step: 119\tloss: 9269.805842\n",
      "Step: 120\tloss: 9262.132982\n",
      "Step: 121\tloss: 9265.701552\n",
      "Step: 122\tloss: 9254.154216\n",
      "Step: 123\tloss: 9256.496092\n",
      "Step: 124\tloss: 9299.746943\n",
      "Step: 125\tloss: 9238.938473\n",
      "Step: 126\tloss: 9292.867950\n",
      "Step: 127\tloss: 9271.496243\n",
      "Step: 128\tloss: 9270.055926\n",
      "Step: 129\tloss: 9255.520258\n",
      "Step: 130\tloss: 9271.751069\n",
      "Step: 131\tloss: 9262.153576\n",
      "Step: 132\tloss: 9281.912689\n",
      "Step: 133\tloss: 9226.266262\n",
      "Step: 134\tloss: 9259.790564\n",
      "Step: 135\tloss: 9297.493567\n",
      "Step: 136\tloss: 9289.879208\n",
      "Step: 137\tloss: 9243.401332\n",
      "Step: 138\tloss: 9293.538443\n",
      "Step: 139\tloss: 9289.105972\n",
      "Step: 140\tloss: 9249.922168\n",
      "Step: 141\tloss: 9273.207446\n",
      "Step: 142\tloss: 9283.267415\n",
      "Step: 143\tloss: 9286.349961\n",
      "Step: 144\tloss: 9228.877165\n",
      "Step: 145\tloss: 9276.782206\n",
      "Step: 146\tloss: 9272.197799\n",
      "Step: 147\tloss: 9228.778153\n",
      "Step: 148\tloss: 9295.058955\n",
      "Step: 149\tloss: 9254.813865\n",
      "Step: 150\tloss: 9255.177382\n",
      "Step: 151\tloss: 9259.025846\n",
      "Step: 152\tloss: 9302.654457\n",
      "Step: 153\tloss: 9273.916707\n",
      "Step: 154\tloss: 9259.670744\n",
      "Step: 155\tloss: 9265.777537\n",
      "Step: 156\tloss: 9272.581009\n",
      "Step: 157\tloss: 9266.391441\n",
      "Step: 158\tloss: 9315.577612\n",
      "Step: 159\tloss: 9219.907223\n",
      "Step: 160\tloss: 9270.615992\n",
      "Step: 161\tloss: 9246.884529\n",
      "Step: 162\tloss: 9299.632685\n",
      "Step: 163\tloss: 9260.862825\n",
      "Step: 164\tloss: 9268.301780\n",
      "Step: 165\tloss: 9220.148079\n",
      "Step: 166\tloss: 9293.043181\n",
      "Step: 167\tloss: 9257.809098\n",
      "Step: 168\tloss: 9304.181769\n",
      "Step: 169\tloss: 9271.710357\n",
      "Step: 170\tloss: 9285.447432\n",
      "Step: 171\tloss: 9279.090651\n",
      "Step: 172\tloss: 9242.988535\n",
      "Step: 173\tloss: 9232.598997\n",
      "Step: 174\tloss: 9277.222349\n",
      "Step: 175\tloss: 9292.330778\n",
      "Step: 176\tloss: 9282.314049\n",
      "Step: 177\tloss: 9268.398463\n",
      "Step: 178\tloss: 9255.541068\n",
      "Step: 179\tloss: 9296.989777\n",
      "Step: 180\tloss: 9257.352027\n",
      "Step: 181\tloss: 9254.680130\n",
      "Step: 182\tloss: 9252.649495\n",
      "Step: 183\tloss: 9264.268114\n",
      "Step: 184\tloss: 9306.628684\n",
      "Step: 185\tloss: 9253.839686\n",
      "Step: 186\tloss: 9280.475452\n",
      "Step: 187\tloss: 9266.851133\n",
      "Step: 188\tloss: 9278.694575\n",
      "Step: 189\tloss: 9251.389684\n",
      "Step: 190\tloss: 9275.792788\n",
      "Step: 191\tloss: 9291.714175\n",
      "Step: 192\tloss: 9253.453715\n",
      "Step: 193\tloss: 9257.177963\n",
      "Step: 194\tloss: 9283.516180\n",
      "Step: 195\tloss: 9249.317958\n",
      "Step: 196\tloss: 9283.026621\n",
      "Step: 197\tloss: 9246.849334\n",
      "Step: 198\tloss: 9266.847988\n",
      "Step: 199\tloss: 9257.458317\n",
      "Step: 200\tloss: 9304.376126\n",
      "pval: 0.009532\n",
      "Step: 201\tloss: 9253.258294\n",
      "Step: 202\tloss: 9308.106675\n",
      "Step: 203\tloss: 9281.183219\n",
      "Step: 204\tloss: 9232.913048\n",
      "Step: 205\tloss: 9261.532883\n",
      "Step: 206\tloss: 9297.546377\n",
      "Step: 207\tloss: 9251.043018\n",
      "Step: 208\tloss: 9264.587937\n",
      "Step: 209\tloss: 9240.956892\n",
      "Step: 210\tloss: 9274.021202\n",
      "Step: 211\tloss: 9283.330739\n",
      "Step: 212\tloss: 9277.219340\n",
      "Step: 213\tloss: 9280.382006\n",
      "Step: 214\tloss: 9271.415830\n",
      "Step: 215\tloss: 9254.443120\n",
      "Step: 216\tloss: 9266.537221\n",
      "Step: 217\tloss: 9259.704701\n",
      "Step: 218\tloss: 9272.631952\n",
      "Step: 219\tloss: 9272.004231\n",
      "Step: 220\tloss: 9268.626897\n",
      "Step: 221\tloss: 9252.662790\n",
      "Step: 222\tloss: 9274.648780\n",
      "Step: 223\tloss: 9278.503258\n",
      "Step: 224\tloss: 9266.141452\n",
      "Step: 225\tloss: 9247.938697\n",
      "Step: 226\tloss: 9279.508741\n",
      "Step: 227\tloss: 9264.541350\n",
      "Step: 228\tloss: 9281.655795\n",
      "Step: 229\tloss: 9234.499318\n",
      "Step: 230\tloss: 9294.743993\n",
      "Step: 231\tloss: 9282.295267\n",
      "Step: 232\tloss: 9262.586605\n",
      "Step: 233\tloss: 9216.691819\n",
      "Step: 234\tloss: 9285.878002\n",
      "Step: 235\tloss: 9276.914405\n",
      "Step: 236\tloss: 9293.983266\n",
      "Step: 237\tloss: 9264.771152\n",
      "Step: 238\tloss: 9268.850136\n",
      "Step: 239\tloss: 9277.776037\n",
      "Step: 240\tloss: 9262.738287\n",
      "Step: 241\tloss: 9248.755022\n",
      "Step: 242\tloss: 9310.616866\n",
      "Step: 243\tloss: 9229.211765\n",
      "Step: 244\tloss: 9285.464084\n",
      "Step: 245\tloss: 9251.972659\n",
      "Step: 246\tloss: 9316.660260\n",
      "Step: 247\tloss: 9254.449396\n",
      "Step: 248\tloss: 9249.369511\n",
      "Step: 249\tloss: 9267.238875\n",
      "Step: 250\tloss: 9274.376994\n",
      "Step: 251\tloss: 9286.163184\n",
      "Step: 252\tloss: 9245.951845\n",
      "Step: 253\tloss: 9268.023693\n",
      "Step: 254\tloss: 9270.195366\n",
      "Step: 255\tloss: 9260.428127\n",
      "Step: 256\tloss: 9273.402224\n",
      "Step: 257\tloss: 9251.820163\n",
      "Step: 258\tloss: 9266.081510\n",
      "Step: 259\tloss: 9283.073536\n",
      "Step: 260\tloss: 9272.285706\n",
      "Step: 261\tloss: 9247.343487\n",
      "Step: 262\tloss: 9278.654118\n",
      "Step: 263\tloss: 9268.995279\n",
      "Step: 264\tloss: 9278.170564\n",
      "Step: 265\tloss: 9288.223826\n",
      "Step: 266\tloss: 9269.585525\n",
      "Step: 267\tloss: 9252.465652\n",
      "Step: 268\tloss: 9263.207551\n",
      "Step: 269\tloss: 9235.787043\n",
      "Step: 270\tloss: 9285.959189\n",
      "Step: 271\tloss: 9284.419058\n",
      "Step: 272\tloss: 9268.996586\n",
      "Step: 273\tloss: 9247.686298\n",
      "Step: 274\tloss: 9276.040484\n",
      "Step: 275\tloss: 9284.843814\n",
      "Step: 276\tloss: 9266.486808\n",
      "Step: 277\tloss: 9236.931500\n",
      "Step: 278\tloss: 9279.557975\n",
      "Step: 279\tloss: 9287.018558\n",
      "Step: 280\tloss: 9271.533472\n",
      "Step: 281\tloss: 9253.361623\n",
      "Step: 282\tloss: 9259.290119\n",
      "Step: 283\tloss: 9296.897043\n",
      "Step: 284\tloss: 9266.515876\n",
      "Step: 285\tloss: 9251.437251\n",
      "Step: 286\tloss: 9280.747589\n",
      "Step: 287\tloss: 9246.179080\n",
      "Step: 288\tloss: 9295.444948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 289\tloss: 9267.646973\n",
      "Step: 290\tloss: 9256.757971\n",
      "Step: 291\tloss: 9269.803401\n",
      "Step: 292\tloss: 9279.803906\n",
      "Step: 293\tloss: 9238.758431\n",
      "Step: 294\tloss: 9276.509319\n",
      "Step: 295\tloss: 9259.175140\n",
      "Step: 296\tloss: 9309.244444\n",
      "Step: 297\tloss: 9259.203673\n",
      "Step: 298\tloss: 9276.826329\n",
      "Step: 299\tloss: 9278.791471\n",
      "Step: 300\tloss: 9266.655260\n",
      "pval: 0.489436\n",
      "Training sequence #1 complete\n"
     ]
    }
   ],
   "source": [
    "estimator.train_sequence(\"QUICK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results with the simulated data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.05\n",
      "Root mean squared deviation of scale:    0.48\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(estimator.par_link_loc, sim.par_link_loc)\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(estimator.par_link_scale, sim.par_link_scale)\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared deviation of location: 0.05\n",
      "Root mean squared deviation of scale:    1.04\n"
     ]
    }
   ],
   "source": [
    "locdiff = glm.utils.stats.rmsd(np.matmul(estimator.design_loc, estimator.par_link_loc), \n",
    "                               np.matmul(sim.design_loc, sim.par_link_loc))\n",
    "print(\"Root mean squared deviation of location: %.2f\" % locdiff)\n",
    "\n",
    "scalediff = glm.utils.stats.rmsd(np.matmul(estimator.design_scale, estimator.par_link_scale), \n",
    "                                 np.matmul(sim.design_scale, sim.par_link_scale))\n",
    "print(\"Root mean squared deviation of scale:    %.2f\" % scalediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the dispersion (the variance mdoel) is badly estimated if the size-factor are not accounted for as they represent unaccounted confoudning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
